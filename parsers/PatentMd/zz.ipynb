{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## patent_parser    2025-0909-1508 记录代码\n",
    "\n",
    "# parsers.py  -> 专利.md\n",
    "\n",
    "from collections import OrderedDict\n",
    "import os, re, json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple, Union\n",
    "\n",
    "\n",
    "\"\"\"   \n",
    "专利类别: \n",
    "- 发明专利       - 普通版PDF文件解析ok\n",
    "- 实用新型专利   - 普通版PDF文件解析ok\n",
    "- 外观设计专利   - 图片多、文字少，并且很有可能会是打印版PDF源文件    -- 这类直接不处理了，文字内容不多没啥意义。 \n",
    "\n",
    "\n",
    "# 中国专利的著录项目\n",
    "(10)\t专利文献标识    \n",
    "(12)\t专利文献名称\n",
    "(15)\t专利文献更正数据\n",
    "(19)\t公布或公告专利文献的国家机构名称\n",
    "(21)\t申请号\t\n",
    "(22)\t申请日 \n",
    "(30)\t优先权数据\n",
    "(43)\t申请公布日\n",
    "(45)\t授权公告日\t\n",
    "(48)\t更正文献出版日                          \n",
    "(51)\t国际专利分类\t\n",
    "(54)\t发明或实用新型名称\n",
    "(56)\t对比文件\n",
    "(57)\t摘要\n",
    "(62)\t分案原申请数据\t\n",
    "(66)\t本国优先权数据\n",
    "(71)\t申请人\n",
    "(72)\t发明人\n",
    "(73)\t专利权人\n",
    "(74)\t专利代理机构及代理人\n",
    "(83)\t生物保藏信息\n",
    "(85)\tPCT国际申请进入国家阶段日\t\n",
    "(86)\tPCT国际申请的申请数据\n",
    "(87)\tPCT国际申请的公布数据\t\n",
    "\n",
    "--- 我解析之后会得到markdown：\n",
    "...\n",
    "...\n",
    "# (54) 发明或实用新型名称    -------- 分开，  上面是、初步筛选的信息、 然后是正文--正文部分再来匹配\n",
    "\n",
    "\n",
    "我需要做的：抠出 图片的引用， 检索到 图片的相关描述， ---> {图1/2/3  : [\"图片的一句话/简短描述\", 图片的路径]，\n",
    "                                                        摘要图：图片的路径}\n",
    "\n",
    "\t\n",
    "\n",
    "\n",
    "\t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "基本及系统级别提示词：\n",
    "\n",
    "# 一些基本的附加知识（可能有用）\n",
    "1.发明专利\n",
    "定义：发明专利是指对产品、方法或者其改进所提出的新的技术方案。它强调技术方案的新颖性、创造性和实用性。\n",
    "特点：发明专利的申请和审查过程较为复杂，通常需要经过初审、公布和实质审查等多个阶段。获得授权后，发明专利的保护期限为20年。 \n",
    "\n",
    "2. 实用新型专利\n",
    "定义：实用新型专利是针对产品的形状、构造或者它们的结合所提出的适于实用的新技术方案。与发明专利相比，实用新型专利更注重产品的实用性和新颖性，而不强调创造性。\n",
    "特点：实用新型专利的申请流程相对简单，通常只需经过初审和授权两个阶段，保护期限为10年。 \n",
    "\n",
    "3. 外观设计专利\n",
    "定义：外观设计专利是指对产品的形状、图案或者其结合以及色彩与形状、图案的结合所作出的富有美感并适于工业应用的新设计。\n",
    "特点：外观设计专利主要保护产品的外观美感，申请过程相对简单，通常只需经过初审和授权两个阶段，保护期限为15年。 \n",
    "\n",
    "\n",
    "路径：专利文件路径：\n",
    "    docs_patents/\n",
    "        + xxxxx/   # 某一个专利的子文件夹\n",
    "            + images/       # 专利markdown中的图片\n",
    "            + full.md       # 专利markdown文件\n",
    "            + *_origin.pdf  # 原始专利pdf文件\n",
    "            + *.json  # 目前用不上的文件\n",
    "        + xxxxx/   # 某一个专利的子文件夹\n",
    "            + images/       # 专利markdown中的图片\n",
    "            + full.md       # 专利markdown文件\n",
    "            + *_origin.pdf  # 原始专利pdf文件\n",
    "            + *.json  # 目前用不上的文件\n",
    "        + ...\n",
    "        \n",
    "--> docs_patents/\n",
    "        + xxxxx/   # 某一个专利的子文件夹\n",
    "            + images/                 # 专利markdown中的图片\n",
    "            + full_filtered.md        # 清理后的专利markdown文件\n",
    "            + MetaDict_filtered.json  # 专利MetaDicy\n",
    "            + .                       # 目前用不上的文件\n",
    "        + xxxxx/   # 某一个专利的子文件夹\n",
    "            + images/                 # 专利markdown中的图片\n",
    "            + full_filtered.md        # 清理后的专利markdown文件\n",
    "            + MetaDict_filtered.json  # 专利MetaDicy\n",
    "            + .                       # 目前用不上的文件\n",
    "        + ...\n",
    "\"\"\"\n",
    "\n",
    "class patentMD_parser:\n",
    "    \"\"\"\n",
    "    读取专利，\n",
    "    \n",
    "    清理图片引用，抽取元数据，输出：\n",
    "      1) MetaDict（OrderedDict，字段按给定骨架）\n",
    "      2) 结构化纯文本 Markdown（<原名>_z.md）到指定目录\n",
    "\n",
    "    结构化纯文本：\n",
    "        # (54) 实用新型名称\n",
    "\n",
    "    解析要点：\n",
    "      1) 基本元数据：title / apply_time / applier / address / inventor / pubno\n",
    "      2) 图片元数据：摘要图（abs_im）+ 附图（图1/图2/...）→ {\"fig_list\": {\"图1\": [\"描述\", \"绝对路径\"], ...}}\n",
    "      3) 清理正文中的图片标记（![](...) + “图X ...”行）后作为向量化文本\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------- 初始化 ---------------------------\n",
    "    def __init__(self, markdown_file: Union[str, Path], out_dir: Union[str, Path, None] = None):\n",
    "        self.markdown_file = str(Path(markdown_file))\n",
    "        self.base_dir: Path = Path(self.markdown_file).parent\n",
    "        self.images_dir: Path = self.base_dir / \"images\"\n",
    "        self.out_dir: Path = Path(out_dir) if out_dir is not None else self.base_dir\n",
    "        self.pdfp = next(Path(self.markdown_file).parent.glob(\"*_origin.pdf\"),None)\n",
    "        assert Path(self.pdfp).is_file()\n",
    "        self.text: str = \"\"  # 原始 md 文本\n",
    "\n",
    "        self.meta_schema = OrderedDict({\n",
    "            \"publ_no\":\"\",          # (10) 申请公布号 or 授权公告号   <公开号>\n",
    "            \"publ_date\":\"\",        # (43) 申请公布日 or (45) 授权公告日\n",
    "            \"is_granted\": False,    # 是否授权，授权的话才会有专利号\n",
    "            \"patent_no\":\"\",        # 专利号（由申请号生成）          <专利号> if is_granted is Ture\n",
    "            \"apply_no\": '',        # (21) 申请号(不重要)            <申请号>\n",
    "            \"apply_time\": \"\",      # (22) 申请日\n",
    "            \"title\": \"\",           # (54) 专利标题（实用新型/发明 名称）\n",
    "            \"applicant\": \"\",      # (54) 专利权人 申请人\n",
    "            \"address\": \"\",         # \"邮编 地址\"\n",
    "            \"inventors\": \"\",       # (71) 发明人\n",
    "            \"doc_type\": \"\",        # <(12) 文献类型，如“实用新型专利/发明专利申请/外观设计专利”>  # 外观设计专利只有图可以展示\n",
    "            \"tech_field\": \"\",      # # 技术领域 的正文（不含标题）\n",
    "            \n",
    "            \"root_dir\": \"\",        # 专利目录（绝对路径）\n",
    "            \"pdf_path\": \"\",        # 原始 PDF 的绝对路径\n",
    "            \"fig_list\": {},        # {\"abs_im\": [\"摘要图\", abs_path], \"图1\": [\"描述\", abs_path], ...}\n",
    "        })\n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        self.pipeline()\n",
    "        \n",
    "    # ============== 主入口 ==============\n",
    "    def pipeline(self):\n",
    "        # 1) 读全文\n",
    "        self.text = self._load_md_text()\n",
    "\n",
    "        # 2) 图片元数据（摘要图 + 附图）\n",
    "        img_meta = self._extract_img_metadata()  # {\"fig_list\": {...}}\n",
    "\n",
    "        # 3) 清理图片引用（含摘要段中的图片）\n",
    "        text_wo_imgs = self._filter_lines(self.text)          # 全文去图片\n",
    "        text_wo_imgs = self._clean_abstract_images(text_wo_imgs)    # 摘要段内再次兜底清图\n",
    "        text_wo_imgs = self._filter_trailing_image_blocks(text_wo_imgs)  # 末尾批量图清理\n",
    "\n",
    "\n",
    "        # 4) 结构化字段\n",
    "        meta_blocks = self._extract_meta_blocks(self.text)  # 注意：元数据从「全文」抓，避免被裁剪掉\n",
    "\n",
    "        # 4.1)  技术领域正文抽取（不含标题）\n",
    "        tech_field_txt = self._extract_section_plain_text(self.text, r\"技术领域\", strip_para_tags=True)\n",
    "        if tech_field_txt:\n",
    "            meta_blocks[\"tech_field\"] = tech_field_txt\n",
    "\n",
    "        # 5) PDF 公告号 \n",
    "        pubno_info = self._extract_pubno() # {\"pubno\": \"...\", \"pdf_path\": \"...\"}\n",
    "\n",
    "        # 6) 汇总 MetaDict\n",
    "        self.meta_schema[\"root_dir\"] = str(self.base_dir.resolve())\n",
    "        self.meta_schema.update(img_meta)\n",
    "        self.meta_schema.update(meta_blocks)\n",
    "        self.meta_schema.update(pubno_info)\n",
    "\n",
    "        # 6-2) MetaDict_norm\n",
    "        self.MetaDict_norm()\n",
    "        \n",
    "        # 6-3) del enpty value  ke-value\n",
    "        self.meta_schema = self.MetaDict_NoEmptyValue(self.meta_schema)\n",
    "\n",
    "\n",
    "        # # 7) 落盘, out_dir不是None的话\n",
    "        if self.out_dir is not None:\n",
    "            # text_wo_imgs     ## structured_md 没有必要删除那么多信息\n",
    "            out_path = self._write_structured_md(text_wo_imgs)\n",
    "            return self.meta_schema, out_path\n",
    "        else:\n",
    "            print(\"no mdz\")\n",
    "        return self.meta_schema\n",
    "\n",
    "    # ============== 基础工具 ==============\n",
    "    def _load_md_text(self) -> str:\n",
    "        return Path(self.markdown_file).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _find_section(text: str, title_pattern: str) -> Optional[str]:\n",
    "        \"\"\"取指定 # 段（到下一个 # 或文末），title_pattern 不含 #。\"\"\"\n",
    "        hdr = re.search(rf\"^\\s*#{{1,3}}\\s*{title_pattern}\\s*$\", text, flags=re.MULTILINE)\n",
    "        if not hdr:\n",
    "            return None\n",
    "        start = hdr.end()\n",
    "        nxt = re.search(r\"^\\s*#\\s+\", text[start:], flags=re.MULTILINE)\n",
    "        return text[start: start + nxt.start()] if nxt else text[start:]\n",
    "\n",
    "\n",
    "    # ============== 中文数字工具（用于图号解析） ==============\n",
    "    @staticmethod\n",
    "    def _chs_num_to_int(s: str) -> Optional[int]:\n",
    "        m = {\"零\":0,\"一\":1,\"二\":2,\"两\":2,\"三\":3,\"四\":4,\"五\":5,\"六\":6,\"七\":7,\"八\":8,\"九\":9,\"十\":10}\n",
    "        s = s.strip()\n",
    "        if not s: return None\n",
    "        if s == \"十\": return 10\n",
    "        if len(s) == 1 and s in m: return m[s]\n",
    "        if s[0] == \"十\":\n",
    "            tail = m.get(s[1:], 0) if s[1:] else 0\n",
    "            return 10 + tail\n",
    "        if \"十\" in s:\n",
    "            left, right = s.split(\"十\", 1)\n",
    "            left_v = m.get(left, 1) if left else 1\n",
    "            right_v = m.get(right, 0) if right else 0\n",
    "            return left_v * 10 + right_v\n",
    "        if s.isdigit(): return int(s)\n",
    "        if s in m: return m[s]\n",
    "        return None\n",
    "\n",
    "    # ============== 图片元数据（摘要图 / 附图） ==============\n",
    "    def _extract_abstract_img(self) -> Tuple[str, Dict[str, Dict[str, Union[List[str], None]]]]:\n",
    "        block = self._find_section(self.text, r\"\\(?57\\)?\\s*摘要\")\n",
    "        if not block:\n",
    "            return \"\", {\"fig_list\": {\"abs_im\": None}}\n",
    "        img_m = re.search(r'!\\[.*?\\]\\((.*?)\\)', block)\n",
    "        if not img_m:\n",
    "            return block.strip(), {\"fig_list\": {\"abs_im\": None}}\n",
    "        rel_path = img_m.group(1).strip()\n",
    "        abs_path = str((self.base_dir / rel_path).resolve())\n",
    "        cleaned = re.sub(r'!\\[.*?\\]\\(.*?\\)\\s*\\n?', \"\", block).strip()\n",
    "        return cleaned, {\"fig_list\": {\"abs_im\": [\"摘要图\", abs_path]}}\n",
    "\n",
    "    def _parse_figure_descriptions(self) -> Dict[str, str]:\n",
    "        \"\"\"从『附图说明』段解析 图X→描述。\"\"\"\n",
    "        desc_map: Dict[str, str] = {}\n",
    "        block = self._find_section(self.text, r\"附图说明\")\n",
    "        if not block:\n",
    "            return desc_map\n",
    "        block = block.replace(\"：\", \":\").replace(\"；\", \";\")\n",
    "        pat = re.compile(\n",
    "            r\"(?:\\[\\d+\\]\\s*)?图\\s*([0-9一二三四五六七八九十]+)\\s*(?:为|是|:)\\s*(.+?)(?=(?:；|;|。|\\.|\\n|$))\",\n",
    "            flags=re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "        for num, desc in pat.findall(block):\n",
    "            idx = self._chs_num_to_int(num) or (int(num) if num.isdigit() else None)\n",
    "            if not idx:\n",
    "                continue\n",
    "            key = f\"图{idx}\"\n",
    "            desc_map[key] = re.sub(r\"[\\s;；。.\\u3000]+$\", \"\", desc.strip())\n",
    "        return desc_map\n",
    "\n",
    "    def _build_fig_map(self) -> Dict[str, str]:\n",
    "        \"\"\"匹配形式：图片行 →（可有空行）→ 下一行 '图N'。\"\"\"\n",
    "        img_map: Dict[str, str] = {}\n",
    "        img_iter = re.finditer(\n",
    "            r'!\\[.*?\\]\\((.*?)\\)\\s*(?:\\n[ \\t]*){0,2}(图[0-9一二三四五六七八九十]+)',\n",
    "            self.text, flags=re.IGNORECASE\n",
    "        )\n",
    "        for m in img_iter:\n",
    "            rel_path = m.group(1).strip()\n",
    "            tag = m.group(2).strip()\n",
    "            mnum = re.match(r\"图\\s*([0-9一二三四五六七八九十]+)\", tag)\n",
    "            if not mnum:\n",
    "                continue\n",
    "            idx_raw = mnum.group(1)\n",
    "            idx = self._chs_num_to_int(idx_raw) or (int(idx_raw) if idx_raw.isdigit() else None)\n",
    "            if not idx:\n",
    "                continue\n",
    "            key = f\"图{idx}\"\n",
    "            abs_path = str((self.base_dir / rel_path).resolve())\n",
    "            if Path(abs_path).exists():\n",
    "                img_map[key] = abs_path\n",
    "        return img_map\n",
    "\n",
    "    def _extract_img_metadata(self) -> Dict[str, Dict[str, Union[List[str], None]]]:\n",
    "        \"\"\"整合摘要图 + 附图说明 + 图片路径为 fig_list。\"\"\"\n",
    "        _, abs_meta = self._extract_abstract_img()\n",
    "        abs_item = abs_meta[\"fig_list\"].get(\"abs_im\")\n",
    "        desc_map = self._parse_figure_descriptions()\n",
    "        img_map  = self._build_fig_map()\n",
    "        fig_list: Dict[str, Union[List[str], None]] = {}\n",
    "        for k, path in img_map.items():\n",
    "            desc = desc_map.get(k, \"\")\n",
    "            fig_list[k] = [desc, path]\n",
    "        if abs_item and isinstance(abs_item, list) and len(abs_item) == 2:\n",
    "            fig_list[\"abs_im\"] = abs_item\n",
    "        return {\"fig_list\": fig_list}\n",
    "\n",
    "    # ============== 文本清理 ==============\n",
    "    def _filter_lines(self, md_text: str) -> str:\n",
    "        \"\"\"\n",
    "        清除所有图片引用 + 紧随的“图X …”行；再清除**孤立的“图N”整行**。\n",
    "        \"\"\"\n",
    "        # 1) 图片 + 紧随“图N …”的一行\n",
    "        pattern = re.compile(\n",
    "            r'!\\[.*?\\]\\([^)]*\\)[ \\t]*(?:\\n[ \\t]*)*\\n?[ \\t]*图[0-9一二三四五六七八九十]+[：:\\s]*[^\\n]*(?:\\n|$)',\n",
    "            flags=re.IGNORECASE | re.MULTILINE\n",
    "        )\n",
    "        out = pattern.sub('', md_text)\n",
    "        # 2) 裸图片\n",
    "        out = re.sub(r'!\\[.*?\\]\\([^)]*\\)\\s*\\n?', '', out)\n",
    "        # 3) NEW: 孤立“图N”整行（只保留纯正文里诸如“如图2所示”，不会删掉这类）\n",
    "        out = re.sub(r'(?m)^\\s*图[0-9一二三四五六七八九十]+\\s*$', '', out)\n",
    "        out = re.sub(r'(?m)^\\s*图[0-9一二三四五六七八九十]+\\s*[：:]\\s*$', '', out)\n",
    "        # 4) 清理多余空行（可选）\n",
    "        out = re.sub(r'\\n{3,}', '\\n\\n', out).strip() + \"\\n\"\n",
    "        return out\n",
    "\n",
    "    def _clean_abstract_images(self, md_text: str) -> str:\n",
    "        \"\"\"专门对 (57)摘要 段做兜底去图（防止摘要内还有残留图片标记）。\"\"\"\n",
    "        sec = self._find_section(md_text, r\"\\(?57\\)?\\s*摘要\")\n",
    "        if not sec:\n",
    "            return md_text\n",
    "        sec_clean = re.sub(r'!\\[.*?\\]\\([^)]*\\)', '', sec).strip()\n",
    "        return md_text.replace(sec, sec_clean)\n",
    "\n",
    "    def _filter_trailing_image_blocks(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        删除文末连续的图块。\n",
    "        支持两种形式反复出现：\n",
    "          a) [图片] (+空行) + 图N...\n",
    "          b) 纯“图N...”行（没有配图）\n",
    "        \"\"\"\n",
    "        new_text = re.sub(\n",
    "            r'('\n",
    "            r'\\n\\s*(?:!\\[.*?\\]\\([^)]+\\)\\s*)?(?:\\n[ \\t]*)*图[0-9一二三四五六七八九十]+[^\\n]*\\s*'\n",
    "            r')+$',\n",
    "            '',\n",
    "            text,\n",
    "            flags=re.MULTILINE | re.IGNORECASE\n",
    "        ).rstrip() + \"\\n\"\n",
    "        return new_text\n",
    "    \n",
    "    # ============== 时间格式 ==============\n",
    "    def _normalize_date(self, s: str) -> str:\n",
    "        s = (s or \"\").strip()\n",
    "        if not s:\n",
    "            return \"\"\n",
    "        s = s.replace(\"年\", \".\").replace(\"月\", \".\").replace(\"日\", \"\")\n",
    "        s = s.replace(\"/\", \".\").replace(\"-\", \".\")\n",
    "        s = re.sub(r'\\.+', '.', s).strip(\".\")\n",
    "        return s\n",
    "\n",
    "    # ============== 元数据抽取（从『全文』提取，避免被裁剪掉） ==============\n",
    "    def _extract_meta_blocks(self, full_text: str) -> Dict[str, str]:\n",
    "        \n",
    "        # (12) 文献类型\n",
    "        m12 = re.search(r'\\(12\\)\\s*([^\\n]+)', full_text)\n",
    "        if m12:\n",
    "            doc_type = re.sub(r\"\\s+\", \"\", m12.group(1))\n",
    "            \n",
    "        \n",
    "        # (54) 标题：下一行是 title\n",
    "        m_title = re.search(r'(?m)^#\\s*\\(54\\)\\s*(?:实用新型|发明)\\s*名称\\s*\\n(.+)$', full_text)\n",
    "        title = (m_title.group(1).strip() if m_title else \"\")\n",
    "\n",
    "        # (21) 申请号\n",
    "        m_apply_no = re.search(r'\\(21\\)\\s*申请号\\s*([0-9]+(?:\\.[0-9A-Za-z])?)', full_text)\n",
    "        apply_no = (m_apply_no.group(1).strip() if m_apply_no else \"\")\n",
    "\n",
    "        # (22) 申请日：形如 2020.09.02 或 2020-09-02 或 2020年09月02日\n",
    "        m_apply = re.search(r'\\(22\\)\\s*申请日\\s*([0-9.\\-年月日/]+)', full_text)\n",
    "        apply_time = (m_apply.group(1).strip() if m_apply else \"\")\n",
    "        apply_time = apply_time.replace(\"年\",\".\").replace(\"月\",\".\").replace(\"日\",\"\").strip(\".\")\n",
    "\n",
    "        # (73) 专利权人  or (71)申请人   +   地址（可能连写：地址<邮编><地址>）   gg  \n",
    "        applicant, address = \"\", \"\"\n",
    "        # 统一从 (73)/(71) 段落中抽取，允许跨行直到下一个 (XX) 字段或文末\n",
    "        block_pat = re.compile(\n",
    "                r'\\((?P<code>71|73)\\)'                    # (71) 或 (73)\n",
    "                r'\\s*(?:申\\s*请\\s*人|专\\s*利\\s*权\\s*人)?'  # 可选“申请人/专利权人”字样\n",
    "                r'\\s*[:：]?\\s*'\n",
    "                r'(?P<name>.*?)'                          # 申请人/专利权人（非贪婪）\n",
    "                r'(?:地址|住\\s*址)\\s*[:：]?\\s*'            # 地址提示词\n",
    "                r'(?:(?P<zip>\\d{6})\\s*[，,、]?\\s*)?'      # 可选 6 位邮编\n",
    "                r'(?P<addr>[^\\(\\)\\r\\n]+)',                # 地址主体\n",
    "                flags=re.S | re.I\n",
    "            )\n",
    "        m = block_pat.search(full_text)\n",
    "        if m:\n",
    "            applicant = m.group('name').strip()\n",
    "            zip_code = m.group('zip') or ''\n",
    "            addr = m.group('addr').strip()\n",
    "            address = f\"{zip_code} {addr}\".strip()\n",
    "        \n",
    "        # # 2. 兜底：单行抓“申请人/专利权人”\n",
    "        # for tag in ('(71)', '(73)'):\n",
    "        #     m_name = re.search(rf'{re.escape(tag)}\\s*(?:申\\s*请\\s*人|专\\s*利\\s*权\\s*人)?\\s*[:：]?\\s*([^\\r\\n]+)', full_text, re.I)\n",
    "        #     if m_name:\n",
    "        #         applicant = m_name.group(1).strip()\n",
    "        #         break\n",
    "        \n",
    "        # # 3. 兜底：单行抓“地址”\n",
    "        # m_addr = re.search(r'(?:地址|住\\s*址)\\s*[:：]?\\s*(\\d{6})?[，,、]?\\s*([^\\r\\n]+)', full_text, re.I)\n",
    "        # if m_addr:\n",
    "        #     zip_code = m_addr.group(1) or ''\n",
    "        #     addr = m_addr.group(2).strip()\n",
    "        #     address = f\"{zip_code} {addr}\".strip()\n",
    "              \n",
    "\n",
    "        # (72) 发明人 or 设计人 → 多个人名，返回 ['xx', 'xx', ...]   --gg  解析还是有点问题\n",
    "        inventors_list = []\n",
    "        m_72 = re.search(\n",
    "            # 抓住 (72) 行后直到“下一字段头（形如 \\n(XX)）”或文末为止\n",
    "            r'(?ms)^\\(72\\)\\s*(?:发明人|设计人)\\s*(.+?)(?=\\n\\(\\d{2}\\)|\\Z)',\n",
    "            full_text\n",
    "        )\n",
    "        if m_72:\n",
    "            raw_inv = m_72.group(1)\n",
    "\n",
    "            # 统一空白（包含全角空格/换行等）为单空格\n",
    "            raw_inv = re.sub(r'\\s+', ' ', raw_inv).strip()\n",
    "\n",
    "            # 先把常见分隔符与连接词统一成 \"、\"\n",
    "            # 顿号/逗号/分号/斜杠 以及 “和/与/及” 两侧可能有空格\n",
    "            normalized = re.sub(r'\\s*(?:、|，|,|；|;|／|/|和|与|及)\\s*', '、', raw_inv)\n",
    "\n",
    "            # 关键：中文名之间仅以空格分隔的情况，也将这个空格视为一个分隔符\n",
    "            normalized = re.sub(r'(?<=[\\u4e00-\\u9fff])\\s+(?=[\\u4e00-\\u9fff])', '、', normalized)\n",
    "\n",
    "            # 分割并清洗\n",
    "            parts = [p.strip() for p in normalized.split('、') if p.strip()]\n",
    "\n",
    "            seen = set()\n",
    "            for p in parts:\n",
    "                # 去掉尾缀“等/等人/等等人”等\n",
    "                p = re.sub(r'(?:等(?:等)?(?:人)?)$', '', p)\n",
    "                if p and p not in seen:\n",
    "                    inventors_list.append(p)\n",
    "                    seen.add(p)\n",
    "\n",
    "        # 结果：列表形式\n",
    "        inventors = inventors_list\n",
    "\n",
    "        #  (43)/(45)/关键词“公开/公告/公布/授权公告(日|日期)” —— 取最先匹配到的\n",
    "        pub_date = \"\"\n",
    "        for pat in [\n",
    "            r'\\(45\\)\\s*授权公告日\\s*([0-9.\\-年月日/]+)',\n",
    "            r'\\(43\\)\\s*(?:公开|公告|公布)\\s*(?:日|日期)?\\s*([0-9.\\-年月日/]+)',\n",
    "            r'(?:授权公告|公开|公告|公布)\\s*(?:日|日期)\\s*[:：]?\\s*([0-9.\\-年月日/]+)',\n",
    "        ]:\n",
    "            m = re.search(pat, full_text)\n",
    "            if m:\n",
    "                pub_date = self._normalize_date(m.group(1))\n",
    "                break\n",
    "\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"apply_no\":apply_no,\n",
    "            \"apply_time\": apply_time,\n",
    "            \"applicant\": applicant,\n",
    "            \"address\": address,\n",
    "            \"inventors\": inventors,\n",
    "            \"publ_date\": pub_date,  \n",
    "            \"doc_type\":doc_type, \n",
    "        }\n",
    "    \n",
    "    # <公布号/公告号>  ->  授权 专利号\n",
    "    def MetaDict_norm(self):\n",
    "        publ_no = self.meta_schema.get(\"publ_no\", None)\n",
    "        if not publ_no: \n",
    "            # 打印版的PDF文件，没法\n",
    "            # 既然不知道 <公布号/公告号>， 也无法得到 patent_no\n",
    "            del self.meta_schema['patent_no']\n",
    "            return \n",
    "        \n",
    "        appl_no = self.meta_schema.get(\"apply_no\", None)\n",
    "        if not appl_no: \n",
    "            return \n",
    "        pub_kind = publ_no.strip()[-1]  # 取<公开号>最后一位\n",
    "        # A  发明专利申请，未授权， 无专利号\n",
    "        # U/C/S 实用新型/发明/外观设计授权, 授权， 有专利号\n",
    "        if pub_kind in ['U', 'C', 'S']:\n",
    "            patent_no = f\"ZL{appl_no}\"  # 授权，专利号=ZL申请号\n",
    "            self.meta_schema['patent_no'] = patent_no\n",
    "            self.meta_schema['is_granted'] = True \n",
    "        elif pub_kind == 'A':\n",
    "            del self.meta_schema['patent_no']\n",
    "        else:\n",
    "            del self.meta_schema['patent_no']\n",
    "        return \n",
    "    \n",
    "    def _is_empty(self, x) -> bool:\n",
    "        \"\"\"定义‘空’：None、空字符串、空的 dict/OrderedDict/list/tuple/set。\"\"\"\n",
    "        if x is None:\n",
    "            return True\n",
    "        if isinstance(x, str):\n",
    "            return x == \"\"        # 需要把全空白也当空的话：x.strip() == \"\"\n",
    "        if isinstance(x, (dict, OrderedDict, list, tuple, set)):\n",
    "            return len(x) == 0\n",
    "        return False  # False、0 等都不是“空”\n",
    "\n",
    "    def MetaDict_NoEmptyValue(self, x) -> OrderedDict:\n",
    "        \"\"\"\n",
    "        递归删除 dict / OrderedDict / list / tuple 中的『空』值。\n",
    "        空定义：None、\"\"、[]、{}（含 OrderedDict）；保留 False、0 等。\n",
    "        保持原容器类型（dict / OrderedDict / list / tuple）。\n",
    "        \"\"\"\n",
    "        \n",
    "        # 映射类型：保持原类型\n",
    "        if isinstance(x, (dict, OrderedDict)):\n",
    "            cls = OrderedDict if isinstance(x, OrderedDict) else dict\n",
    "            out = cls()\n",
    "            for k, v in x.items():\n",
    "                v2 = self.MetaDict_NoEmptyValue(v)           # 先递归清理\n",
    "                if not self._is_empty(v2):       # 再按清理后的结果决定是否保留\n",
    "                    out[k] = v2\n",
    "            return out\n",
    "        \n",
    "        # 列表\n",
    "        if isinstance(x, list):\n",
    "            out = []\n",
    "            for item in x:\n",
    "                i2 = self.MetaDict_NoEmptyValue(item)\n",
    "                if not self._is_empty(i2):\n",
    "                    out.append(i2)\n",
    "            return out\n",
    "\n",
    "        # 元组（可选：保持元组类型）\n",
    "        if isinstance(x, tuple):\n",
    "            out = []\n",
    "            for item in x:\n",
    "                i2 = self.MetaDict_NoEmptyValue(item)\n",
    "                if self._is_empty(i2):\n",
    "                    out.append(i2)\n",
    "            return tuple(out)\n",
    "\n",
    "        # 其他原子类型原样返回（False、0 会被保留）\n",
    "        return x\n",
    "        \n",
    "    # 提取指定标题段的“纯正文”（不含标题本身）\n",
    "    def _extract_section_plain_text(self, full_text: str, title_pattern: str, strip_para_tags: bool = False) -> str:\n",
    "        sec = self._find_section(full_text, title_pattern)\n",
    "        if not sec:\n",
    "            return \"\"\n",
    "        txt = sec.strip()\n",
    "        if strip_para_tags:\n",
    "            # 去掉每段行首的 [0001]、[0017] 等\n",
    "            txt = re.sub(r'(?m)^\\s*\\[\\d+\\]\\s*', '', txt)\n",
    "        return txt\n",
    "\n",
    "    # ============== PDF 公告号  ==============\n",
    "    def _extract_pubno(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        直接从 markdown 文本中提取公告号/授权公告号（如 CN110405743A、CN211234567U、CN123456789B1）。\n",
    "        提取顺序：\n",
    "        1) (11) 行（公开号/公告号/授权公告号）\n",
    "        2) 带“公告号/公开号/授权公告号”关键词的行\n",
    "        \"\"\"\n",
    "        text = self.text\n",
    "\n",
    "        # 1) (11) 行优先\n",
    "        m_11 = re.search(r'\\(11\\)[^\\n]*?(CN[0-9]{7,12}[A-Z0-9]?)', text, flags=re.IGNORECASE)\n",
    "        if m_11:\n",
    "            return {\"publ_no\": m_11.group(1).upper(), \"pdf_path\": self.pdfp or \"\"}\n",
    "\n",
    "        # 2) 关键词行（公告号/公开号/授权公告号）\n",
    "        m_kw = re.search(\n",
    "            r'(公告号|公布号|申请公布号|授权公告号)\\s*[:：]?\\s*(CN[0-9]{7,12}[A-Z0-9]?)',\n",
    "            text, flags=re.IGNORECASE\n",
    "        )\n",
    "        if m_kw:\n",
    "            return {\"publ_no\": m_kw.group(2).upper(), \"pdf_path\": self.pdfp or \"\"}\n",
    "\n",
    "        # 都没找到则置空\n",
    "        return {\"publ_no\": \"\", \"pdf_path\": self.pdfp or \"\"}\n",
    "    \n",
    "    # ============== 写文件 ==============\n",
    "    def _write_structured_md(self, content: str) -> Path:\n",
    "        self.out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        in_name = Path(self.markdown_file).stem\n",
    "        out_path = self.out_dir / f\"{in_name}_filtered.md\"\n",
    "        out_path.write_text(content, encoding=\"utf-8\")\n",
    "        \n",
    "        # self.meta_schema 写入json\n",
    "        json_path = self.out_dir / \"MetaDict_filtered.json\"\n",
    "        with open(json_path, \"w\", encoding='utf-8') as fj:\n",
    "            json.dump(self.meta_schema, fj, ensure_ascii=False, indent=4, default=str)\n",
    "        return out_path\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    root_dir = r\".log/SimplePDF\"\n",
    "    sub_dirs = list(Path(root_dir).glob(\"*/full.md\"))\n",
    "    print(len(sub_dirs))  # ok \n",
    "    for md in sub_dirs:\n",
    "        mdp = Path(md)\n",
    "        parsers = patentMD_parser(mdp)\n",
    "        doc = parsers()\n",
    "        print(parsers.meta_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "\n",
    "zhuanli_path_pdf = Path(r\".\\demo.pdf\")\n",
    "zhuanli_path_md = Path(r\".\\demo.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "\n",
    "pdfp = r\"demo.pdf\"  # 打印版的PDF    都不行\n",
    "\n",
    "\n",
    "# # 一般般\n",
    "reader = PdfReader(pdfp)\n",
    "text = \"\"\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text()\n",
    "    # print(text)   # 倒数第二行的样子\n",
    "    break\n",
    "\n",
    "\n",
    "# # # 一般般\n",
    "import fitz  # PyMuPDF\n",
    "doc = fitz.open(pdfp)\n",
    "for page in doc:\n",
    "    text = page.get_text()\n",
    "    # print(text)\n",
    "    break\n",
    "\n",
    "import pdfplumber\n",
    "with pdfplumber.open(str(pdfp)) as pdf:\n",
    "    first_page = pdf.pages[0]\n",
    "    textp = first_page.extract_text()\n",
    "print(textp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from typing import Dict \n",
    "from pathlib import Path \n",
    "\n",
    "def get_md_text(mdp):\n",
    "    return Path(mdp).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "\n",
    "def extract_pubno(mdp) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    直接从 markdown 文本中提取公告号/授权公告号（如 CN110405743A、CN211234567U、CN123456789B1）。\n",
    "    提取顺序：\n",
    "    1) (11) 行（公开号/公告号/授权公告号）\n",
    "    2) 带“公告号/公开号/授权公告号”关键词的行\n",
    "    3) 全文兜底扫描 CN + 数字 + 可选结尾字母（且不跟小数点，避免把申请号 CN2020...*.5 误判）\n",
    "    4) 文件名 / 目录名兜底扫描\n",
    "    \"\"\"\n",
    "    text = get_md_text(mdp)\n",
    "    \n",
    "    pdfp = next(Path(mdp).parent.glob(\"*_origin.pdf\"),None)\n",
    "\n",
    "    # 1) (11) 行优先\n",
    "    m_11 = re.search(r'\\(11\\)[^\\n]*?(CN[0-9]{7,12}[A-Z0-9]?)', text, flags=re.IGNORECASE)\n",
    "    if m_11:\n",
    "        return {\"pubno\": m_11.group(1).upper(), \"pdf_path\": pdfp or \"\"}\n",
    "\n",
    "    # 2) 关键词行（公告号|公布号|申请公布号|授权公告号）\n",
    "    m_kw = re.search(\n",
    "        r'(公告号|公布号|申请公布号|授权公告号)\\s*[:：]?\\s*(CN[0-9]{7,12}[A-Z0-9]?)',\n",
    "        text, flags=re.IGNORECASE\n",
    "    )\n",
    "    if m_kw:\n",
    "        return {\"pubno\": m_kw.group(2).upper(), \"pdf_path\": pdfp or \"\"}\n",
    "\n",
    "    # 3) 全文兜底扫描（避免匹配到带小数点的申请号：使用负向前瞻 (?!\\.) ）\n",
    "    m_any = re.search(r'(CN[0-9]{7,12}[A-Z0-9]?)(?!\\.)', text, flags=re.IGNORECASE)\n",
    "    if m_any:\n",
    "        return {\"pubno\": m_any.group(1).upper(), \"pdf_path\": pdfp or \"\"}\n",
    "\n",
    "    # 4) 文件名/目录名兜底   \n",
    "    candidates = [str(Path(mdp).name), str(Path(mdp).parent)]\n",
    "    for s in candidates:\n",
    "        m_fs = re.search(r'(CN[0-9]{7,12}[A-Z0-9]?)(?!\\.)', s, flags=re.IGNORECASE)\n",
    "        if m_fs:\n",
    "            return {\"pubno\": m_fs.group(1).upper(), \"pdf_path\": pdfp or \"\"}\n",
    "\n",
    "    # 都没找到则置空\n",
    "    return {\"pubno\": \"\", \"pdf_path\": pdfp or \"\"}\n",
    "mdp = r\"sucai\\CN202430526527.2-机械手（机器人灵巧手）.pdf-cf433862-c84e-4835-8712-c29434c3e6f5\\full.md\"\n",
    "extract_pubno(mdp)\n",
    "# 'pubno': 'CN20243052652',   这个不对，文件名中的 CN202430526527.2 是申请号，\n",
    "# apply_id这是需要添加到MedaDict中的\n",
    "# 可以尝试用 from pypdf import PdfReader  PdfReader读取第一页pdf然后匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_granted(patent):\n",
    "    pub_kind = patent['publ_no'].strip()[-1]  # 取<公开号>最后一位\n",
    "    # A  发明专利申请，未授权， 无专利号\n",
    "    # U/C/S 实用新型/发明/外观设计授权, 授权， 有专利号\n",
    "    if pub_kind in ['U', 'C', 'S']:\n",
    "        return True, f\"ZL {patent['appl_no']}\"  # 授权，专利号=ZL申请号\n",
    "    elif pub_kind == 'A':\n",
    "        return False, None  # 未授权，无专利号\n",
    "    else:\n",
    "        return None, None   # 未授权\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 申请公布号： 文件名中获取 or 正文部分匹配 （CN + 申请号， (21)申请号 202411146096.2） ） or  PdfReader读取pdf的第一页\n",
    "# 申请公布日： 正文部分匹配（(43)申请公布日 2024.10.11）  or  PdfReader读取pdf的第一页\n",
    "\n",
    "# PdfReader读取pdf的第一页 会得到这样的结果：\n",
    "\"\"\"   \n",
    "(19)中华人民共和国国家知识产权局\n",
    "(12)实用新型专利\n",
    "(10)授权公告号 \n",
    "(45)授权公告日 \n",
    "(21)申请号 202021894937.5\n",
    "(22)申请日 2020.09.02\n",
    "(73)专利权人 杭州宇树科技有限公司\n",
    "地址 310053 浙江省杭州市滨江区西兴街\n",
    "道东流路88号1幢306室\n",
    "(72)发明人 王兴兴　\n",
    "(74)专利代理机构 浙江翔隆专利事务所(普通\n",
    "合伙) 33206\n",
    "代理人 许守金\n",
    "(51)Int.Cl.\n",
    "B25J 5/00(2006.01)\n",
    "B25J 9/10(2006.01)\n",
    "B62D 57/032(2006.01)\n",
    " \n",
    "(54)实用新型名称\n",
    "一种结构紧凑的回转动力单元以及应用其\n",
    "的机器人\n",
    "(57)摘要\n",
    "本实用新型公开了一种结构紧凑的回转动\n",
    "力单元以及应用其的机器人，属于动力单元以及\n",
    "机器人技术领域。现有回转单元方案，需要在输\n",
    "...\n",
    "CN 213034612 U\n",
    "2021.04.23\n",
    "CN 213034612 U\n",
    "\"\"\"\n",
    "# 末尾处就是我们期待的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "mdp = r\"sucai\\CN202430526527.2-机械手（机器人灵巧手）.pdf-cf433862-c84e-4835-8712-c29434c3e6f5\\full.md\"\n",
    "pdfp = next(Path(mdp).parent.glob(\"*_origin.pdf\"), None) # 202021894937.5\n",
    "assert pdfp is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from pathlib import Path \n",
    "mdp = r\"D:\\ddesktop\\agentdemos\\codespace\\zhuanliParser\\result\\CN202130119955.X-机器人足端.pdf-b78a68ee-f572-4c41-8774-00c792a7761e\\full.md\"\n",
    "pdfp = next(Path(mdp).parent.glob(\"*_origin.pdf\"), None) \n",
    "# print(pdfp)\n",
    "assert Path(pdfp).is_file() is True \n",
    "with pdfplumber.open(str(pdfp)) as pdf:\n",
    "    first_page = pdf.pages[0]\n",
    "    textp = first_page.extract_text()\n",
    "print(textp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pdfplumber, re\n",
    "pdfp = r\"demo.pdf\"\n",
    "\n",
    "def pdfplumber_extract_publno(pdfp)->str:\n",
    "    with pdfplumber.open(str(pdfp)) as pdf:\n",
    "        first_page = pdf.pages[0]\n",
    "        textp = first_page.extract_text()\n",
    "    print(f\"{textp=}\")\n",
    "    m_kw = re.search(\n",
    "    r'(公告号|公布号|申请公布号|授权公告号)\\s*[:：]?\\s*(CN[0-9]{7,12}[A-Z0-9]?)',\n",
    "    textp, flags=re.IGNORECASE)\n",
    "    if m_kw:\n",
    "        publno = m_kw.group(2).upper()\n",
    "        return publno\n",
    "    return None \n",
    "pdfplumber_extract_publno(pdfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "import pdfplumber\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def parse_PdfFontPage(pdf_path: str|Path) -> OrderedDict:\n",
    "    \"\"\" \n",
    "    用pdfplumber读取 专利pdf 的第一页，解析\n",
    "    OrderedDict({\n",
    "         \"publ_no\":   <(10) 公开号/授权公告号>,\n",
    "         \"publ_date\": <(45)/(43) 公告/公开日>,\n",
    "         \"doc_type\":  <(12) 文献类型，如“实用新型专利/发明专利申请/外观设计专利”>\n",
    "      })\n",
    "    \"\"\"\n",
    "    pdf_path= Path(pdf_path)\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        if not pdf.pages:\n",
    "            return OrderedDict(publ_no=\"\", publ_date=\"\", doc_type=\"\")\n",
    "        text = pdf.pages[0].extract_text() or \"\"\n",
    "\n",
    "    # 规范化常见字符\n",
    "    norm = (text or \"\").replace(\"：\", \":\").replace(\"\\u3000\", \" \")\n",
    "    \n",
    "    \n",
    "    def _norm_date(s: str) -> str:\n",
    "        s = (s.strip()\n",
    "               .replace(\"年\", \".\").replace(\"月\", \".\").replace(\"日\", \"\")\n",
    "               .replace(\"/\", \".\").replace(\"-\", \".\"))\n",
    "        parts = [p for p in s.split(\".\") if p]\n",
    "        if len(parts) >= 3:\n",
    "            y, m, d = parts[0], parts[1], parts[2]\n",
    "            return f\"{y}.{m.zfill(2)}.{d.zfill(2)}\"\n",
    "        return s\n",
    "    \n",
    "    def _clean_cn_no(s: str) -> str:\n",
    "        s = re.sub(r\"\\s+\", \"\", s.upper())\n",
    "        s = s if s.startswith(\"CN\") else \"CN\" + s\n",
    "        return re.sub(r\"[^A-Z0-9]+$\", \"\", s)\n",
    "\n",
    "    def _find1(pat: str, txt: str, flags=0) -> str:\n",
    "        m = re.search(pat, txt, flags)\n",
    "        return m.group(1).strip() if m else \"\"\n",
    "\n",
    "    # ---- (10) 公开号/授权公告号 ----\n",
    "    publ_no = \"\"\n",
    "    for pat in [\n",
    "        r\"\\(10\\)\\s*(?:授权公告号|公告号|公开号|申请公布号)\\s*([A-Z]{0,2}\\s*\\d{6,12}\\s*[A-Z0-9]?)\",\n",
    "        r\"\\(10\\)[^\\n]*?(CN\\s*\\d{6,12}\\s*[A-Z0-9]?)\",\n",
    "    ]:\n",
    "        v = _find1(pat, norm, flags=re.IGNORECASE)\n",
    "        if v:\n",
    "            publ_no = _clean_cn_no(v)\n",
    "            break\n",
    "\n",
    "    # ---- (45)/(43) 公告/公开日 ----\n",
    "    publ_date = \"\"\n",
    "    for pat in [\n",
    "        r\"\\(45\\)\\s*(?:授权公告日|公告日)\\s*([0-9.\\-年月日/]+)\",\n",
    "        r\"\\(43\\)\\s*(?:公开|公告|公布|申请公布)\\s*(?:日|日期)?\\s*([0-9.\\-年月日/]+)\",\n",
    "        r\"(?:授权公告|公开|公告|公布|申请公布)\\s*(?:日|日期)\\s*[:：]?\\s*([0-9.\\-年月日/]+)\",\n",
    "    ]:\n",
    "        v = _find1(pat, norm)\n",
    "        if v:\n",
    "            publ_date = _norm_date(v)\n",
    "            break\n",
    "\n",
    "    # ---- (12) 文献类型 ----\n",
    "    doc_type = \"\"\n",
    "    # 常见是“(12)实用新型专利 / (12)发明专利申请 / (12)外观设计专利”\n",
    "    v = _find1(r\"\\(12\\)\\s*([^\\n]+)\", norm)\n",
    "    if v:\n",
    "        # 简单清洗：去两端空格、多余空白\n",
    "        doc_type = re.sub(r\"\\s+\", \"\", v)\n",
    "\n",
    "    # ---- 兜底：页末“CN …\\nYYYY.MM.DD”版式 ----\n",
    "    if not publ_no:\n",
    "        cns = re.findall(r\"\\bCN\\s*\\d{6,12}\\s*[A-Z0-9]?\\b\", norm, flags=re.IGNORECASE)\n",
    "        if cns:\n",
    "            publ_no = _clean_cn_no(cns[-1])\n",
    "\n",
    "    if not publ_date:\n",
    "        dates = re.findall(r\"\\b\\d{4}[./-]\\d{1,2}[./-]\\d{1,2}\\b\", norm)\n",
    "        if dates:\n",
    "            publ_date = _norm_date(dates[-1])\n",
    "\n",
    "    return OrderedDict(publ_no=publ_no, publ_date=publ_date, doc_type=doc_type)\n",
    "\n",
    "root_dir = r\"D:\\ddesktop\\agentdemos\\codespace\\zhuanliParser\\result\"\n",
    "sub_dirs = list(Path(root_dir).glob(\"*/full.md\"))\n",
    "print(len(sub_dirs))  # ok \n",
    "for md in sub_dirs:\n",
    "    mdp = Path(md)\n",
    "    pdfp = next(mdp.parent.glob(\"*_origin.pdf\"),None)\n",
    "    assert pdfp is not None \n",
    "    xx = parse_PdfFontPage(pdfp)\n",
    "    print(xx)\n",
    "\n",
    "\n",
    "# parse_PdfFontPage(pdfp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "doc = fitz.open(pdfp)\n",
    "page = doc[0]\n",
    "text = page.get_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_FirstPdfPageText(pdf_path: str):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "        # print(text)   # 倒数第二行的样子\n",
    "        break\n",
    "    return text\n",
    " \n",
    "\n",
    "\n",
    "def _normalize_date(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = s.replace(\"年\", \".\").replace(\"月\", \".\").replace(\"日\", \"\")\n",
    "    s = s.replace(\"/\", \".\").replace(\"-\", \".\")\n",
    "    parts = [p for p in s.split(\".\") if p]\n",
    "    if len(parts) >= 3:\n",
    "        y, m, d = parts[0], parts[1], parts[2]\n",
    "        # 统一补零\n",
    "        m = m.zfill(2)\n",
    "        d = d.zfill(2)\n",
    "        return f\"{y}.{m}.{d}\"\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def extract_pubno_from_text(textIn: str) -> str:\n",
    "    # 关键处常见格式： 'CN 213034612 U' or 'CN213034612U'\n",
    "    m = re.search(r'(CN\\s*\\d{7,12}\\s*[A-Z0-9]?)', textIn, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return re.sub(r'\\s+', '', m.group(1).upper())\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_pubdate_from_text(textIn: str) -> str:\n",
    "    # 先关键字段\n",
    "    for pat in [\n",
    "        r'\\(45\\)\\s*授权公告日\\s*([0-9.\\-年月日/]+)',\n",
    "        r'\\(43\\)\\s*(?:公开|公告|公布|申请公布)\\s*(?:日|日期)?\\s*([0-9.\\-年月日/]+)',\n",
    "        r'(?:授权公告|公开|公告|公布|申请公布)\\s*(?:日|日期)\\s*[:：]?\\s*([0-9.\\-年月日/]+)',\n",
    "    ]:\n",
    "        m = re.search(pat, textIn)\n",
    "        if m:\n",
    "            return _normalize_date(m.group(1))\n",
    "\n",
    "    # 兜底：抓所有“像日期”的，取时间上最大的一个\n",
    "    candidates = re.findall(r'(20\\d{2}[.\\-/年]\\d{1,2}[.\\-/月]\\d{1,2})', textIn)\n",
    "    if not candidates:\n",
    "        return \"\"\n",
    "    def _to_key(s: str):\n",
    "        norm = _normalize_date(s)  # YYYY.MM.DD\n",
    "        mm = re.match(r'(\\d{4})\\.(\\d{1,2})\\.(\\d{1,2})$', norm)\n",
    "        if not mm:\n",
    "            return (0,0,0)\n",
    "        return (int(mm.group(1)), int(mm.group(2)), int(mm.group(3)))\n",
    "    best = max(candidates, key=_to_key)\n",
    "    return _normalize_date(best)\n",
    "\n",
    "\n",
    "\n",
    "# texts = get_md_text(mdp)\n",
    "# extract_pubno_from_text(texts)  # ''\n",
    "\n",
    "texts = get_FirstPdfPageText(pdfp)\n",
    "print(texts)\n",
    "extract_pubno_from_text(texts)  # '' \n",
    "\n",
    " \n",
    "# extract_pubdate_from_text(texts)  # ok\n",
    "\n",
    "\n",
    "## ok\n",
    "# def extract_applyno_from_fname(md_path: str):\n",
    "#     sub_root_name = Path(md_path).parent.name\n",
    "#     applyno = sub_root_name.split('-')[0]\n",
    "#     print(applyno)\n",
    "    \n",
    "# extract_applyno_from_fname(md_path=mdp)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import logging, os\n",
    "from typing import List\n",
    "\n",
    "from llama_index.core import Document, VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser, SentenceSplitter\n",
    "from llama_index.core.ingestion import run_transformations\n",
    "from llama_index.core.schema import BaseNode, MetadataMode, NodeWithScore\n",
    "\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.readers.file.markdown import MarkdownReader\n",
    "import chromadb\n",
    "\n",
    "# ---------- logging ----------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s.%(msecs)03d [%(levelname)-8s] %(name)25s - %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(\"patent_parser_min\")\n",
    "\n",
    "# ---------- paths ----------\n",
    "PERSIST_DIR = Path(\"../temp\").resolve()\n",
    "PERSIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHROMA_DIR = PERSIST_DIR / \"chroma_db\"\n",
    "\n",
    "# ---------- components ----------\n",
    "def build_embedding():\n",
    "    return HuggingFaceEmbedding(model_name=\"Qwen/Qwen3-Embedding-0.6B\")\n",
    "\n",
    "def build_vector_store():\n",
    "    client = chromadb.PersistentClient(path=str(CHROMA_DIR))\n",
    "    collection = client.get_or_create_collection(\"patents\")\n",
    "    return ChromaVectorStore(chroma_collection=collection)\n",
    "\n",
    "def read_file_to_documents(path: Path) -> List[Document]:\n",
    "    ext = path.suffix.lower()\n",
    "    if ext == \".pdf\":\n",
    "        return PyMuPDFReader().load_data(str(path))\n",
    "    if ext == \".md\":\n",
    "        return MarkdownReader().load_data(str(path))\n",
    "    # 兜底：按纯文本\n",
    "    text = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    return [Document(text=text, metadata={\"file_name\": path.name})]\n",
    "\n",
    "def show_nodes(nodes: List[BaseNode], max_n: int = 5):\n",
    "    print(\"\\n=== Sample BaseNodes ===\")\n",
    "    for i, n in enumerate(nodes[:max_n], 1):\n",
    "        preview = n.get_content()[:160].replace(\"\\n\", \" \")\n",
    "        print(f\"[{i}] node_id={n.node_id}  ref_doc_id={n.ref_doc_id}\")\n",
    "        print(\"    text:\", preview, \"...\" if len(n.get_content()) > 160 else \"\")\n",
    "        print(\"    meta:\", {k: n.metadata.get(k) for k in list(n.metadata)[:6]})\n",
    "        # 邻接\n",
    "        print(\"    prev:\", getattr(n.prev_node, \"node_id\", None), \"  next:\", getattr(n.next_node, \"node_id\", None))\n",
    "\n",
    "def main(input_path: str, query: str | None = None, use_window: bool = True):\n",
    "    path = Path(input_path).resolve()\n",
    "    assert path.exists(), f\"File not found: {path}\"\n",
    "\n",
    "    # 1) 读取文档\n",
    "    docs = read_file_to_documents(path)\n",
    "    for d in docs:\n",
    "        d.metadata.setdefault(\"file_name\", path.name)\n",
    "        d.metadata[\"doc_id\"] = d.doc_id\n",
    "        # 与 private-gpt 一致：不把这些元数据送进 embedding / LLM\n",
    "        d.excluded_embed_metadata_keys = [\"doc_id\"]\n",
    "        d.excluded_llm_metadata_keys = [\"file_name\", \"doc_id\", \"page_label\"]\n",
    "\n",
    "    # 2) 组件\n",
    "    embed = build_embedding()\n",
    "    vs = build_vector_store()\n",
    "    storage = StorageContext.from_defaults(vector_store=vs)\n",
    "\n",
    "    # 3) 选择分块器\n",
    "    if use_window:\n",
    "        # 与 private-gpt 默认一致：句子 + window（后续对话可用 MetadataReplacementPostProcessor(\"window\")）\n",
    "        parser = SentenceWindowNodeParser.from_defaults()  # window_size 默认 3\n",
    "    else:\n",
    "        # 定长切片示例\n",
    "        parser = SentenceSplitter.from_defaults(chunk_size=600, chunk_overlap=200)\n",
    "\n",
    "    # 4) 先直接跑 transformations 看看“裸的 Node”\n",
    "    nodes = list(run_transformations(docs, [parser, embed], show_progress=True))\n",
    "    show_nodes(nodes, max_n=5)\n",
    "\n",
    "    # 5) 写索引 + 向量库（并持久化）\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        docs,\n",
    "        storage_context=storage,\n",
    "        embed_model=embed,\n",
    "        transformations=[parser],   # 也可只给 parser，embedding 由 embed_model 负责\n",
    "        show_progress=True,\n",
    "    )\n",
    "    index.storage_context.persist(persist_dir=str(PERSIST_DIR))\n",
    "    print(f\"\\n✅ Persisted to: {PERSIST_DIR}\")\n",
    "\n",
    "    # 6) 即时检索（看 NodeWithScore）\n",
    "    if query:\n",
    "        retriever = index.as_retriever(similarity_top_k=5)\n",
    "        results: List[NodeWithScore] = retriever.retrieve(query)\n",
    "        print(\"\\n=== Top matches ===\")\n",
    "        for r in results:\n",
    "            txt = r.node.get_content()[:120].replace(\"\\n\", \" \")\n",
    "            print(f\"score={round(r.score or 0, 3)}  node_id={r.node.node_id}  -> {txt} ...\")\n",
    "\n",
    "    # 7) 模拟“重新加载后”查看 DocStore 里的节点\n",
    "    storage2 = StorageContext.from_defaults(vector_store=vs)\n",
    "    _ = load_index_from_storage(storage2, persist_dir=str(PERSIST_DIR))\n",
    "    ref_infos = storage2.docstore.get_all_ref_doc_info() or {}\n",
    "    print(f\"\\n=== DocStore has {len(ref_infos)} ref docs ===\")\n",
    "    for doc_id, info in list(ref_infos.items())[:3]:\n",
    "        print(\"ref_doc:\", doc_id, \"  meta keys:\", list((info.metadata or {}).keys()))\n",
    "        # 如果你的 LlamaIndex 版本携带 node_ids，可以尝试：\n",
    "        node_ids = getattr(info, \"node_ids\", None)\n",
    "        if node_ids:\n",
    "            some_nodes = storage2.docstore.get_nodes(node_ids[:3])\n",
    "            print(\"  sample nodes:\", [n.node_id for n in some_nodes])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    inp = r\"./demo.md\"\n",
    "    q = \"解析一下专利的摘要\"\n",
    "    main(inp, q)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama-index 解析 专利pdf、专利md（MinerU解析得到的md）\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Any, List, Tuple, Union, Optional \n",
    "import abc,  itertools, multiprocessing, multiprocessing.pool, threading\n",
    "from injector import inject, Injector, singleton \n",
    "from queue import Queue\n",
    "\n",
    "\n",
    "from llama_index.core.data_structs import IndexDict\n",
    "from llama_index.core.embeddings.utils import EmbedType\n",
    "from llama_index.core.indices import VectorStoreIndex, load_index_from_storage\n",
    "from llama_index.core.indices.base import BaseIndex\n",
    "from llama_index.core.ingestion import run_transformations\n",
    "from llama_index.core.schema import BaseNode, Document, TransformComponent\n",
    "from llama_index.core.storage import StorageContext\n",
    "from llama_index.core.readers import StringIterableReader\n",
    "from llama_index.core.readers.base import BaseReader\n",
    "from llama_index.core.readers.json import JSONReader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import logging\n",
    "# Set to 'DEBUG' to have extensive logging turned on, even for libraries\n",
    "ROOT_LOG_LEVEL = \"INFO\"\n",
    "PRETTY_LOG_FORMAT = (\n",
    "    \"%(asctime)s.%(msecs)03d [%(levelname)-8s] %(name)+25s - %(message)s\"\n",
    ")\n",
    "logging.basicConfig(level=ROOT_LOG_LEVEL, format=PRETTY_LOG_FORMAT, datefmt=\"%H:%M:%S\")\n",
    "logging.captureWarnings(True)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# setting\n",
    "local_data_path = zhuanli_path_pdf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# file loader    【\"zhuanli.pdf\", \"zhuanli.md\"】 \"image.jpg\"\n",
    "def _try_loading_included_file_formats() -> dict[str, type[BaseReader]]:\n",
    "    try:\n",
    "        from llama_index.readers.file.docs import PDFReader\n",
    "        from llama_index.readers.file.markdown import MarkdownReader\n",
    "        from llama_index.readers.file.image import ImageReader  # type: ignore\n",
    "    except ImportError as e: raise ImportError(\"import error\")\n",
    "    \n",
    "    default_file_reader_cls: dict[str, type[BaseReader]] = {\n",
    "        \".pdf\": PDFReader,\n",
    "        \".md\": MarkdownReader,\n",
    "        \".jpg\": ImageReader,\n",
    "        \".png\": ImageReader,\n",
    "        \".jpeg\": ImageReader,\n",
    "    }\n",
    "    return default_file_reader_cls\n",
    "file_reader_cls = _try_loading_included_file_formats()\n",
    "file_reader_cls.update({\".json\": JSONReader})\n",
    "class IngestionHelper:\n",
    "    \"\"\"Helper class to transform a file into a list of documents.\n",
    "\n",
    "    This class should be used to transform a file into a list of documents.\n",
    "    These methods are thread-safe (and multiprocessing-safe).\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def transform_file_into_documents(\n",
    "        file_name: str, file_data: Path\n",
    "    ) -> list[Document]:\n",
    "        documents = IngestionHelper._load_file_to_documents(file_name, file_data)\n",
    "        for document in documents:\n",
    "            document.metadata[\"file_name\"] = file_name\n",
    "        IngestionHelper._exclude_metadata(documents)\n",
    "        return documents\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_file_to_documents(file_name: str, file_data: Path) -> list[Document]:\n",
    "        logger.debug(\"Transforming file_name=%s into documents\", file_name)\n",
    "        extension = Path(file_name).suffix\n",
    "        reader_cls = file_reader_cls.get(extension)\n",
    "        if reader_cls is None:\n",
    "            logger.debug(\n",
    "                \"No reader found for extension=%s, using default string reader\",\n",
    "                extension,\n",
    "            )\n",
    "            # Read as a plain text\n",
    "            string_reader = StringIterableReader()\n",
    "            return string_reader.load_data([file_data.read_text()])\n",
    "\n",
    "        logger.debug(\"Specific reader found for extension=%s\", extension)\n",
    "        documents = reader_cls().load_data(file_data)\n",
    "\n",
    "        # Sanitize NUL bytes in text which can't be stored in Postgres\n",
    "        for i in range(len(documents)):\n",
    "            documents[i].text = documents[i].text.replace(\"\\u0000\", \"\")\n",
    "\n",
    "        return documents\n",
    "\n",
    "    @staticmethod\n",
    "    def _exclude_metadata(documents: list[Document]) -> None:\n",
    "        logger.debug(\"Excluding metadata from count=%s documents\", len(documents))\n",
    "        for document in documents:\n",
    "            document.metadata[\"doc_id\"] = document.doc_id\n",
    "            # We don't want the Embeddings search to receive this metadata\n",
    "            document.excluded_embed_metadata_keys = [\"doc_id\"]\n",
    "            # We don't want the LLM to receive these metadata in the context\n",
    "            document.excluded_llm_metadata_keys = [\"file_name\", \"doc_id\", \"page_label\"]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class IngestedDoc(BaseModel):\n",
    "    object: Literal[\"ingest.document\"]\n",
    "    doc_id: str = Field(examples=[\"c202d5e6-7b69-4869-81cc-dd574ee8ee11\"])\n",
    "    doc_metadata: dict[str, Any] | None = Field(\n",
    "        examples=[\n",
    "            {\n",
    "                \"page_label\": \"2\",\n",
    "                \"file_name\": \"Sales Report Q3 2023.pdf\",\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    @staticmethod\n",
    "    def curate_metadata(metadata: dict[str, Any]) -> dict[str, Any]:\n",
    "        \"\"\"Remove unwanted metadata keys.\"\"\"\n",
    "        for key in [\"doc_id\", \"window\", \"original_text\"]:\n",
    "            metadata.pop(key, None)\n",
    "        return metadata\n",
    "\n",
    "    @staticmethod\n",
    "    def from_document(document: Document) -> \"IngestedDoc\":\n",
    "        return IngestedDoc(\n",
    "            object=\"ingest.document\",\n",
    "            doc_id=document.doc_id,\n",
    "            doc_metadata=IngestedDoc.curate_metadata(document.metadata),\n",
    "        )\n",
    "\n",
    "class BaseIngestComponent(abc.ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        storage_context: StorageContext,\n",
    "        embed_model: EmbedType,\n",
    "        transformations: list[TransformComponent],\n",
    "        *args: Any,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        logger.debug(\"Initializing base ingest component type=%s\", type(self).__name__)\n",
    "        self.storage_context = storage_context\n",
    "        self.embed_model = embed_model\n",
    "        self.transformations = transformations\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def ingest(self, file_name: str, file_data: Path) -> list[Document]:\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def bulk_ingest(self, files: list[tuple[str, Path]]) -> list[Document]:\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def delete(self, doc_id: str) -> None:\n",
    "        pass\n",
    "\n",
    "class BaseIngestComponentWithIndex(BaseIngestComponent, abc.ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        storage_context: StorageContext,\n",
    "        embed_model: EmbedType,\n",
    "        transformations: list[TransformComponent],\n",
    "        *args: Any,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(storage_context, embed_model, transformations, *args, **kwargs)\n",
    "\n",
    "        self.show_progress = True\n",
    "        self._index_thread_lock = (\n",
    "            threading.Lock()\n",
    "        )  # Thread lock! Not Multiprocessing lock\n",
    "        self._index = self._initialize_index()\n",
    "\n",
    "    def _initialize_index(self) -> BaseIndex[IndexDict]:\n",
    "        \"\"\"Initialize the index from the storage context.\"\"\"\n",
    "        try:\n",
    "            # Load the index with store_nodes_override=True to be able to delete them\n",
    "            index = load_index_from_storage(\n",
    "                storage_context=self.storage_context,\n",
    "                store_nodes_override=True,  # Force store nodes in index and document stores\n",
    "                show_progress=self.show_progress,\n",
    "                embed_model=self.embed_model,\n",
    "                transformations=self.transformations,\n",
    "            )\n",
    "        except ValueError:\n",
    "            # There are no index in the storage context, creating a new one\n",
    "            logger.info(\"Creating a new vector store index\")\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                [],\n",
    "                storage_context=self.storage_context,\n",
    "                store_nodes_override=True,  # Force store nodes in index and document stores\n",
    "                show_progress=self.show_progress,\n",
    "                embed_model=self.embed_model,\n",
    "                transformations=self.transformations,\n",
    "            )\n",
    "            index.storage_context.persist(persist_dir=local_data_path)\n",
    "        return index\n",
    "\n",
    "    def _save_index(self) -> None:\n",
    "        self._index.storage_context.persist(persist_dir=local_data_path)\n",
    "\n",
    "    def delete(self, doc_id: str) -> None:\n",
    "        with self._index_thread_lock:\n",
    "            # Delete the document from the index\n",
    "            self._index.delete_ref_doc(doc_id, delete_from_docstore=True)\n",
    "\n",
    "            # Save the index\n",
    "            self._save_index()\n",
    "\n",
    "class BatchIngestComponent(BaseIngestComponentWithIndex):\n",
    "    \"\"\"Parallelize the file reading and parsing on multiple CPU core.\n",
    "\n",
    "    This also makes the embeddings to be computed in batches (on GPU or CPU).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        storage_context: StorageContext,\n",
    "        embed_model: EmbedType,\n",
    "        transformations: list[TransformComponent],\n",
    "        count_workers: int,\n",
    "        *args: Any,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(storage_context, embed_model, transformations, *args, **kwargs)\n",
    "        # Make an efficient use of the CPU and GPU, the embedding\n",
    "        # must be in the transformations\n",
    "        assert (\n",
    "            len(self.transformations) >= 2\n",
    "        ), \"Embeddings must be in the transformations\"\n",
    "        assert count_workers > 0, \"count_workers must be > 0\"\n",
    "        self.count_workers = count_workers\n",
    "\n",
    "        self._file_to_documents_work_pool = multiprocessing.Pool(\n",
    "            processes=self.count_workers\n",
    "        )\n",
    "\n",
    "    def ingest(self, file_name: str, file_data: Path) -> list[Document]:\n",
    "        logger.info(\"Ingesting file_name=%s\", file_name)\n",
    "        documents = IngestionHelper.transform_file_into_documents(file_name, file_data)\n",
    "        logger.info(\n",
    "            \"Transformed file=%s into count=%s documents\", file_name, len(documents)\n",
    "        )\n",
    "        logger.debug(\"Saving the documents in the index and doc store\")\n",
    "        return self._save_docs(documents)\n",
    "\n",
    "    def bulk_ingest(self, files: list[tuple[str, Path]]) -> list[Document]:\n",
    "        documents = list(\n",
    "            itertools.chain.from_iterable(\n",
    "                self._file_to_documents_work_pool.starmap(\n",
    "                    IngestionHelper.transform_file_into_documents, files\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        logger.info(\n",
    "            \"Transformed count=%s files into count=%s documents\",\n",
    "            len(files),\n",
    "            len(documents),\n",
    "        )\n",
    "        return self._save_docs(documents)\n",
    "\n",
    "    def _save_docs(self, documents: list[Document]) -> list[Document]:\n",
    "        logger.debug(\"Transforming count=%s documents into nodes\", len(documents))\n",
    "        nodes = run_transformations(\n",
    "            documents,  # type: ignore[arg-type]\n",
    "            self.transformations,\n",
    "            show_progress=self.show_progress,\n",
    "        )\n",
    "        # Locking the index to avoid concurrent writes\n",
    "        with self._index_thread_lock:\n",
    "            logger.info(\"Inserting count=%s nodes in the index\", len(nodes))\n",
    "            self._index.insert_nodes(nodes, show_progress=True)\n",
    "            for document in documents:\n",
    "                self._index.docstore.set_document_hash(\n",
    "                    document.get_doc_id(), document.hash\n",
    "                )\n",
    "            logger.debug(\"Persisting the index and nodes\")\n",
    "            # persist the index and nodes\n",
    "            self._save_index()\n",
    "            logger.debug(\"Persisted the index and nodes\")\n",
    "        return documents\n",
    "\n",
    "\n",
    "\n",
    "def get_ingestion_component(\n",
    "    storage_context: StorageContext,\n",
    "    embed_model: EmbedType,\n",
    "    transformations: list[TransformComponent],\n",
    "    settings: Settings,\n",
    ") -> BaseIngestComponent:\n",
    "    \"\"\"Get the ingestion component for the given configuration.\"\"\"\n",
    "    ingest_mode = settings.embedding.ingest_mode\n",
    "    if ingest_mode == \"batch\":\n",
    "        return BatchIngestComponent(\n",
    "            storage_context=storage_context,\n",
    "            embed_model=embed_model,\n",
    "            transformations=transformations,\n",
    "            count_workers=settings.embedding.count_workers,\n",
    "        )\n",
    "\n",
    "    elif ingest_mode == \"pipeline\":\n",
    "        return PipelineIngestComponent(\n",
    "            storage_context=storage_context,\n",
    "            embed_model=embed_model,\n",
    "            transformations=transformations,\n",
    "            count_workers=settings.embedding.count_workers,\n",
    "        )\n",
    "    else:\n",
    "        return SimpleIngestComponent(\n",
    "            storage_context=storage_context,\n",
    "            embed_model=embed_model,\n",
    "            transformations=transformations,\n",
    "        )\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "@singleton\n",
    "class IngestService:\n",
    "    @inject\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_component: LLMComponent,\n",
    "        vector_store_component: VectorStoreComponent,\n",
    "        embedding_component: EmbeddingComponent,\n",
    "        node_store_component: NodeStoreComponent,\n",
    "    ) -> None:\n",
    "        self.llm_service = llm_component\n",
    "        self.storage_context = StorageContext.from_defaults(\n",
    "            vector_store=vector_store_component.vector_store,\n",
    "            docstore=node_store_component.doc_store,\n",
    "            index_store=node_store_component.index_store,\n",
    "        )\n",
    "        node_parser = SentenceWindowNodeParser.from_defaults()\n",
    "\n",
    "        self.ingest_component = get_ingestion_component(\n",
    "            self.storage_context,\n",
    "            embed_model=embedding_component.embedding_model,\n",
    "            transformations=[node_parser, embedding_component.embedding_model],\n",
    "            settings=settings(),\n",
    "        )\n",
    "\n",
    "    def _ingest_data(self, file_name: str, file_data: AnyStr) -> list[IngestedDoc]:\n",
    "        logger.debug(\"Got file data of size=%s to ingest\", len(file_data))\n",
    "        # llama-index mainly supports reading from files, so\n",
    "        # we have to create a tmp file to read for it to work\n",
    "        # delete=False to avoid a Windows 11 permission error.\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tmp:\n",
    "            try:\n",
    "                path_to_tmp = Path(tmp.name)\n",
    "                if isinstance(file_data, bytes):\n",
    "                    path_to_tmp.write_bytes(file_data)\n",
    "                else:\n",
    "                    path_to_tmp.write_text(str(file_data))\n",
    "                return self.ingest_file(file_name, path_to_tmp)\n",
    "            finally:\n",
    "                tmp.close()\n",
    "                path_to_tmp.unlink()\n",
    "\n",
    "    def ingest_file(self, file_name: str, file_data: Path) -> list[IngestedDoc]:\n",
    "        logger.info(\"Ingesting file_name=%s\", file_name)\n",
    "        documents = self.ingest_component.ingest(file_name, file_data)\n",
    "        logger.info(\"Finished ingestion file_name=%s\", file_name)\n",
    "        return [IngestedDoc.from_document(document) for document in documents]\n",
    "\n",
    "    def ingest_text(self, file_name: str, text: str) -> list[IngestedDoc]:\n",
    "        logger.debug(\"Ingesting text data with file_name=%s\", file_name)\n",
    "        return self._ingest_data(file_name, text)\n",
    "\n",
    "    def ingest_bin_data(\n",
    "        self, file_name: str, raw_file_data: BinaryIO\n",
    "    ) -> list[IngestedDoc]:\n",
    "        logger.debug(\"Ingesting binary data with file_name=%s\", file_name)\n",
    "        file_data = raw_file_data.read()\n",
    "        return self._ingest_data(file_name, file_data)\n",
    "\n",
    "    def bulk_ingest(self, files: list[tuple[str, Path]]) -> list[IngestedDoc]:\n",
    "        logger.info(\"Ingesting file_names=%s\", [f[0] for f in files])\n",
    "        documents = self.ingest_component.bulk_ingest(files)\n",
    "        logger.info(\"Finished ingestion file_name=%s\", [f[0] for f in files])\n",
    "        return [IngestedDoc.from_document(document) for document in documents]\n",
    "\n",
    "    def list_ingested(self) -> list[IngestedDoc]:\n",
    "        ingested_docs: list[IngestedDoc] = []\n",
    "        try:\n",
    "            docstore = self.storage_context.docstore\n",
    "            ref_docs: dict[str, RefDocInfo] | None = docstore.get_all_ref_doc_info()\n",
    "\n",
    "            if not ref_docs:\n",
    "                return ingested_docs\n",
    "\n",
    "            for doc_id, ref_doc_info in ref_docs.items():\n",
    "                doc_metadata = None\n",
    "                if ref_doc_info is not None and ref_doc_info.metadata is not None:\n",
    "                    doc_metadata = IngestedDoc.curate_metadata(ref_doc_info.metadata)\n",
    "                ingested_docs.append(\n",
    "                    IngestedDoc(\n",
    "                        object=\"ingest.document\",\n",
    "                        doc_id=doc_id,\n",
    "                        doc_metadata=doc_metadata,\n",
    "                    )\n",
    "                )\n",
    "        except ValueError:\n",
    "            logger.warning(\"Got an exception when getting list of docs\", exc_info=True)\n",
    "            pass\n",
    "        logger.debug(\"Found count=%s ingested documents\", len(ingested_docs))\n",
    "        return ingested_docs\n",
    "\n",
    "    def delete(self, doc_id: str) -> None:\n",
    "        \"\"\"Delete an ingested document.\n",
    "\n",
    "        :raises ValueError: if the document does not exist\n",
    "        \"\"\"\n",
    "        logger.info(\n",
    "            \"Deleting the ingested document=%s in the doc and index store\", doc_id\n",
    "        )\n",
    "        self.ingest_component.delete(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import span_marker\n",
    "from llama_index.extractors.entity import EntityExtractor \n",
    "from llama_index.core.node_parser import SentenceSplitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除嵌套字典中，值为空的键值对\n",
    "\n",
    "# ###1 返回新对象\n",
    "from collections import OrderedDict\n",
    "\n",
    "def _is_empty(x):\n",
    "    \"\"\"定义‘空’：None、空字符串、空的 dict/OrderedDict/list/tuple/set。\"\"\"\n",
    "    if x is None:\n",
    "        return True\n",
    "    if isinstance(x, str):\n",
    "        return x == \"\"        # 需要把全空白也当空的话：x.strip() == \"\"\n",
    "    if isinstance(x, (dict, OrderedDict, list, tuple, set)):\n",
    "        return len(x) == 0\n",
    "    return False  # False、0 等都不是“空”\n",
    "\n",
    "def del_empty(x):\n",
    "    \"\"\"\n",
    "    递归删除 dict / OrderedDict / list / tuple 中的『空』值。\n",
    "    空定义：None、\"\"、[]、{}（含 OrderedDict）；保留 False、0 等。\n",
    "    保持原容器类型（dict / OrderedDict / list / tuple）。\n",
    "    \"\"\"\n",
    "    # 映射类型：保持原类型\n",
    "    if isinstance(x, (dict, OrderedDict)):\n",
    "        cls = OrderedDict if isinstance(x, OrderedDict) else dict\n",
    "        out = cls()\n",
    "        for k, v in x.items():\n",
    "            v2 = del_empty(v)           # 先递归清理\n",
    "            if not _is_empty(v2):       # 再按清理后的结果决定是否保留\n",
    "                out[k] = v2\n",
    "        return out\n",
    "\n",
    "    # 列表\n",
    "    if isinstance(x, list):\n",
    "        out = []\n",
    "        for item in x:\n",
    "            i2 = del_empty(item)\n",
    "            if not _is_empty(i2):\n",
    "                out.append(i2)\n",
    "        return out\n",
    "\n",
    "    # 元组（可选：保持元组类型）\n",
    "    if isinstance(x, tuple):\n",
    "        out = []\n",
    "        for item in x:\n",
    "            i2 = del_empty(item)\n",
    "            if not _is_empty(i2):\n",
    "                out.append(i2)\n",
    "        return tuple(out)\n",
    "\n",
    "    # 其他原子类型原样返回（False、0 会被保留）\n",
    "    return x\n",
    "\n",
    "\n",
    "# 示例\n",
    "meta_schema = OrderedDict({\n",
    "    \"publ_no\": \"\",\n",
    "    \"publ_date\": \"2024-06-01\",\n",
    "    \"is_granted\": False,\n",
    "    \"patent_no\": \"\",\n",
    "    \"apply_no\": \"\",\n",
    "    \"apply_time\": \"\",\n",
    "    \"title\": \"一种可折叠无人机\",\n",
    "    \"applicant\": \"\",\n",
    "    \"address\": \"\",\n",
    "    \"inventors\": [\"张三\", \"李四\"],\n",
    "    \"doc_type\": \"发明专利\",\n",
    "    \"tech_field\": \"\",\n",
    "    \"root_dir\": \"/data/patents/2024\",\n",
    "    \"pdf_path\": \"/data/patents/2024/xxx.pdf\",\n",
    "    \"fig_list\": {\"abs_im\": [\"摘要图\", \"\"], \"图1\": [\"\", \"\"]}\n",
    "})\n",
    "\n",
    "cleaned = del_empty(meta_schema)\n",
    "print(cleaned)\n",
    "# OrderedDict({'publ_date': '2024-06-01', 'is_granted': False, 'title': '一种可折叠无人机', 'inventors': ['张三', '李四'], 'doc_type': '发明专利', 'root_dir': '/data/patents/2024', 'pdf_path': '/data/patents/2024/xxx.pdf', 'fig_list': {'abs_im': ['摘要图'], '图1': []}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### 2 原地更新\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def _is_empty(x):\n",
    "    if x is None:\n",
    "        return True\n",
    "    if isinstance(x, str):\n",
    "        return x == \"\"\n",
    "    if isinstance(x, (dict, OrderedDict, list, tuple, set)):\n",
    "        return len(x) == 0\n",
    "    return False\n",
    "\n",
    "def del_empty_inplace(x):\n",
    "    if isinstance(x, (dict, OrderedDict)):\n",
    "        for k in list(x.keys()):\n",
    "            v2 = del_empty_inplace(x[k])\n",
    "            if _is_empty(v2):\n",
    "                del x[k]\n",
    "            else:\n",
    "                x[k] = v2\n",
    "        return x\n",
    "    if isinstance(x, list):\n",
    "        i = 0\n",
    "        while i < len(x):\n",
    "            v2 = del_empty_inplace(x[i])\n",
    "            if _is_empty(v2):\n",
    "                x.pop(i)\n",
    "            else:\n",
    "                x[i] = v2\n",
    "                i += 1\n",
    "        return x\n",
    "    if isinstance(x, tuple):\n",
    "        # 返回“新元组”，调用方需接收\n",
    "        return tuple(v for v in (del_empty_inplace(i) for i in x) if not _is_empty(v))\n",
    "    return x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
