{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# step0: patent.md \n",
    "\n",
    "# step1:  split to :  figs_MetaDict.json   full_filtered.md  -split\n",
    "#  文本信息、图片引用 可以嵌入了\n",
    "# -> full_split.md\n",
    "# step2: full_filtered.md -->  struct: # 著录信息  # 权利要求书   # 说明书 \n",
    "#   ---> str\n",
    "# -> full_split_split.md\n",
    "# step3: extractor                     record      claims         specification\n",
    "#                         def record_extractor  claims_extractor  specification_extractor\n",
    "#   --->  norm_(str) \n",
    "#  \n",
    "# --> full_split_split_norm.md\n",
    "# 代码逻辑可能存在复用的可能  code ++\n",
    "\n",
    "# [xxxx] 以开头的段落  去掉[xxxx]   todo.\n",
    "# 无关的内容也可以去掉，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingestion-1  embed-model \n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding \n",
    "from llama_index.core import Settings\n",
    "\n",
    "embedding = HuggingFaceEmbedding(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    device=\"cpu\",                 # 建议放顶层\n",
    "    cache_folder=r\"E:\\local_models\\huggingface\\cache\\hub\",\n",
    "    trust_remote_code=True,       # 建议放顶层\n",
    "    model_kwargs={\"local_files_only\": True},   # 允许联网 False\n",
    ")\n",
    "\n",
    "# \n",
    "Settings.embed_model = embedding\n",
    "Settings.llm = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingestion-2  load-file -> str \n",
    "\n",
    "from pathlib import Path \n",
    "import json \n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "# load md-text json-figs \n",
    "def load_md_and_figs(md_path: Path, figs_name: str=\"figs.json\") -> Tuple[str, Dict[str, Any]]:\n",
    "    text = md_path.read_text(encoding='utf-8', errors='ignore')\n",
    "    fj = md_path.with_name(figs_name)\n",
    "    assert fj.is_file()\n",
    "    with open(fj, \"r\",encoding='utf-8') as f:\n",
    "        figs = json.load(f)\n",
    "    return text, figs \n",
    "\n",
    "# test \n",
    "data_root = Path.cwd().parent / \".log/SimplePDF\"\n",
    "assert Path(data_root).is_dir()\n",
    "mdfs: Path = next(Path(data_root).rglob('full_split_struct.md'), None)\n",
    "assert  Path(mdfs).exists()\n",
    "figs: Path = Path(mdfs).with_name(\"figs.json\")\n",
    "assert figs.is_file()\n",
    "text_, figs_ = load_md_and_figs(md_path=mdfs)\n",
    "print(\"纯文本长度：\", len(text_))\n",
    "print(\"figs.json 键：\", list(figs_.keys()))\n",
    "print(\"ims_desc 示例：\", list((figs_.get(\"ims_desc\") or {}).items())[:3])\n",
    "print(\"ims_absp 示例：\", list((figs_.get(\"ims_desc\") or {}).items())[:3])\n",
    "print(\"ims_annos 示例：\", [(figs_.get(\"ims_annos\") or \"\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingestion-2  content-str -> node \n",
    "\n",
    "import uuid \n",
    "from typing import List \n",
    "\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "def _to_ascii_digits(s: str) -> str:\n",
    "    _DIGIT_TRANS = str.maketrans(\"０１２３４５６７８９\", \"0123456789\")\n",
    "    return (s or \"\").translate(_DIGIT_TRANS)\n",
    "\n",
    "\n",
    "def build_text_node_from_markdown(text: str, doc_id: str, \n",
    "                                  chunk_size: int=700, \n",
    "                                  chunk_overlap: int=128) -> List[TextNode]:\n",
    "    splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks  = splitter.split_text(text)\n",
    "    nodes: List[TextNode] = []\n",
    "    for i, ch in enumerate(chunks, 1):\n",
    "        nodes.append(TextNode(\n",
    "            text = ch,\n",
    "            id_ = f\"{doc_id}::text::{i}\",\n",
    "            metadata = {\n",
    "                \"doc_id\": doc_id,\n",
    "                \"node_type\": \"text\",\n",
    "                \"chunk_id\": i\n",
    "            },\n",
    "        ))\n",
    "    return nodes \n",
    "    \n",
    "def build_figure_node_from_figs(figs: Dict[str, Any], doc_id: str) -> List[TextNode]:\n",
    "    nodes: List[TextNode] = []\n",
    "    \n",
    "    ims_desc: Dict[str, str] = figs.get(\"ims_desc\", {}) or {}\n",
    "    ims_absp: Dict[str, str] = figs.get(\"ims_absp\", {}) or {}\n",
    "    ims_bs64: Dict[str, str] = figs.get(\"ims_bs64\", {}) or {}\n",
    "    ims_annos: str = (figs.get(\"ims_annos\") or \"\").strip()\n",
    "    \n",
    "    # 摘要图\n",
    "    im_abs = figs.get(\"im_abs\") or []\n",
    "    if isinstance(im_abs, List) and im_abs[0]:\n",
    "        abs_path = im_abs[0] if isinstance(im_abs[0], str) else \"\" \n",
    "        assert Path(abs_path).is_file(), f\"摘要图{abs_path}路径不存在\"\n",
    "        text_for_embed = \"摘要图\"\n",
    "        nodes.append(TextNode(\n",
    "            text = text_for_embed,\n",
    "            id_ = f\"{doc_id}::figure::0\",\n",
    "            metadata = {\n",
    "                \"doc_id\": doc_id,\n",
    "                \"node_type\": \"figure\",\n",
    "                \"fig_no\": \"0\",\n",
    "                \"fig_desc\": \"摘要图\",\n",
    "                \"fig_path\": abs_path,\n",
    "                \"fig_bs64\": \"B64(omitted)\" if im_abs[1:] else \"\",  # ---\n",
    "                \"fig_annos\": ims_annos,\n",
    "                \"display_text\": \"摘要图\",    # 前端用\n",
    "            },\n",
    "            excluded_embed_metadata_keys = [\"fig_path\", \"fig_b64\", \"fig_annos\", \"display_text\"],     \n",
    "            excluded_llm_metadata_keys = [\"fig_b64\"],            \n",
    "        ))\n",
    "    \n",
    "    # 普通图\n",
    "    def _key_sorter(k: str) -> str:\n",
    "        try:\n",
    "            return int(_to_ascii_digits(k))\n",
    "        except:\n",
    "            return 10**9\n",
    "    \n",
    "    for k in sorted(ims_desc.keys(), key=_key_sorter):\n",
    "        desc = (ims_desc.get(k) or \"\").strip()\n",
    "        pth = (ims_absp.get(k) or \"\").strip()\n",
    "        assert Path(pth).is_file(), f\"图{k}路径{abs_path}不存在\"\n",
    "        text_for_embed = f\"图{k}为{desc}\"\n",
    "        nodes.append(TextNode(\n",
    "            text=text_for_embed,\n",
    "            id_=f\"{doc_id}::figure::{k}\",\n",
    "            metadata={\n",
    "                \"doc_id\": doc_id,\n",
    "                \"node_type\": \"figure\",\n",
    "                \"fig_no\": k,\n",
    "                \"fig_desc\": desc,\n",
    "                \"fig_path\": pth,\n",
    "                \"fig_bs64\": \"B64(omitted)\" if (ims_bs64.get(k) or \"\") else \"\",\n",
    "                \"fig_annos\": ims_annos,\n",
    "                \"display_text\": f\"图{k} {desc}\",  # 前端用\n",
    "                },\n",
    "            excluded_embed_metadata_keys = [\"fig_path\", \"fig_b64\", \"fig_annos\", \"display_text\"],     \n",
    "            excluded_llm_metadata_keys   = [\"fig_b64\"],  # 太长了、可能会影响正文信息 ---\n",
    "        ))\n",
    "    return nodes \n",
    "    \n",
    "    \n",
    "# test\n",
    "doc_id_demo = str(uuid.uuid5(uuid.NAMESPACE_URL, str(mdfs.resolve())))\n",
    "test_nodes = build_text_node_from_markdown(text_, doc_id_demo) + build_figure_node_from_figs(figs_, doc_id_demo)\n",
    "len(test_nodes), sum(1 for n in test_nodes if n.metadata[\"node_type\"]==\"figure\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingestion-3  build_index(nodes) \n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "\n",
    "import faiss \n",
    "\n",
    "persist_dir = Path.cwd().parent / \".log/faiss_db\"\n",
    "persist_dir.mkdir(parents=True, exist_ok=True)\n",
    "faiss_index_path = persist_dir / \"faiss.index\"\n",
    "meta_path = persist_dir / \"vector_meta.json\"\n",
    "\n",
    "def build_index(md_files: List[Path], embeded_dim=1024, mode=\"build\"): \n",
    "    \"\"\" 新建一个faiss向量库 \"\"\"\n",
    "    # nodes     \n",
    "    all_nodes: List[TextNode] = []\n",
    "    for md in md_files:\n",
    "        text_, figs_ = load_md_and_figs(md)\n",
    "        doc_id = str(uuid.uuid5(uuid.NAMESPACE_URL, str(md.resolve()))) \n",
    "        all_nodes += build_text_node_from_markdown(text_, doc_id)\n",
    "        all_nodes += build_figure_node_from_figs(figs_, doc_id)\n",
    "    \n",
    "    # init\n",
    "    vector_store = FaissVectorStore(faiss_index=faiss.IndexFlatL2(embeded_dim))\n",
    "    # build\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    storage_index = VectorStoreIndex(all_nodes, storage_context=storage_context, show_progress=True)\n",
    "    \n",
    "    # persist \n",
    "    storage_context.persist(persist_dir=str(persist_dir))\n",
    "    print(f\"[build]新建索引：nodes={len(all_nodes)}\")\n",
    "    return storage_index, all_nodes\n",
    "    \n",
    "\n",
    "vector_store_index, nodes_cache = build_index(md_files=[mdfs])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingestion-4.1  vector_index persist      -- faiss_db\n",
    "\n",
    "from llama_index.core import load_index_from_storage\n",
    "from llama_index.core.indices.base import BaseIndex\n",
    "\n",
    "def load_index(embeded_dim=1024, mode=\"faiss\") -> BaseIndex:\n",
    "    \n",
    "    # init\n",
    "    vector_store = FaissVectorStore(faiss_index=faiss.IndexFlatL2(embeded_dim))\n",
    "    \n",
    "    # load \n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        persist_dir=persist_dir,\n",
    "        vector_store=vector_store,\n",
    "    )\n",
    "    vector_index = load_index_from_storage(storage_context)\n",
    "\n",
    "    return vector_index\n",
    "\n",
    "vector_store_index_ = load_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingestion-4.2 nodes persist     --  bm25_db\n",
    "\n",
    "# 补充说明：\n",
    "# faiss向量知识库  persist持久化  到底保存了那些东西到本地\n",
    "# - 向量数据库存的就是向量（向量索引）。\n",
    "#   无法从向量索引中得到TextNode\n",
    "\n",
    "# - 在 LlamaIndex 的持久化体系里，节点内容/元数据 存在 docstore.json（DocumentStore），\n",
    "#   而向量索引结构的元数据（比如向量库命名空间、id 映射等）在 index_store.json；\n",
    "#   真正的向量由对应的 vector_store 管（FAISS 就是 faiss.index 文件）。\n",
    "\n",
    "# - 加载顺序通常是：StorageContext.from_defaults(persist_dir=...) → 它会把 docstore.json、index_store.json 读回来；\n",
    "#   然后 FaissVectorStore.from_persist_dir(...) 或由 LlamaIndex 把 FAISS 文件挂回。\n",
    "\n",
    "# - 检索时先用 embed_model 得到 query 向量 → FAISS 搜索得到“向量行号/内部 id” → 通过 index_store/docstore 的映射 \n",
    "#   找回 Node 的 node_id → 再去 docstore 里把 TextNode 取出来给你。\n",
    "\n",
    "# 向量索引存一份  vector_db/...  <faiss_db>\n",
    "# nodes存一份    nodes_db/..    <bm25_db>\n",
    "\n",
    "\n",
    "# bm25_db   -- nodes persist\n",
    "\n",
    "\n",
    "\"\"\"  普通的构建 bm25  nodes_db \n",
    "from llama_index.retrievers.bm25 import BM25Retriever \n",
    "from typing import Literal\n",
    "\n",
    "# 构建nodes_db ， 区分开cevtor_db的那一套逻辑\n",
    "def nodes_persist(md_files: List[Path], mode=Literal[\"bm25\"]) -> BM25Retriever:\n",
    "    \n",
    "    nodes_cache: List[TextNode] = []\n",
    "    for md in md_files:\n",
    "        text, figs = load_md_and_figs(md)\n",
    "        doc_id = str(uuid.uuid5(uuid.NAMESPACE_URL, str(md.resolve())))\n",
    "        nodes_cache += build_text_node_from_markdown(text, doc_id)\n",
    "        nodes_cache += build_figure_node_from_figs(figs, doc_id)\n",
    "    \n",
    "    bm25_retriever = BM25Retriever.from_defaults(\n",
    "        nodes=nodes_cache,\n",
    "        similarity_top_k=5,\n",
    "        # 默认的语言是英文，可能有点影响吧。。\n",
    "    )\n",
    "    root_dir = Path.cwd().parent / \".log/bm25_db\"\n",
    "    root_dir.mkdir(parents=True, exist_ok=True)\n",
    "    bm25_retriever.persist(root_dir)\n",
    "    return root_dir\n",
    "     \n",
    "def build_bm25_retriever(mode=Literal[\"local\"])-> BM25Retriever:\n",
    "    root_dir = Path.cwd().parent / \".log/bm25_db\"\n",
    "    loaded_bm25_retriever = BM25Retriever.from_persist_dir(str(root_dir))\n",
    "    return  loaded_bm25_retriever\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 结构化保存 nodes   -- bm25\n",
    "\n",
    "import gzip, json, time, re  \n",
    "from rank_bm25 import BM25Okapi \n",
    "from dataclasses import dataclass \n",
    "from llama_index.core import QueryBundle, VectorStoreIndex\n",
    "from llama_index.core.schema import TextNode, NodeWithScore \n",
    "\n",
    "\n",
    "# ------------ 轻量分词（可替换为更强中文分词）-----------------\n",
    "def _tokenize(s: str) -> List[str]:\n",
    "    s = s or \"\"\n",
    "    # 中文按字切 + 保留英文/数字 token\n",
    "    toks_cn = [ch for ch in s if \"\\u4e00\" <= ch <= \"\\u9fff\"]\n",
    "    toks_en = re.findall(r\"[A-Za-z0-9_]+\", s)\n",
    "    return toks_cn + toks_en\n",
    "\n",
    "\n",
    "# ------------  从 nodes_cache 构建若干辅助索引 ----------------------\n",
    "def build_text_nodes_by_doc(nodes: List[TextNode]) -> Dict[str, List[TextNode]]:\n",
    "    by_doc: Dict[str, List[TextNode]] = {}\n",
    "    for n in nodes:\n",
    "        if n.metadata.get(\"node_type\") == \"text\":\n",
    "            doc_id = n.metadata.get(\"doc_id\") or \"unknown\"\n",
    "            by_doc.setdefault(doc_id, []).append(n)\n",
    "    return by_doc\n",
    "\n",
    "def build_fig_index_by_doc(nodes: List[TextNode]) -> Dict[str, Dict[int, TextNode]]:\n",
    "    \"\"\"fig_no -> TextNode （每个 doc 各自维护，避免不同文档里都有图1冲突）\"\"\"\n",
    "    idx: Dict[str, Dict[int, TextNode]] = {}\n",
    "    for n in nodes:\n",
    "        if n.metadata.get(\"node_type\") == \"figure\":\n",
    "            doc_id = n.metadata.get(\"doc_id\") or \"unknown\"\n",
    "            try:\n",
    "                no = int(str(n.metadata.get(\"fig_no\", \"\")).strip())\n",
    "            except Exception:\n",
    "                continue\n",
    "            idx.setdefault(doc_id, {})[no] = n\n",
    "    return idx\n",
    "\n",
    "TEXT_NODES_BY_DOC: Dict[str, List[TextNode]] = build_text_nodes_by_doc(nodes_cache)\n",
    "FIG_INDEX_BY_DOC: Dict[str, Dict[int, TextNode]] = build_fig_index_by_doc(nodes_cache)\n",
    "NODE_LOOKUP: Dict[str, TextNode] = {n.node_id: n for n in nodes_cache}\n",
    "\n",
    "\n",
    "# ------------ BM25 分片持久化（按 doc 分片） --------------------\n",
    "@dataclass\n",
    "class BM25Shard:\n",
    "    bm25: BM25Okapi\n",
    "    node_ids: List[str]          # 与 bm25.corpus 顺序一一对应\n",
    "\n",
    "# ---- 持久化 I/O ----\n",
    "def save_bm25_shard(doc_id: str, text_nodes: List[TextNode], bm25_dir: Path) -> Path:\n",
    "    bm25_dir = Path(bm25_dir)\n",
    "    shard_dir = bm25_dir / \"shards\"\n",
    "    shard_dir.mkdir(parents=True, exist_ok=True)\n",
    "    outp = shard_dir / f\"{doc_id}.jsonl.gz\"\n",
    "\n",
    "    with gzip.open(outp, \"wt\", encoding=\"utf-8\") as fo:\n",
    "        for n in text_nodes:\n",
    "            tokens = _tokenize(n.get_content())\n",
    "            fo.write(json.dumps({\"node_id\": n.node_id, \"tokens\": tokens}, ensure_ascii=False) + \"\\n\")\n",
    "    return outp\n",
    "\n",
    "def load_bm25_shard(doc_id: str, bm25_dir: Path) -> Optional[BM25Shard]:\n",
    "    shard_path = Path(bm25_dir) / \"shards\" / f\"{doc_id}.jsonl.gz\"\n",
    "    if not shard_path.exists():\n",
    "        return None\n",
    "    node_ids, corpus_tokens = [], []\n",
    "    with gzip.open(shard_path, \"rt\", encoding=\"utf-8\") as fi:\n",
    "        for line in fi:\n",
    "            obj = json.loads(line)\n",
    "            node_ids.append(obj[\"node_id\"])\n",
    "            corpus_tokens.append(obj[\"tokens\"])\n",
    "    return BM25Shard(bm25=BM25Okapi(corpus_tokens), node_ids=node_ids)\n",
    "\n",
    "def write_manifest(bm25_dir: Path, doc_ids: List[str], version: str = \"v1\") -> None:\n",
    "    bm25_dir = Path(bm25_dir)\n",
    "    bm25_dir.mkdir(parents=True, exist_ok=True)\n",
    "    manifest = {\n",
    "        \"version\": version,\n",
    "        \"created_at\": int(time.time()),\n",
    "        \"doc_count\": len(doc_ids),\n",
    "        \"doc_ids\": doc_ids,\n",
    "    }\n",
    "    (bm25_dir / \"manifest.json\").write_text(json.dumps(manifest, ensure_ascii=False, indent=2), \"utf-8\")\n",
    "\n",
    "def read_manifest(bm25_dir: Path) -> Dict[str, any]:\n",
    "    p = Path(bm25_dir) / \"manifest.json\"\n",
    "    return json.loads(p.read_text(\"utf-8\")) if p.exists() else {}\n",
    "\n",
    "# ---- 构建/更新 BM25 库（按文档分片） ----\n",
    "def build_or_update_bm25_db(\n",
    "    bm25_dir: Path,\n",
    "    text_nodes_by_doc: Dict[str, List[TextNode]],\n",
    "    overwrite: bool = False,\n",
    ") -> Dict[str, Path]:\n",
    "    bm25_dir = Path(bm25_dir)\n",
    "    shard_dir = bm25_dir / \"shards\"\n",
    "    shard_dir.mkdir(parents=True, exist_ok=True)\n",
    "    written: Dict[str, Path] = {}\n",
    "    for doc_id, nodes in text_nodes_by_doc.items():\n",
    "        shard_file = shard_dir / f\"{doc_id}.jsonl.gz\"\n",
    "        if shard_file.exists() and not overwrite:\n",
    "            written[doc_id] = shard_file\n",
    "            continue\n",
    "        save_bm25_shard(doc_id, nodes, bm25_dir)\n",
    "        written[doc_id] = shard_file\n",
    "    write_manifest(bm25_dir, list(text_nodes_by_doc.keys()))\n",
    "    return written\n",
    "\n",
    "# ---- 查询时仅在“候选文档”上做 BM25 检索（懒加载分片） ----\n",
    "def bm25_query_on_docs(\n",
    "    query: str,\n",
    "    doc_ids: List[str],\n",
    "    bm25_dir: Path,\n",
    "    node_lookup: Dict[str, TextNode],\n",
    "    per_doc_topk: int = 12,\n",
    ") -> List[Tuple[TextNode, float]]:\n",
    "    toks = _tokenize(query)\n",
    "    out: List[Tuple[TextNode, float]] = []\n",
    "    for doc_id in doc_ids:\n",
    "        shard = load_bm25_shard(doc_id, bm25_dir)\n",
    "        if not shard:\n",
    "            continue\n",
    "        scores = shard.bm25.get_scores(toks)\n",
    "        idxs = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:per_doc_topk]\n",
    "        for i in idxs:\n",
    "            nid = shard.node_ids[i]\n",
    "            n = node_lookup.get(nid)\n",
    "            if n is not None:\n",
    "                out.append((n, float(scores[i])))\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "# --------- 混合检索（FAISS→候选文档→BM25），可加“图N偏好/联动”---------\n",
    "def _vector_hits(index: VectorStoreIndex, query: str, top_k: int = 24) -> List[Tuple[TextNode, float]]:\n",
    "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
    "    hits = retriever.retrieve(QueryBundle(query))\n",
    "    out: List[Tuple[TextNode, float]] = []\n",
    "    for h in hits:\n",
    "        out.append((h.node, float(getattr(h, \"score\", 0.0) or 0.0)))\n",
    "    return out\n",
    "\n",
    "def _choose_top_docs_from_vec(vec_pairs: List[Tuple[TextNode, float]], k_docs: int = 3) -> List[str]:\n",
    "    agg: Dict[str, float] = {}\n",
    "    for n, s in vec_pairs:\n",
    "        doc_id = n.metadata.get(\"doc_id\") or \"unknown\"\n",
    "        agg[doc_id] = max(agg.get(doc_id, 0.0), s)   # 也可以 sum / mean\n",
    "    return [d for d, _ in sorted(agg.items(), key=lambda x: x[1], reverse=True)[:k_docs]]\n",
    "\n",
    "def _minmax_norm(pairs: List[Tuple[TextNode, float]]) -> Dict[str, float]:\n",
    "    if not pairs:\n",
    "        return {}\n",
    "    scores = [s for _, s in pairs]\n",
    "    mn, mx = min(scores), max(scores)\n",
    "    rng = (mx - mn) or 1.0\n",
    "    out = {}\n",
    "    for n, s in pairs:\n",
    "        out[n.node_id] = (s - mn) / rng\n",
    "    return out\n",
    "\n",
    "def _fig_nums_in_text(s: str) -> List[int]:\n",
    "    PAT = re.compile(r\"图\\s*([0-9０-９]+)\")\n",
    "    trans = str.maketrans(\"０１２３４５６７８９\",\"0123456789\")\n",
    "    out = []\n",
    "    for m in PAT.finditer(s or \"\"):\n",
    "        try:\n",
    "            out.append(int(m.group(1).translate(trans)))\n",
    "        except: pass\n",
    "    return out\n",
    "\n",
    "def hybrid_search_sharded_persisted(\n",
    "    index: VectorStoreIndex,\n",
    "    query: str,\n",
    "    bm25_dir: Path,\n",
    "    *,\n",
    "    top_k: int = 8,\n",
    "    mmr_boost_fig_mention: float = 0.02,\n",
    "    top_docs_for_bm25: int = 3,\n",
    "    per_doc_topk: int = 20,\n",
    "    w_vec: float = 0.7,\n",
    "    w_bm25: float = 0.3,\n",
    "    prefer_fig: bool = True,\n",
    "    fig_link: bool = True,\n",
    ") -> List[Tuple[TextNode, float]]:\n",
    "    # 1) 向量近邻（取多一点，用于挑候选文档）\n",
    "    vec_pairs = _vector_hits(index, query, top_k=max(top_k, 24))\n",
    "    cand_docs  = _choose_top_docs_from_vec(vec_pairs, k_docs=top_docs_for_bm25)\n",
    "\n",
    "    # 2) 对候选文档做 BM25（从本地分片懒加载）\n",
    "    bm25_pairs = bm25_query_on_docs(\n",
    "        query, cand_docs, bm25_dir=bm25_dir, node_lookup=NODE_LOOKUP, per_doc_topk=per_doc_topk\n",
    "    )\n",
    "\n",
    "    # 3) 归一化 + 融合\n",
    "    vnorm = _minmax_norm(vec_pairs)\n",
    "    bnorm = _minmax_norm(bm25_pairs)\n",
    "    pool: Dict[str, Tuple[TextNode, float]] = {}\n",
    "    for n, _ in vec_pairs:\n",
    "        pool[n.node_id] = (n, w_vec * vnorm.get(n.node_id, 0.0))\n",
    "    for n, _ in bm25_pairs:\n",
    "        if n.node_id in pool:\n",
    "            old_n, old_s = pool[n.node_id]\n",
    "            pool[n.node_id] = (old_n, old_s + w_bm25 * bnorm.get(n.node_id, 0.0))\n",
    "        else:\n",
    "            pool[n.node_id] = (n, w_bm25 * bnorm.get(n.node_id, 0.0))\n",
    "\n",
    "    merged = sorted(pool.values(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 4) “图N”偏好（用户 query 明确提到某图号时，给相应 figure 小幅加分）\n",
    "    if prefer_fig:\n",
    "        mention = set(_fig_nums_in_text(query))\n",
    "        if mention:\n",
    "            boosted: List[Tuple[TextNode, float]] = []\n",
    "            for n, s in merged:\n",
    "                if n.metadata.get(\"node_type\") == \"figure\":\n",
    "                    try:\n",
    "                        no = int(str(n.metadata.get(\"fig_no\", \"\")).strip())\n",
    "                    except Exception:\n",
    "                        no = None\n",
    "                    if no in mention:\n",
    "                        boosted.append((n, s + mmr_boost_fig_mention))\n",
    "                        continue\n",
    "                boosted.append((n, s))\n",
    "            merged = sorted(boosted, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 5) “图N联动”（文本段落里出现“图N”，则补充当前 doc 的对应图节点）\n",
    "    if fig_link:\n",
    "        # 已命中集合\n",
    "        have_ids = set(n.node_id for n, _ in merged)\n",
    "        # 从文本命中里抽取“图N”\n",
    "        wanted: Dict[str, set[int]] = {}\n",
    "        for n, _ in merged[:top_k*2]:\n",
    "            if n.metadata.get(\"node_type\") == \"text\":\n",
    "                doc_id = n.metadata.get(\"doc_id\") or \"unknown\"\n",
    "                wanted.setdefault(doc_id, set()).update(_fig_nums_in_text(n.get_content()))\n",
    "        # 追加\n",
    "        extras: List[Tuple[TextNode, float]] = []\n",
    "        base = merged[-1][1] if merged else 0.0\n",
    "        for doc_id, fig_set in wanted.items():\n",
    "            fig_map = FIG_INDEX_BY_DOC.get(doc_id, {})\n",
    "            for fno in sorted(fig_set):\n",
    "                fn = fig_map.get(fno)\n",
    "                if fn and fn.node_id not in have_ids:\n",
    "                    extras.append((fn, base + 1e-6))\n",
    "                    have_ids.add(fn.node_id)\n",
    "        if extras:\n",
    "            merged = sorted(merged + extras, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return merged[:top_k]\n",
    "\n",
    "\n",
    "# ---------  统一打印/渲染（annos 只打印一次）---------\n",
    "def render_answer(query: str, pairs: List[Tuple[TextNode, float]], show: int = 8) -> None:\n",
    "    print(\"Q:\", query)\n",
    "    printed_annos = False\n",
    "    for i, (n, s) in enumerate(pairs[:show], 1):\n",
    "        node_id = n.id_\n",
    "        mt = n.metadata or {}\n",
    "        ntype = mt.get(\"node_type\", \"text\")\n",
    "        if ntype == \"figure\":\n",
    "            no   = mt.get(\"fig_no\")\n",
    "            desc = mt.get(\"fig_desc\", \"\")\n",
    "            path = mt.get(\"fig_path\", \"\")\n",
    "            print(f\"{i:>2}. [FIG {no}] {node_id} | {desc} | path={path} | score={s:.4f}\")\n",
    "            if not printed_annos and (mt.get(\"fig_annos\") or \"\").strip():\n",
    "                ann = mt[\"fig_annos\"]\n",
    "                print(\"    └─ 附图标记说明：\", ann[:200] + (\"…\" if len(ann) > 200 else \"\"))\n",
    "                printed_annos = True\n",
    "        else:\n",
    "            txt = (n.get_content() or \"\").replace(\"\\n\", \" \").strip()\n",
    "            print(f\"{i:>2}. [TEXT] {node_id} | {txt[:180]}{'…' if len(txt) > 180 else ''} | score={s:.4f}\")\n",
    "\n",
    "\n",
    "# --------- 构建/更新 BM25_DB，然后小测试 ---------\n",
    "bm25_dir = Path.cwd().parent / \".log\" / \"bm25_db\"\n",
    "_ = build_or_update_bm25_db(bm25_dir, TEXT_NODES_BY_DOC, overwrite=False)\n",
    "\n",
    "# 试一把：通用问题\n",
    "q1 = \"这个专利的核心结构与技术要点是什么？\"\n",
    "hits1 = hybrid_search_sharded_persisted(vector_store_index, q1, bm25_dir=bm25_dir, top_k=8, prefer_fig=True, fig_link=True)\n",
    "render_answer(q1, hits1, show=8)\n",
    "\n",
    "# 试一把：带“图N”偏好 + 联动\n",
    "q2 = \"请解释图2的含义，并给出相关段落\"   # 单独找这个 图2  有点难。\n",
    "hits2 = hybrid_search_sharded_persisted(vector_store_index, q2, bm25_dir=bm25_dir, top_k=8, prefer_fig=True, fig_link=True)\n",
    "render_answer(q2, hits2, show=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ingestion-4.3  load persist_vector_db + persit_nodes_db    -- faissdb  -- bm25_db )\n",
    "\n",
    "\n",
    "def load_vector_db_local(mode=\"faiss\"):\n",
    "    pass \n",
    "\n",
    "\n",
    "def load_nodes_db_local(mode=\"bm25\"):\n",
    "    pass \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingestion-5 retrieve \n",
    "\n",
    "import re \n",
    "from dataclasses import dataclass \n",
    "from typing import Iterable \n",
    "\n",
    "from llama_index.core import QueryBundle\n",
    "from llama_index.retrievers.bm25 import BM25Retriever \n",
    "\n",
    "# retrieve config \n",
    "TOP_K = 8\n",
    "VECTOR_MODE = \"mmr\"       # \"default\" | \"mmr\"\n",
    "MMR_ALPHA = 0.5           # 仅当 VECTOR_MODE=\"mmr\" 生效，越大越多样化\n",
    "USE_BM25 = True           # 默认使用\n",
    "HYBRID_W_VEC = 0.70       # 混合检索加权：向量\n",
    "HYBRID_W_BM25 = 0.30      # 混合检索加权：BM25\n",
    "PREFER_FIG_BOOST = 0.02   # 检索命中“图N”时，小幅提升对应 figure 的分数\n",
    "TEXT_PREVIEW_CHARS = 400  # 打印预览字数  \n",
    "\n",
    "# 数字\n",
    "def _to_ascii_int(s: str, int_out: bool=True) -> Optional[int]:\n",
    "    _DIGIT_TRANS = str.maketrans(\"０１２３４５６７８９\", \"0123456789\")\n",
    "    try:\n",
    "        return int(s.translate(_DIGIT_TRANS))\n",
    "    except Exception:\n",
    "        if int_out:\n",
    "            return 10**9\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# \"图n\"\n",
    "FIG_PAT = re.compile(r\"图\\s*([0-9０-９]+)\")\n",
    "def _fig_nums_in_text(s: str) -> List[int]:\n",
    "    out = []\n",
    "    for m in FIG_PAT.finditer(s or \"\"):\n",
    "        n = _to_ascii_int(m.group(1))\n",
    "        if n is not None:\n",
    "            out.append(n)\n",
    "    return out\n",
    "\n",
    "# ---------------------   统一结果结构  ----------------\n",
    "@dataclass \n",
    "class HitRow:\n",
    "    node_id: str \n",
    "    node_type: str\n",
    "    score: float \n",
    "    text_preview: str \n",
    "    # figure \n",
    "    fig_no: Optional[str] = None  # abs -> 0 , 子图： 1 2 3 ...\n",
    "    fig_desc: Optional[str] = None\n",
    "    fig_path: Optional[str] = None\n",
    "    fig_bs64: Optional[str] = None\n",
    "    fig_annos: Optional[str] = None\n",
    "    \n",
    "\n",
    "def _coerce_hits(hits: Iterable[Any]) -> List[Tuple[TextNode, float]]:\n",
    "    \"\"\"  把多种 hits 统一成 List[(node, score)] \"\"\"\n",
    "    out = [] \n",
    "    for h in hits:\n",
    "        if hasattr(h, \"node\"):  # NodeWithScore   -> vector-method \n",
    "            out.append((h.node, float(getattr(h, \"score\", 0.0))))\n",
    "        elif isinstance(h, TextNode): # 直接node\n",
    "            out.append((h, 0.0))\n",
    "        else:\n",
    "            try:\n",
    "                n,s = h\n",
    "                out.append((n,float(s)))\n",
    "            except Exception:\n",
    "                pass \n",
    "    return out \n",
    "\n",
    "\n",
    "def _build_hit_row(n: TextNode, score: float, preview_chars: int=TEXT_PREVIEW_CHARS) -> HitRow:\n",
    "    ntype = (n.metadata or {}).get(\"node_type\", \"text\")\n",
    "    if ntype == \"figure\":\n",
    "        pr = (n.get_content() or \"\")\n",
    "        pr = pr.replace(\"\\n\", \" \").strip()\n",
    "        pr = pr[:preview_chars] + (\"…\" if len(pr) > preview_chars else \"\")\n",
    "        return HitRow(\n",
    "            node_id=n.node_id,\n",
    "            node_type=\"figure\",\n",
    "            score=score,\n",
    "            text_preview=pr,\n",
    "            fig_no=str((n.metadata or {}).get(\"fig_no\")),\n",
    "            fig_desc=(n.metadata or {}).get(\"fig_desc\") or \"\",\n",
    "            fig_path=(n.metadata or {}).get(\"fig_path\") or \"\",\n",
    "            fig_bs64=(n.metadata or {}).get(\"fig_bs64\") or \"\", \n",
    "            fig_annos=(n.metadata or {}).get(\"fig_annos\") or \"\", \n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        pr = (n.get_content() or \"\")\n",
    "        pr = pr.replace(\"\\n\", \" \").strip()\n",
    "        pr = pr[:preview_chars] + (\"…\" if len(pr) > preview_chars else \"\")\n",
    "        return HitRow(\n",
    "            node_id=n.node_id, node_type=\"text\", score=score,\n",
    "            text_preview=pr,\n",
    "        )\n",
    "\n",
    "\n",
    "def print_rows(query: str, rows: List[HitRow], show: int = 10, show_annos_once: bool = True):\n",
    "    \"\"\"统一打印；附图标记说明（annos）只打印一次\"\"\"\n",
    "    print(f\"Q: {query}\\n\")\n",
    "    annos_printed = False\n",
    "    for i, r in enumerate(rows[:show], 1):\n",
    "        if r.node_type == \"figure\":\n",
    "            print(f\"{i:>2}. [FIG  ] score={r.score:.4f} | {r.node_id} | 图{r.fig_no} {r.fig_desc} | path={r.fig_path}\")\n",
    "            if show_annos_once and (not annos_printed) and r.fig_annos:\n",
    "                print(f\"    └─ 附图标记说明：{r.fig_annos[:200]}{'…' if len(r.fig_annos)>200 else ''}\")\n",
    "                annos_printed = True\n",
    "        else:\n",
    "            print(f\"{i:>2}. [TEXT ] score={r.score:.4f} | {r.node_id} |{r.text_preview}\")\n",
    "\n",
    "\n",
    "# bm25 retriever  -->  针对目标文档 -- 检索到的这些文档  <-- doc_id\n",
    "# bm25 & 图n快速映射\n",
    "from llama_index.retrievers.bm25 import BM25Retriever \n",
    "import Stemmer\n",
    "\n",
    "# 需要：你已在之前单元拿到了 index, nodes_cache\n",
    "assert \"vector_store_index\" in globals(), \"请先运行你构建索引的单元，获得 `vector_store_index`。\"\n",
    "assert \"nodes_cache\" in globals(), \"请先运行你构建节点的单元，获得 `nodes_cache`。\"\n",
    "\n",
    "BM25_TOP_K_DEFAULT = 15    # 候选越多，融合时越有余地\n",
    "BM25_RET = None \n",
    "if USE_BM25:\n",
    "    BM25_RET = BM25Retriever.from_defaults(nodes=nodes_cache, \n",
    "                                           similarity_top_k=BM25_TOP_K_DEFAULT,\n",
    "                                           stemmer=Stemmer.Stemmer(\"english\"),\n",
    "                                            language=\"english\",\n",
    "                                           )\n",
    "\n",
    "# 图n 联动索引（text命中里提到了 图n 时， 补充相应的 figure node）\n",
    "FIG_NODE_INDEX: Dict[int, TextNode] = {}\n",
    "for n in nodes_cache:\n",
    "    if (n.metadata or {}).get(\"node_type\") == \"figure\":\n",
    "        no_raw = (n.metadata or {}).get(\"fig_no\")\n",
    "        try:\n",
    "            k = int(str(no_raw).strip())\n",
    "            FIG_NODE_INDEX[k]=n\n",
    "        except Exception:\n",
    "            pass \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= ③ 检索模式：vector / bm25 / hybrid =========\n",
    "\n",
    "def vector_search(\n",
    "    index: VectorStoreIndex,\n",
    "    query: str,\n",
    "    *,\n",
    "    top_k: int = TOP_K,\n",
    "    vector_mode: str = VECTOR_MODE,\n",
    "    mmr_alpha: float = MMR_ALPHA,\n",
    ") -> List[Tuple[TextNode, float]]:\n",
    "    retriever = index.as_retriever(\n",
    "        similarity_top_k=top_k,\n",
    "        vector_store_query_mode=vector_mode,\n",
    "        alpha=mmr_alpha if vector_mode == \"mmr\" else None,\n",
    "    )\n",
    "    hits = retriever.retrieve(QueryBundle(query))\n",
    "    return _coerce_hits(hits)\n",
    "\n",
    "def bm25_search(\n",
    "    query: str,\n",
    "    *,\n",
    "    top_k: int = TOP_K,\n",
    ") -> List[Tuple[TextNode, float]]:\n",
    "    if not (USE_BM25 and BM25_RET is not None):\n",
    "        return []\n",
    "    return _coerce_hits(BM25_RET.retrieve(query)[:top_k])\n",
    "\n",
    "def _normalize_scores(pairs: List[Tuple[TextNode, float]]) -> List[Tuple[TextNode, float, float]]:\n",
    "    \"\"\"min-max 归一化到 [0,1]，返回 (node, raw, norm)\"\"\"\n",
    "    if not pairs:\n",
    "        return []\n",
    "    vals = [s for _, s in pairs]\n",
    "    mx, mn = max(vals), min(vals)\n",
    "    rng = (mx - mn) or 1.0\n",
    "    out = []\n",
    "    for n, s in pairs:\n",
    "        out.append((n, s, (s - mn) / rng))\n",
    "    return out\n",
    "\n",
    "def hybrid_search(\n",
    "    index: VectorStoreIndex,\n",
    "    query: str,\n",
    "    *,\n",
    "    top_k: int = TOP_K,\n",
    "    w_vec: float = HYBRID_W_VEC,\n",
    "    w_bm25: float = HYBRID_W_BM25,\n",
    "    vector_mode: str = VECTOR_MODE,\n",
    "    mmr_alpha: float = MMR_ALPHA,\n",
    ") -> List[Tuple[TextNode, float]]:\n",
    "    vec_pairs = vector_search(index, query, top_k=top_k, vector_mode=vector_mode, mmr_alpha=mmr_alpha)\n",
    "    bm_pairs  = bm25_search(query, top_k=top_k)\n",
    "\n",
    "    vec_norm = _normalize_scores(vec_pairs)\n",
    "    bm_norm  = _normalize_scores(bm_pairs)\n",
    "\n",
    "    pool: Dict[str, Tuple[TextNode, float]] = {}\n",
    "    for n, _, nv in vec_norm:\n",
    "        pool[n.node_id] = (n, w_vec * nv)\n",
    "    for n, _, nb in bm_norm:\n",
    "        if n.node_id in pool:\n",
    "            old_n, old_s = pool[n.node_id]\n",
    "            pool[n.node_id] = (old_n, old_s + w_bm25 * nb)\n",
    "        else:\n",
    "            pool[n.node_id] = (n, w_bm25 * nb)\n",
    "\n",
    "    merged = sorted(pool.values(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    return merged\n",
    "\n",
    "# ---- 简单试跑 ----\n",
    "for mode_name, fn in [\n",
    "    (\"vector\", lambda q: vector_search(vector_store_index, q, top_k=5)),\n",
    "    (\"bm25\",   lambda q: bm25_search(q, top_k=5)),\n",
    "    (\"hybrid\", lambda q: hybrid_search(vector_store_index, q, top_k=5)),\n",
    "]:\n",
    "    q = \"该专利发明了什么？\"\n",
    "    pairs = fn(q)\n",
    "    rows = [_build_hit_row(n, s) for n, s in pairs]\n",
    "    print(f\"\\n=== {mode_name.upper()} ===\")\n",
    "    print_rows(q, rows, show=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z    llm \n",
    "\n",
    "# local_llm\n",
    "\n",
    "model_name = \"Qwen/Qwen3-1.7B\"\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM \n",
    "from llama_index.core import Settings \n",
    "\n",
    "local_llm = HuggingFaceLLM(\n",
    "    model_name=model_name,\n",
    "    tokenizer_name=model_name,\n",
    "    context_window=1400,\n",
    "    max_new_tokens=300,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "    device_map='cpu'\n",
    ")\n",
    "\n",
    "\n",
    "# cloud_llm\n",
    "from openai import OpenAI \n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "DEEPSEEK_API_KEY = os.getenv(\"GLM_API_KEY\")   # https://api.deepseek.com\n",
    "QWEN_API_KEY = os.getenv(\"GLM_API_KEY\")       # https://dashscope.aliyuncs.com/compatible-mode/v1\n",
    "\n",
    "# client\n",
    "client = OpenAI(\n",
    "    api_key=QWEN_API_KEY,\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "\n",
    "\n",
    "Settings.llm = local_llm\n",
    "\n",
    "\n",
    "\n",
    "# 自定义retriever "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
