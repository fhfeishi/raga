{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "\n",
    "zhuanli_path_pdf = Path(r\".\\demo.pdf\")\n",
    "zhuanli_path_md = Path(r\".\\demo.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-01 17:09:35,581 - INFO - Load pretrained SentenceTransformer: Qwen/Qwen3-Embedding-0.6B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191b4e157d474420916004f9ee7d8e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277c67a55e2a4ffba49150f906a0ef0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788b27bc237a46b993b0c2f4c560e1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fee395d96f642e5aac313bb7fe201c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009ea62b05b645f28779580c8f9e9df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073caeb851cb40f3969fef8fd0a57010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3a9e9d09d2477a9251cfe25e907ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc89a5ab2ef45128fbb520df21e4556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d991f9d8889473ab98a7da73e1071f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c2573f74264f70bb434e28dc6d95ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/313 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-01 17:10:04,365 - INFO - 1 prompt is loaded, with the key: query\n",
      "2025-09-01 17:10:04,512 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f25211a61da4494b955e7b5d6b87d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9f852790bd43d89ea2c204e10a7e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample BaseNodes ===\n",
      "[1] node_id=96b51ce5-2da2-4c83-af3c-3de5ac90163c  ref_doc_id=8c838bf1-03d3-4b58-b575-c289078fb295\n",
      "    text:   (19)中华人民共和国国家知识产权局  \n",
      "    meta: {'file_name': 'demo.md', 'doc_id': '8c838bf1-03d3-4b58-b575-c289078fb295', 'window': '\\n\\n(19)中华人民共和国国家知识产权局\\n', 'original_text': '\\n\\n(19)中华人民共和国国家知识产权局\\n'}\n",
      "    prev: None   next: None\n",
      "[2] node_id=70bbfcd8-1726-4c5e-80a5-af1af63dbbe2  ref_doc_id=9a57d372-e40b-4fff-8d9f-251c30cecac0\n",
      "    text:   (12)实用新型专利 (21)申请号202021894937.5 (22)申请日2020.09.02 (73)专利权人杭州宇树科技有限公司地址310053浙江省杭州市滨江区西兴街道东流路88号1幢306室 (72)发明人王兴兴 (74)专利代理机构浙江翔隆专利事务所（普通合伙）33206 代理人许守金 (51)In ...\n",
      "    meta: {'file_name': 'demo.md', 'doc_id': '9a57d372-e40b-4fff-8d9f-251c30cecac0', 'window': '\\n\\n(12)实用新型专利\\n(21)申请号202021894937.5\\n(22)申请日2020.09.02\\n(73)专利权人杭州宇树科技有限公司地址310053浙江省杭州市滨江区西兴街道东流路88号1幢306室\\n(72)发明人王兴兴\\n(74)专利代理机构浙江翔隆专利事务所（普通合伙）33206\\n代理人许守金\\n(51)Int.Cl.\\n B25J5/00(2006.01)\\nB25J9/10(2006.01)\\nB62D57/032(2006.01)', 'original_text': '\\n\\n(12)实用新型专利\\n(21)申请号202021894937.5\\n(22)申请日2020.09.02\\n(73)专利权人杭州宇树科技有限公司地址310053浙江省杭州市滨江区西兴街道东流路88号1幢306室\\n(72)发明人王兴兴\\n(74)专利代理机构浙江翔隆专利事务所（普通合伙）33206\\n代理人许守金\\n(51)Int.Cl.\\n'}\n",
      "    prev: None   next: a06ac772-7a65-47ed-8414-2d1298305cde\n",
      "[3] node_id=a06ac772-7a65-47ed-8414-2d1298305cde  ref_doc_id=9a57d372-e40b-4fff-8d9f-251c30cecac0\n",
      "    text: B25J5/00(2006.01) B25J9/10(2006.01) B62D57/032(2006.01) \n",
      "    meta: {'file_name': 'demo.md', 'doc_id': '9a57d372-e40b-4fff-8d9f-251c30cecac0', 'window': '\\n\\n(12)实用新型专利\\n(21)申请号202021894937.5\\n(22)申请日2020.09.02\\n(73)专利权人杭州宇树科技有限公司地址310053浙江省杭州市滨江区西兴街道东流路88号1幢306室\\n(72)发明人王兴兴\\n(74)专利代理机构浙江翔隆专利事务所（普通合伙）33206\\n代理人许守金\\n(51)Int.Cl.\\n B25J5/00(2006.01)\\nB25J9/10(2006.01)\\nB62D57/032(2006.01)', 'original_text': 'B25J5/00(2006.01)\\nB25J9/10(2006.01)\\nB62D57/032(2006.01)'}\n",
      "    prev: 70bbfcd8-1726-4c5e-80a5-af1af63dbbe2   next: None\n",
      "[4] node_id=dd557fdf-9d19-4c60-bcdd-351f32245d5b  ref_doc_id=a83be989-72c9-4cf9-be30-472400b61a43\n",
      "    text:   (54)实用新型名称 一种结构紧凑的回转动力单元以及应用其的机器人 \n",
      "    meta: {'file_name': 'demo.md', 'doc_id': 'a83be989-72c9-4cf9-be30-472400b61a43', 'window': '\\n\\n(54)实用新型名称\\n一种结构紧凑的回转动力单元以及应用其的机器人', 'original_text': '\\n\\n(54)实用新型名称\\n一种结构紧凑的回转动力单元以及应用其的机器人'}\n",
      "    prev: None   next: None\n",
      "[5] node_id=cf8a1c9b-3c52-4418-8251-80040d48e8a0  ref_doc_id=328a07a2-c8e6-4225-841a-5dbeaab52d0f\n",
      "    text:   (57)摘要 本实用新型公开了一种结构紧凑的回转动力单元以及应用其的机器人，属于动力单元以及机器人技术领域。现有回转单元方案，需要在输出端设置磁编码器，但会导致关节输出机构无法做到紧凑。本实用新型一种结构紧凑的回转动力单元在太阳轮设置具有中空腔的回转中心；并把输出端编码器芯片设置在回转中心的中空腔内。本实用新型经过 ...\n",
      "    meta: {'file_name': 'demo.md', 'doc_id': '328a07a2-c8e6-4225-841a-5dbeaab52d0f', 'window': '\\n\\n(57)摘要\\n本实用新型公开了一种结构紧凑的回转动力单元以及应用其的机器人，属于动力单元以及机器人技术领域。现有回转单元方案，需要在输出端设置磁编码器，但会导致关节输出机构无法做到紧凑。本实用新型一种结构紧凑的回转动力单元在太阳轮设置具有中空腔的回转中心；并把输出端编码器芯片设置在回转中心的中空腔内。本实用新型经过不断探索以及试验，改变现有的输出端磁编码器的设置位置，在太阳轮的回转中心设置中空腔，所述中空腔能够容纳磁编码器的输出端编码器芯片，进而把输出端编码器芯片放置在中空腔中，能够有效减少磁编码器的空间占用，使得回转动力单元结构更加紧凑，结构简单、实用，方案切实可行。\\n!\\n 1.  一种结构紧凑的回转动力单元，包括能够提供动能的电机、与电机转子相连接的太阳轮(13)、能够检测转动角度的磁编码器、用于装配电机的基座(17)；其特征在于，\\n所述太阳轮(13)在回转中心设置中空腔；\\n所述磁编码器包括输出端编码器芯片(41)；\\n所述基座(17)固接一能够穿设太阳轮(13)中空腔的支撑杆(4)；\\n所述支撑杆(4)上装配所述输出端编码器芯片(41)，并连同输出端编码器芯片(41)设置在太阳轮(13)的中空腔内。\\n2.  如权利要求1所述的一种结构紧凑的回转动力单元，其特征在于，\\n所述磁编码器设置一能够与输出端编码器芯片(41)磁性感应的输出端磁环(6)；\\n所述太阳轮(13)装配一行星架(7)；\\n所述输出端磁环(6)与行星架(7)共轴线安装在一起。\\n3. ', 'original_text': '\\n\\n(57)摘要\\n本实用新型公开了一种结构紧凑的回转动力单元以及应用其的机器人，属于动力单元以及机器人技术领域。现有回转单元方案，需要在输出端设置磁编码器，但会导致关节输出机构无法做到紧凑。本实用新型一种结构紧凑的回转动力单元在太阳轮设置具有中空腔的回转中心；并把输出端编码器芯片设置在回转中心的中空腔内。本实用新型经过不断探索以及试验，改变现有的输出端磁编码器的设置位置，在太阳轮的回转中心设置中空腔，所述中空腔能够容纳磁编码器的输出端编码器芯片，进而把输出端编码器芯片放置在中空腔中，能够有效减少磁编码器的空间占用，使得回转动力单元结构更加紧凑，结构简单、实用，方案切实可行。\\n!\\n'}\n",
      "    prev: None   next: 89b868db-5bc5-4761-8ce3-4f2dacd09cb4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "728260b084104e11979305a00a120c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb72696b61c4ae6b878b317cad9b328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Persisted to: D:\\ddesktop\\agentdemos\\codespace\\raga\\scripts\\local_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-01 17:13:56,740 - INFO - Loading all indices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Top matches ===\n",
      "score=0.293  node_id=469e6744-4250-477c-8ab6-1af0348aebe9  ->   (54)实用新型名称 一种结构紧凑的回转动力单元以及应用其的机器人 ...\n",
      "score=0.29  node_id=499b6f68-9ef1-4768-88ff-f8f3707af396  ->   (12)实用新型专利 (21)申请号202021894937.5 (22)申请日2020.09.02 (73)专利权人杭州宇树科技有限公司地址310053浙江省杭州市滨江区西兴街道东流路88号1幢306室 (72)发明人王兴兴 (74) ...\n",
      "score=0.287  node_id=66b2dc03-0771-4778-a5a4-9f5e327c296b  ->   (19)中华人民共和国国家知识产权局  ...\n",
      "score=0.258  node_id=7dbc70f1-aaa2-46dd-a855-a9b5c76ec699  ->   技术领域 [0001] 本实用新型涉及一种结构紧凑的回转动力单元以及应用其的机器人，属于动力单元以及机器人技术领域。 ...\n",
      "score=0.254  node_id=352c6335-df88-4bb1-aa89-807dbf53bec1  ->   一种结构紧凑的回转动力单元以及应用其的机器人  ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No index in storage context, check if you specified the right persist_dir.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 126\u001b[39m\n\u001b[32m    124\u001b[39m inp = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./demo.md\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    125\u001b[39m q = \u001b[33m\"\u001b[39m\u001b[33m解析一下专利的摘要\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 111\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(input_path, query, use_window)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# 7) 模拟“重新加载后”查看 DocStore 里的节点\u001b[39;00m\n\u001b[32m    110\u001b[39m storage2 = StorageContext.from_defaults(vector_store=vs)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m _ = \u001b[43mload_index_from_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPERSIST_DIR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m ref_infos = storage2.docstore.get_all_ref_doc_info() \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    113\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== DocStore has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ref_infos)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ref docs ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\environment\\miniconda\\envs\\rag\\Lib\\site-packages\\llama_index\\core\\indices\\loading.py:38\u001b[39m, in \u001b[36mload_index_from_storage\u001b[39m\u001b[34m(storage_context, index_id, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m indices = load_indices_from_storage(storage_context, index_ids=index_ids, **kwargs)\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indices) == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     39\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo index in storage context, check if you specified the right persist_dir.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     40\u001b[39m     )\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indices) > \u001b[32m1\u001b[39m:\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     43\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected to load a single index, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(indices)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m instead. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     44\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease specify index_id.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     45\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: No index in storage context, check if you specified the right persist_dir."
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import logging, os\n",
    "from typing import List\n",
    "\n",
    "from llama_index.core import Document, VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser, SentenceSplitter\n",
    "from llama_index.core.ingestion import run_transformations\n",
    "from llama_index.core.schema import BaseNode, MetadataMode, NodeWithScore\n",
    "\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.readers.file.markdown import MarkdownReader\n",
    "import chromadb\n",
    "\n",
    "# ---------- logging ----------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s.%(msecs)03d [%(levelname)-8s] %(name)25s - %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(\"patent_parser_min\")\n",
    "\n",
    "# ---------- paths ----------\n",
    "PERSIST_DIR = Path(\"../temp\").resolve()\n",
    "PERSIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHROMA_DIR = PERSIST_DIR / \"chroma_db\"\n",
    "\n",
    "# ---------- components ----------\n",
    "def build_embedding():\n",
    "    return HuggingFaceEmbedding(model_name=\"Qwen/Qwen3-Embedding-0.6B\")\n",
    "\n",
    "def build_vector_store():\n",
    "    client = chromadb.PersistentClient(path=str(CHROMA_DIR))\n",
    "    collection = client.get_or_create_collection(\"patents\")\n",
    "    return ChromaVectorStore(chroma_collection=collection)\n",
    "\n",
    "def read_file_to_documents(path: Path) -> List[Document]:\n",
    "    ext = path.suffix.lower()\n",
    "    if ext == \".pdf\":\n",
    "        return PyMuPDFReader().load_data(str(path))\n",
    "    if ext == \".md\":\n",
    "        return MarkdownReader().load_data(str(path))\n",
    "    # 兜底：按纯文本\n",
    "    text = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    return [Document(text=text, metadata={\"file_name\": path.name})]\n",
    "\n",
    "def show_nodes(nodes: List[BaseNode], max_n: int = 5):\n",
    "    print(\"\\n=== Sample BaseNodes ===\")\n",
    "    for i, n in enumerate(nodes[:max_n], 1):\n",
    "        preview = n.get_content()[:160].replace(\"\\n\", \" \")\n",
    "        print(f\"[{i}] node_id={n.node_id}  ref_doc_id={n.ref_doc_id}\")\n",
    "        print(\"    text:\", preview, \"...\" if len(n.get_content()) > 160 else \"\")\n",
    "        print(\"    meta:\", {k: n.metadata.get(k) for k in list(n.metadata)[:6]})\n",
    "        # 邻接\n",
    "        print(\"    prev:\", getattr(n.prev_node, \"node_id\", None), \"  next:\", getattr(n.next_node, \"node_id\", None))\n",
    "\n",
    "def main(input_path: str, query: str | None = None, use_window: bool = True):\n",
    "    path = Path(input_path).resolve()\n",
    "    assert path.exists(), f\"File not found: {path}\"\n",
    "\n",
    "    # 1) 读取文档\n",
    "    docs = read_file_to_documents(path)\n",
    "    for d in docs:\n",
    "        d.metadata.setdefault(\"file_name\", path.name)\n",
    "        d.metadata[\"doc_id\"] = d.doc_id\n",
    "        # 与 private-gpt 一致：不把这些元数据送进 embedding / LLM\n",
    "        d.excluded_embed_metadata_keys = [\"doc_id\"]\n",
    "        d.excluded_llm_metadata_keys = [\"file_name\", \"doc_id\", \"page_label\"]\n",
    "\n",
    "    # 2) 组件\n",
    "    embed = build_embedding()\n",
    "    vs = build_vector_store()\n",
    "    storage = StorageContext.from_defaults(vector_store=vs)\n",
    "\n",
    "    # 3) 选择分块器\n",
    "    if use_window:\n",
    "        # 与 private-gpt 默认一致：句子 + window（后续对话可用 MetadataReplacementPostProcessor(\"window\")）\n",
    "        parser = SentenceWindowNodeParser.from_defaults()  # window_size 默认 3\n",
    "    else:\n",
    "        # 定长切片示例\n",
    "        parser = SentenceSplitter.from_defaults(chunk_size=600, chunk_overlap=200)\n",
    "\n",
    "    # 4) 先直接跑 transformations 看看“裸的 Node”\n",
    "    nodes = list(run_transformations(docs, [parser, embed], show_progress=True))\n",
    "    show_nodes(nodes, max_n=5)\n",
    "\n",
    "    # 5) 写索引 + 向量库（并持久化）\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        docs,\n",
    "        storage_context=storage,\n",
    "        embed_model=embed,\n",
    "        transformations=[parser],   # 也可只给 parser，embedding 由 embed_model 负责\n",
    "        show_progress=True,\n",
    "    )\n",
    "    index.storage_context.persist(persist_dir=str(PERSIST_DIR))\n",
    "    print(f\"\\n✅ Persisted to: {PERSIST_DIR}\")\n",
    "\n",
    "    # 6) 即时检索（看 NodeWithScore）\n",
    "    if query:\n",
    "        retriever = index.as_retriever(similarity_top_k=5)\n",
    "        results: List[NodeWithScore] = retriever.retrieve(query)\n",
    "        print(\"\\n=== Top matches ===\")\n",
    "        for r in results:\n",
    "            txt = r.node.get_content()[:120].replace(\"\\n\", \" \")\n",
    "            print(f\"score={round(r.score or 0, 3)}  node_id={r.node.node_id}  -> {txt} ...\")\n",
    "\n",
    "    # 7) 模拟“重新加载后”查看 DocStore 里的节点\n",
    "    storage2 = StorageContext.from_defaults(vector_store=vs)\n",
    "    _ = load_index_from_storage(storage2, persist_dir=str(PERSIST_DIR))\n",
    "    ref_infos = storage2.docstore.get_all_ref_doc_info() or {}\n",
    "    print(f\"\\n=== DocStore has {len(ref_infos)} ref docs ===\")\n",
    "    for doc_id, info in list(ref_infos.items())[:3]:\n",
    "        print(\"ref_doc:\", doc_id, \"  meta keys:\", list((info.metadata or {}).keys()))\n",
    "        # 如果你的 LlamaIndex 版本携带 node_ids，可以尝试：\n",
    "        node_ids = getattr(info, \"node_ids\", None)\n",
    "        if node_ids:\n",
    "            some_nodes = storage2.docstore.get_nodes(node_ids[:3])\n",
    "            print(\"  sample nodes:\", [n.node_id for n in some_nodes])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    inp = r\"./demo.md\"\n",
    "    q = \"解析一下专利的摘要\"\n",
    "    main(inp, q)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama-index 解析 专利pdf、专利md（MinerU解析得到的md）\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Any, List, Tuple, Union, Optional \n",
    "import abc,  itertools, multiprocessing, multiprocessing.pool, threading\n",
    "from injector import inject, Injector, singleton \n",
    "from queue import Queue\n",
    "\n",
    "\n",
    "from llama_index.core.data_structs import IndexDict\n",
    "from llama_index.core.embeddings.utils import EmbedType\n",
    "from llama_index.core.indices import VectorStoreIndex, load_index_from_storage\n",
    "from llama_index.core.indices.base import BaseIndex\n",
    "from llama_index.core.ingestion import run_transformations\n",
    "from llama_index.core.schema import BaseNode, Document, TransformComponent\n",
    "from llama_index.core.storage import StorageContext\n",
    "from llama_index.core.readers import StringIterableReader\n",
    "from llama_index.core.readers.base import BaseReader\n",
    "from llama_index.core.readers.json import JSONReader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import logging\n",
    "# Set to 'DEBUG' to have extensive logging turned on, even for libraries\n",
    "ROOT_LOG_LEVEL = \"INFO\"\n",
    "PRETTY_LOG_FORMAT = (\n",
    "    \"%(asctime)s.%(msecs)03d [%(levelname)-8s] %(name)+25s - %(message)s\"\n",
    ")\n",
    "logging.basicConfig(level=ROOT_LOG_LEVEL, format=PRETTY_LOG_FORMAT, datefmt=\"%H:%M:%S\")\n",
    "logging.captureWarnings(True)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# setting\n",
    "local_data_path = zhuanli_path_pdf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# file loader    【\"zhuanli.pdf\", \"zhuanli.md\"】 \"image.jpg\"\n",
    "def _try_loading_included_file_formats() -> dict[str, type[BaseReader]]:\n",
    "    try:\n",
    "        from llama_index.readers.file.docs import PDFReader\n",
    "        from llama_index.readers.file.markdown import MarkdownReader\n",
    "        from llama_index.readers.file.image import ImageReader  # type: ignore\n",
    "    except ImportError as e: raise ImportError(\"import error\")\n",
    "    \n",
    "    default_file_reader_cls: dict[str, type[BaseReader]] = {\n",
    "        \".pdf\": PDFReader,\n",
    "        \".md\": MarkdownReader,\n",
    "        \".jpg\": ImageReader,\n",
    "        \".png\": ImageReader,\n",
    "        \".jpeg\": ImageReader,\n",
    "    }\n",
    "    return default_file_reader_cls\n",
    "file_reader_cls = _try_loading_included_file_formats()\n",
    "file_reader_cls.update({\".json\": JSONReader})\n",
    "class IngestionHelper:\n",
    "    \"\"\"Helper class to transform a file into a list of documents.\n",
    "\n",
    "    This class should be used to transform a file into a list of documents.\n",
    "    These methods are thread-safe (and multiprocessing-safe).\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def transform_file_into_documents(\n",
    "        file_name: str, file_data: Path\n",
    "    ) -> list[Document]:\n",
    "        documents = IngestionHelper._load_file_to_documents(file_name, file_data)\n",
    "        for document in documents:\n",
    "            document.metadata[\"file_name\"] = file_name\n",
    "        IngestionHelper._exclude_metadata(documents)\n",
    "        return documents\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_file_to_documents(file_name: str, file_data: Path) -> list[Document]:\n",
    "        logger.debug(\"Transforming file_name=%s into documents\", file_name)\n",
    "        extension = Path(file_name).suffix\n",
    "        reader_cls = file_reader_cls.get(extension)\n",
    "        if reader_cls is None:\n",
    "            logger.debug(\n",
    "                \"No reader found for extension=%s, using default string reader\",\n",
    "                extension,\n",
    "            )\n",
    "            # Read as a plain text\n",
    "            string_reader = StringIterableReader()\n",
    "            return string_reader.load_data([file_data.read_text()])\n",
    "\n",
    "        logger.debug(\"Specific reader found for extension=%s\", extension)\n",
    "        documents = reader_cls().load_data(file_data)\n",
    "\n",
    "        # Sanitize NUL bytes in text which can't be stored in Postgres\n",
    "        for i in range(len(documents)):\n",
    "            documents[i].text = documents[i].text.replace(\"\\u0000\", \"\")\n",
    "\n",
    "        return documents\n",
    "\n",
    "    @staticmethod\n",
    "    def _exclude_metadata(documents: list[Document]) -> None:\n",
    "        logger.debug(\"Excluding metadata from count=%s documents\", len(documents))\n",
    "        for document in documents:\n",
    "            document.metadata[\"doc_id\"] = document.doc_id\n",
    "            # We don't want the Embeddings search to receive this metadata\n",
    "            document.excluded_embed_metadata_keys = [\"doc_id\"]\n",
    "            # We don't want the LLM to receive these metadata in the context\n",
    "            document.excluded_llm_metadata_keys = [\"file_name\", \"doc_id\", \"page_label\"]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class IngestedDoc(BaseModel):\n",
    "    object: Literal[\"ingest.document\"]\n",
    "    doc_id: str = Field(examples=[\"c202d5e6-7b69-4869-81cc-dd574ee8ee11\"])\n",
    "    doc_metadata: dict[str, Any] | None = Field(\n",
    "        examples=[\n",
    "            {\n",
    "                \"page_label\": \"2\",\n",
    "                \"file_name\": \"Sales Report Q3 2023.pdf\",\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    @staticmethod\n",
    "    def curate_metadata(metadata: dict[str, Any]) -> dict[str, Any]:\n",
    "        \"\"\"Remove unwanted metadata keys.\"\"\"\n",
    "        for key in [\"doc_id\", \"window\", \"original_text\"]:\n",
    "            metadata.pop(key, None)\n",
    "        return metadata\n",
    "\n",
    "    @staticmethod\n",
    "    def from_document(document: Document) -> \"IngestedDoc\":\n",
    "        return IngestedDoc(\n",
    "            object=\"ingest.document\",\n",
    "            doc_id=document.doc_id,\n",
    "            doc_metadata=IngestedDoc.curate_metadata(document.metadata),\n",
    "        )\n",
    "\n",
    "class BaseIngestComponent(abc.ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        storage_context: StorageContext,\n",
    "        embed_model: EmbedType,\n",
    "        transformations: list[TransformComponent],\n",
    "        *args: Any,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        logger.debug(\"Initializing base ingest component type=%s\", type(self).__name__)\n",
    "        self.storage_context = storage_context\n",
    "        self.embed_model = embed_model\n",
    "        self.transformations = transformations\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def ingest(self, file_name: str, file_data: Path) -> list[Document]:\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def bulk_ingest(self, files: list[tuple[str, Path]]) -> list[Document]:\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def delete(self, doc_id: str) -> None:\n",
    "        pass\n",
    "\n",
    "class BaseIngestComponentWithIndex(BaseIngestComponent, abc.ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        storage_context: StorageContext,\n",
    "        embed_model: EmbedType,\n",
    "        transformations: list[TransformComponent],\n",
    "        *args: Any,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(storage_context, embed_model, transformations, *args, **kwargs)\n",
    "\n",
    "        self.show_progress = True\n",
    "        self._index_thread_lock = (\n",
    "            threading.Lock()\n",
    "        )  # Thread lock! Not Multiprocessing lock\n",
    "        self._index = self._initialize_index()\n",
    "\n",
    "    def _initialize_index(self) -> BaseIndex[IndexDict]:\n",
    "        \"\"\"Initialize the index from the storage context.\"\"\"\n",
    "        try:\n",
    "            # Load the index with store_nodes_override=True to be able to delete them\n",
    "            index = load_index_from_storage(\n",
    "                storage_context=self.storage_context,\n",
    "                store_nodes_override=True,  # Force store nodes in index and document stores\n",
    "                show_progress=self.show_progress,\n",
    "                embed_model=self.embed_model,\n",
    "                transformations=self.transformations,\n",
    "            )\n",
    "        except ValueError:\n",
    "            # There are no index in the storage context, creating a new one\n",
    "            logger.info(\"Creating a new vector store index\")\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                [],\n",
    "                storage_context=self.storage_context,\n",
    "                store_nodes_override=True,  # Force store nodes in index and document stores\n",
    "                show_progress=self.show_progress,\n",
    "                embed_model=self.embed_model,\n",
    "                transformations=self.transformations,\n",
    "            )\n",
    "            index.storage_context.persist(persist_dir=local_data_path)\n",
    "        return index\n",
    "\n",
    "    def _save_index(self) -> None:\n",
    "        self._index.storage_context.persist(persist_dir=local_data_path)\n",
    "\n",
    "    def delete(self, doc_id: str) -> None:\n",
    "        with self._index_thread_lock:\n",
    "            # Delete the document from the index\n",
    "            self._index.delete_ref_doc(doc_id, delete_from_docstore=True)\n",
    "\n",
    "            # Save the index\n",
    "            self._save_index()\n",
    "\n",
    "class BatchIngestComponent(BaseIngestComponentWithIndex):\n",
    "    \"\"\"Parallelize the file reading and parsing on multiple CPU core.\n",
    "\n",
    "    This also makes the embeddings to be computed in batches (on GPU or CPU).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        storage_context: StorageContext,\n",
    "        embed_model: EmbedType,\n",
    "        transformations: list[TransformComponent],\n",
    "        count_workers: int,\n",
    "        *args: Any,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(storage_context, embed_model, transformations, *args, **kwargs)\n",
    "        # Make an efficient use of the CPU and GPU, the embedding\n",
    "        # must be in the transformations\n",
    "        assert (\n",
    "            len(self.transformations) >= 2\n",
    "        ), \"Embeddings must be in the transformations\"\n",
    "        assert count_workers > 0, \"count_workers must be > 0\"\n",
    "        self.count_workers = count_workers\n",
    "\n",
    "        self._file_to_documents_work_pool = multiprocessing.Pool(\n",
    "            processes=self.count_workers\n",
    "        )\n",
    "\n",
    "    def ingest(self, file_name: str, file_data: Path) -> list[Document]:\n",
    "        logger.info(\"Ingesting file_name=%s\", file_name)\n",
    "        documents = IngestionHelper.transform_file_into_documents(file_name, file_data)\n",
    "        logger.info(\n",
    "            \"Transformed file=%s into count=%s documents\", file_name, len(documents)\n",
    "        )\n",
    "        logger.debug(\"Saving the documents in the index and doc store\")\n",
    "        return self._save_docs(documents)\n",
    "\n",
    "    def bulk_ingest(self, files: list[tuple[str, Path]]) -> list[Document]:\n",
    "        documents = list(\n",
    "            itertools.chain.from_iterable(\n",
    "                self._file_to_documents_work_pool.starmap(\n",
    "                    IngestionHelper.transform_file_into_documents, files\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        logger.info(\n",
    "            \"Transformed count=%s files into count=%s documents\",\n",
    "            len(files),\n",
    "            len(documents),\n",
    "        )\n",
    "        return self._save_docs(documents)\n",
    "\n",
    "    def _save_docs(self, documents: list[Document]) -> list[Document]:\n",
    "        logger.debug(\"Transforming count=%s documents into nodes\", len(documents))\n",
    "        nodes = run_transformations(\n",
    "            documents,  # type: ignore[arg-type]\n",
    "            self.transformations,\n",
    "            show_progress=self.show_progress,\n",
    "        )\n",
    "        # Locking the index to avoid concurrent writes\n",
    "        with self._index_thread_lock:\n",
    "            logger.info(\"Inserting count=%s nodes in the index\", len(nodes))\n",
    "            self._index.insert_nodes(nodes, show_progress=True)\n",
    "            for document in documents:\n",
    "                self._index.docstore.set_document_hash(\n",
    "                    document.get_doc_id(), document.hash\n",
    "                )\n",
    "            logger.debug(\"Persisting the index and nodes\")\n",
    "            # persist the index and nodes\n",
    "            self._save_index()\n",
    "            logger.debug(\"Persisted the index and nodes\")\n",
    "        return documents\n",
    "\n",
    "\n",
    "\n",
    "def get_ingestion_component(\n",
    "    storage_context: StorageContext,\n",
    "    embed_model: EmbedType,\n",
    "    transformations: list[TransformComponent],\n",
    "    settings: Settings,\n",
    ") -> BaseIngestComponent:\n",
    "    \"\"\"Get the ingestion component for the given configuration.\"\"\"\n",
    "    ingest_mode = settings.embedding.ingest_mode\n",
    "    if ingest_mode == \"batch\":\n",
    "        return BatchIngestComponent(\n",
    "            storage_context=storage_context,\n",
    "            embed_model=embed_model,\n",
    "            transformations=transformations,\n",
    "            count_workers=settings.embedding.count_workers,\n",
    "        )\n",
    "\n",
    "    elif ingest_mode == \"pipeline\":\n",
    "        return PipelineIngestComponent(\n",
    "            storage_context=storage_context,\n",
    "            embed_model=embed_model,\n",
    "            transformations=transformations,\n",
    "            count_workers=settings.embedding.count_workers,\n",
    "        )\n",
    "    else:\n",
    "        return SimpleIngestComponent(\n",
    "            storage_context=storage_context,\n",
    "            embed_model=embed_model,\n",
    "            transformations=transformations,\n",
    "        )\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "@singleton\n",
    "class IngestService:\n",
    "    @inject\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_component: LLMComponent,\n",
    "        vector_store_component: VectorStoreComponent,\n",
    "        embedding_component: EmbeddingComponent,\n",
    "        node_store_component: NodeStoreComponent,\n",
    "    ) -> None:\n",
    "        self.llm_service = llm_component\n",
    "        self.storage_context = StorageContext.from_defaults(\n",
    "            vector_store=vector_store_component.vector_store,\n",
    "            docstore=node_store_component.doc_store,\n",
    "            index_store=node_store_component.index_store,\n",
    "        )\n",
    "        node_parser = SentenceWindowNodeParser.from_defaults()\n",
    "\n",
    "        self.ingest_component = get_ingestion_component(\n",
    "            self.storage_context,\n",
    "            embed_model=embedding_component.embedding_model,\n",
    "            transformations=[node_parser, embedding_component.embedding_model],\n",
    "            settings=settings(),\n",
    "        )\n",
    "\n",
    "    def _ingest_data(self, file_name: str, file_data: AnyStr) -> list[IngestedDoc]:\n",
    "        logger.debug(\"Got file data of size=%s to ingest\", len(file_data))\n",
    "        # llama-index mainly supports reading from files, so\n",
    "        # we have to create a tmp file to read for it to work\n",
    "        # delete=False to avoid a Windows 11 permission error.\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tmp:\n",
    "            try:\n",
    "                path_to_tmp = Path(tmp.name)\n",
    "                if isinstance(file_data, bytes):\n",
    "                    path_to_tmp.write_bytes(file_data)\n",
    "                else:\n",
    "                    path_to_tmp.write_text(str(file_data))\n",
    "                return self.ingest_file(file_name, path_to_tmp)\n",
    "            finally:\n",
    "                tmp.close()\n",
    "                path_to_tmp.unlink()\n",
    "\n",
    "    def ingest_file(self, file_name: str, file_data: Path) -> list[IngestedDoc]:\n",
    "        logger.info(\"Ingesting file_name=%s\", file_name)\n",
    "        documents = self.ingest_component.ingest(file_name, file_data)\n",
    "        logger.info(\"Finished ingestion file_name=%s\", file_name)\n",
    "        return [IngestedDoc.from_document(document) for document in documents]\n",
    "\n",
    "    def ingest_text(self, file_name: str, text: str) -> list[IngestedDoc]:\n",
    "        logger.debug(\"Ingesting text data with file_name=%s\", file_name)\n",
    "        return self._ingest_data(file_name, text)\n",
    "\n",
    "    def ingest_bin_data(\n",
    "        self, file_name: str, raw_file_data: BinaryIO\n",
    "    ) -> list[IngestedDoc]:\n",
    "        logger.debug(\"Ingesting binary data with file_name=%s\", file_name)\n",
    "        file_data = raw_file_data.read()\n",
    "        return self._ingest_data(file_name, file_data)\n",
    "\n",
    "    def bulk_ingest(self, files: list[tuple[str, Path]]) -> list[IngestedDoc]:\n",
    "        logger.info(\"Ingesting file_names=%s\", [f[0] for f in files])\n",
    "        documents = self.ingest_component.bulk_ingest(files)\n",
    "        logger.info(\"Finished ingestion file_name=%s\", [f[0] for f in files])\n",
    "        return [IngestedDoc.from_document(document) for document in documents]\n",
    "\n",
    "    def list_ingested(self) -> list[IngestedDoc]:\n",
    "        ingested_docs: list[IngestedDoc] = []\n",
    "        try:\n",
    "            docstore = self.storage_context.docstore\n",
    "            ref_docs: dict[str, RefDocInfo] | None = docstore.get_all_ref_doc_info()\n",
    "\n",
    "            if not ref_docs:\n",
    "                return ingested_docs\n",
    "\n",
    "            for doc_id, ref_doc_info in ref_docs.items():\n",
    "                doc_metadata = None\n",
    "                if ref_doc_info is not None and ref_doc_info.metadata is not None:\n",
    "                    doc_metadata = IngestedDoc.curate_metadata(ref_doc_info.metadata)\n",
    "                ingested_docs.append(\n",
    "                    IngestedDoc(\n",
    "                        object=\"ingest.document\",\n",
    "                        doc_id=doc_id,\n",
    "                        doc_metadata=doc_metadata,\n",
    "                    )\n",
    "                )\n",
    "        except ValueError:\n",
    "            logger.warning(\"Got an exception when getting list of docs\", exc_info=True)\n",
    "            pass\n",
    "        logger.debug(\"Found count=%s ingested documents\", len(ingested_docs))\n",
    "        return ingested_docs\n",
    "\n",
    "    def delete(self, doc_id: str) -> None:\n",
    "        \"\"\"Delete an ingested document.\n",
    "\n",
    "        :raises ValueError: if the document does not exist\n",
    "        \"\"\"\n",
    "        logger.info(\n",
    "            \"Deleting the ingested document=%s in the doc and index store\", doc_id\n",
    "        )\n",
    "        self.ingest_component.delete(doc_id)\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
