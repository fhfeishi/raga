# agents/rag_agent.py
"""
RAG Agent æ¨¡å—ï¼šè´Ÿè´£æ£€ç´¢ä¸ç”Ÿæˆçš„æ ¸å¿ƒé€»è¾‘
æ”¯æŒ HuggingFace / Ollama æ¨¡å‹
æ”¯æŒæµå¼è¾“å‡ºï¼Œå¯è¿‡æ»¤ <think> ç­‰ä¸­é—´æ¨ç†æ ‡è®°
"""


from langchain_core.prompts import ChatPromptTemplate
import time 
from typing import Iterator
from models.chat_model import chat_model_cratefn
from models.embedding_model import enbedding_model_cratefn
from data_db.vector_db import vector_store_cratefn
from agents.prompts import SYSTEM_PROMPT

import logging 
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')


class RAGAgent:
    def __init__(self, mode='auto'):

        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = enbedding_model_cratefn()

        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.vectordb = vector_store_cratefn()

        # åˆå§‹åŒ– LLM
        self.llm = chat_model_cratefn()

        # åˆå§‹åŒ–æç¤ºè¯
        self.prompt = self._create_prompt(prompt=SYSTEM_PROMPT)

 
    def _create_prompt(self, prompt) -> ChatPromptTemplate:
        return ChatPromptTemplate.from_messages([
            ("system", prompt),
            ("human", "{question}")
        ])

    def retrieve_context(self, question: str, k: int = 2) -> str:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µ"""
        docs = self.vectordb.similarity_search(question, k=k)
        return "\n\n".join(d.page_content for d in docs)

    def _stream_response(self, question: str, k: int = 2, hide_think: bool = True) -> Iterator[str]:
        """
        æµå¼ç”Ÿæˆå›ç­”ï¼Œå¯é€‰æ‹©æ˜¯å¦è¿‡æ»¤ <think> ç­‰ä¸­é—´æ¨ç†æ ‡è®°
        æµå¼ç”Ÿæˆå›ç­”ï¼Œå…¼å®¹ Linux/Ubuntu ç¼“å†²é—®é¢˜
        """
        t0 = time.perf_counter()
        context = self.retrieve_context(question, k=k)
        logger.info(f"ğŸ” æ£€ç´¢è€—æ—¶: {(time.perf_counter() - t0)*1000:.0f} ms")
        logger.debug(f"æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡: {context}")

        # æ„é€ æ¶ˆæ¯
        messages = self.prompt.format_prompt(question=question, context=context).to_messages()

        first_token_time = None
        if hide_think:
            START_TAG = "</think>"
            MAX_TAG = 10
            ring = ""
            in_answer = False

            for chunk in self.llm.stream(messages):
                content = chunk.content or ""
                if not content:
                    continue

                ring += content
                if len(ring) > MAX_TAG:
                    ring = ring[-MAX_TAG:]

                if not in_answer:
                    idx = ring.lower().find(START_TAG)
                    if idx != -1:
                        post = ring[idx + len(START_TAG):]
                        if post:
                            yield post
                            if first_token_time is None:
                                first_token_time = time.perf_counter()
                        in_answer = True
                        ring = ""
                    continue

                # å·²è¿›å…¥å›ç­”ï¼Œæ­£å¸¸è¾“å‡º
                if content:
                    if first_token_time is None:
                        first_token_time = time.perf_counter()
                    yield content
        else:
            for chunk in self.llm.stream(messages):
                content = chunk.content or ""
                if content:
                    if first_token_time is None:
                        first_token_time = time.perf_counter()
                    yield content

        total_time = time.perf_counter() - t0
        first_token_latency = (first_token_time - t0) * 1000 if first_token_time else 0
        logger.info(f"â±ï¸ é¦– token å»¶è¿Ÿ: {first_token_latency:.0f} ms | æ€»è€—æ—¶: {total_time*1000:.0f} ms")

    def stream_tokens(self, question: str, k: int = 2, hide_think: bool = True) -> Iterator[str]:
        """å¯¹å¤–æš´éœ²çš„æµå¼ token ç”Ÿæˆæ¥å£"""
        yield from self._stream_response(question, k=k, hide_think=hide_think)