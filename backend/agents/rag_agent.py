# agents/rag_agent.py
"""
RAG Agent æ¨¡å—ï¼šè´Ÿè´£æ£€ç´¢ä¸ç”Ÿæˆçš„æ ¸å¿ƒé€»è¾‘
æ”¯æŒ HuggingFace / Ollama æ¨¡å‹
æ”¯æŒæµå¼è¾“å‡ºï¼Œå¯è¿‡æ»¤ <think> ç­‰ä¸­é—´æ¨ç†æ ‡è®°
"""

from langchain_core.prompts import ChatPromptTemplate
import time 
from typing import Iterator, List 
from models.chat_model import chat_model_cratefn
from models.embedding_model import enbedding_model_cratefn
from data_db.vector_db import vector_store_cratefn
from models.rerank_model import rerank_model_createfn
from agents.prompts import SYSTEM_PROMPT

import logging 
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')


class RAGAgent:
    def __init__(
        self, 
        mode: str='auto',
        use_rerank: bool = True,
        pre_k: int=20, # ç²—æ’å¬å›æ•°é‡ï¼ˆå‘é‡æ£€ç´¢ï¼‰
        top_k: int=4   # ç²¾æ’ä¿ç•™æ•°é‡ï¼ˆäº¤å‰ç¼–ç å™¨ï¼‰
        ):
        
        """
        :param use_rerank: æ˜¯å¦å¯ç”¨äº¤å‰ç¼–ç å™¨ Re-rank
        :param pre_k: å‘é‡ç²—æ’å¬å›æ•°é‡
        :param top_k: äº¤å‰ç¼–ç å™¨ç²¾æ’åä¿ç•™æ•°é‡ï¼ˆè¿›å…¥ prompt çš„æ–‡æ¡£æ•°ï¼‰
        """

        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = enbedding_model_cratefn()

        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.vectordb = vector_store_cratefn()

        # åˆå§‹åŒ– LLM
        self.llm = chat_model_cratefn()

        # åˆå§‹åŒ–æç¤ºè¯
        self.prompt = self._create_prompt(prompt=SYSTEM_PROMPT)

        self.use_rerank = use_rerank 
        self.pre_k = pre_k
        self.top_k = top_k 
        
        self.reranker = None 
        if self.use_rerank:
            try:
                self.reranker = rerank_model_createfn()
                logger.info("ğŸ”§ Reranker å·²å¯ç”¨")
            except Exception as e:
                logger.warning(f"âš ï¸ Reranker åŠ è½½å¤±è´¥ï¼Œé™çº§ä¸ºçº¯å‘é‡æ£€ç´¢: {e}")
                self.reranker = None
        
        
 
    def _create_prompt(self, prompt) -> ChatPromptTemplate:
        return ChatPromptTemplate.from_messages([
            ("system", prompt),
            ("human", "{question}")
        ])

    # ----- æ£€é”™ï¼ˆæ”¯æŒç²—æ’+ç²¾æ’ï¼‰
    def retrieve_docs(self, question: str) -> List:
        t0 = time.perf_counter()
        # 1) å‘é‡ç²—æ’
        rough_docs = self.vectordb.similarity_search(question, k=self.pre_k)
        t_vec = (time.perf_counter() - t0) * 1000
        logger.info(f"ğŸ” å‘é‡æ£€ç´¢: {len(rough_docs)} æ¡ï¼Œç”¨æ—¶ {t_vec:.0f} ms")

        # 2) äº¤å‰ç¼–ç å™¨ç²¾æ’
        if self.reranker is not None and rough_docs:
            try: 
                t1 = time.perf_counter()
                ranked = self.reranker.rerank(
                    question, 
                    rough_docs, 
                    top_k=self.top_k, 
                    with_score=True,
                    batch_size=8)  #  1 8 16
                t_rerank = (time.perf_counter() - t1) * 1000
                logger.info(f"ğŸ Re-rank å®Œæˆ: å– Top-{self.top_k}ï¼Œç”¨æ—¶ {t_rerank:.0f} ms")
                # åªè¿”å›æ–‡æ¡£ï¼Œå¿…è¦æ—¶ä½ ä¹Ÿå¯ä»¥æŠŠåˆ†æ•°å¸¦å›å»åšè°ƒè¯•
                return [item["doc"] for item in ranked]
            except Exception as e:
                logger.warning(f"âš ï¸ Re-rank å¤±è´¥ï¼Œé™çº§çº¯å‘é‡æ£€ç´¢: {e}")
            
        else:
            # æ—  reranker æ—¶ï¼Œç›´æ¥å–å‰ top_k
            return rough_docs[: self.top_k]

    def retrieve_context(self, question: str) -> str:
        """æ‹¼æ¥æ–‡æ¡£å†…å®¹ä¸ºå•ä¸€ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
        docs = self.retrieve_docs(question)
        return "\n\n".join(getattr(d, "page_content", str(d)) for d in docs)

    # ------ æµå¼ ç”Ÿæˆ
    def _stream_response(self, question: str, hide_think: bool = True) -> Iterator[str]:
        """
        æµå¼ç”Ÿæˆå›ç­”ï¼Œå¯é€‰æ‹©æ˜¯å¦è¿‡æ»¤ <think> ç­‰ä¸­é—´æ¨ç†æ ‡è®°
        æµå¼ç”Ÿæˆå›ç­”ï¼Œå…¼å®¹ Linux/Ubuntu ç¼“å†²é—®é¢˜
        """
        t0 = time.perf_counter()
        context = self.retrieve_context(question)
        logger.debug(f"æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡: {context}")

        # æ„é€ æ¶ˆæ¯
        messages = self.prompt.format_prompt(question=question, context=context).to_messages()

        first_token_time = None
        if hide_think:
            START_TAG = "</think>"
            MAX_TAG = 10
            ring = ""
            in_answer = False

            for chunk in self.llm.stream(messages):
                content = chunk.content or ""
                if not content:
                    continue

                ring += content
                if len(ring) > MAX_TAG:
                    ring = ring[-MAX_TAG:]

                if not in_answer:
                    idx = ring.lower().find(START_TAG)
                    if idx != -1:
                        post = ring[idx + len(START_TAG):]
                        if post:
                            yield post
                            if first_token_time is None:
                                first_token_time = time.perf_counter()
                        in_answer = True
                        ring = ""
                    continue

                # å·²è¿›å…¥å›ç­”ï¼Œæ­£å¸¸è¾“å‡º
                if content:
                    if first_token_time is None:
                        first_token_time = time.perf_counter()
                    yield content
        else:
            for chunk in self.llm.stream(messages):
                content = chunk.content or ""
                if content:
                    if first_token_time is None:
                        first_token_time = time.perf_counter()
                    yield content

        total_time = time.perf_counter() - t0
        first_token_latency = (first_token_time - t0) * 1000 if first_token_time else 0
        logger.info(f"â±ï¸ é¦– token å»¶è¿Ÿ: {first_token_latency:.0f} ms | æ€»è€—æ—¶: {total_time*1000:.0f} ms")

    def stream_tokens(self, question: str, hide_think: bool = True) -> Iterator[str]:
        """å¯¹å¤–æš´éœ²çš„æµå¼ token ç”Ÿæˆæ¥å£"""
        yield from self._stream_response(question, hide_think=hide_think)