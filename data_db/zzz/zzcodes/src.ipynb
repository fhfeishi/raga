{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13b4fa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:05:30.626 [INFO    ] private_gpt.settings.settings_loader - Starting application with profiles=['default']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'd:\\\\codespace\\\\fhfeishi\\\\data_db\\\\settings.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mzzb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_injector, IngestService\n\u001b[32m      4\u001b[39m inj = build_injector()\n\u001b[32m      5\u001b[39m svc = inj.get(IngestService)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\codespace\\fhfeishi\\data_db\\zzb\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# __init__.py\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mingest\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IngestService\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_injector\n\u001b[32m      5\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mIngestService\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mbuild_injector\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\codespace\\fhfeishi\\data_db\\zzb\\ingest.py:13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstorage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StorageContext\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# 复用原组件（保留工程师风格与行为）\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprivate_gpt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcomponents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membedding\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membedding_component\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EmbeddingComponent\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprivate_gpt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcomponents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_component\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMComponent\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprivate_gpt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcomponents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnode_store\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnode_store_component\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NodeStoreComponent\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\codespace\\fhfeishi\\data_db\\private_gpt\\components\\embedding\\embedding_component.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minjector\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m inject, singleton\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseEmbedding, MockEmbedding\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprivate_gpt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpaths\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m models_cache_path\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprivate_gpt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msettings\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msettings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Settings\n\u001b[32m      9\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\codespace\\fhfeishi\\data_db\\private_gpt\\paths.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprivate_gpt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PROJECT_ROOT_PATH\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprivate_gpt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msettings\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msettings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m settings\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_absolute_or_from_project_root\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) -> Path:\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m path.startswith(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\codespace\\fhfeishi\\data_db\\private_gpt\\settings\\settings.py:618\u001b[39m\n\u001b[32m    610\u001b[39m     milvus: MilvusSettings | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    613\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    614\u001b[39m \u001b[33;03mThis is visible just for DI or testing purposes.\u001b[39;00m\n\u001b[32m    615\u001b[39m \n\u001b[32m    616\u001b[39m \u001b[33;03mUse dependency injection or `settings()` method instead.\u001b[39;00m\n\u001b[32m    617\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m618\u001b[39m unsafe_settings = \u001b[43mload_active_settings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    621\u001b[39m \u001b[33;03mThis is visible just for DI or testing purposes.\u001b[39;00m\n\u001b[32m    622\u001b[39m \n\u001b[32m    623\u001b[39m \u001b[33;03mUse dependency injection or `settings()` method instead.\u001b[39;00m\n\u001b[32m    624\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    625\u001b[39m unsafe_typed_settings = Settings(**unsafe_settings)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\codespace\\fhfeishi\\data_db\\private_gpt\\settings\\settings_loader.py:54\u001b[39m, in \u001b[36mload_active_settings\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load active profiles and merge them.\"\"\"\u001b[39;00m\n\u001b[32m     52\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mStarting application with profiles=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, active_profiles)\n\u001b[32m     53\u001b[39m loaded_profiles = [\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     \u001b[43mload_settings_from_profile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m profile \u001b[38;5;129;01min\u001b[39;00m active_profiles\n\u001b[32m     55\u001b[39m ]\n\u001b[32m     56\u001b[39m merged: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = merge_settings(loaded_profiles)\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m merged\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\codespace\\fhfeishi\\data_db\\private_gpt\\settings\\settings_loader.py:43\u001b[39m, in \u001b[36mload_settings_from_profile\u001b[39m\u001b[34m(profile)\u001b[39m\n\u001b[32m     40\u001b[39m     profile_file_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msettings-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprofile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.yaml\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     42\u001b[39m path = Path(_settings_folder) / profile_file_name\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     44\u001b[39m     config = load_yaml_with_envvars(f)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\pathlib.py:1013\u001b[39m, in \u001b[36mPath.open\u001b[39m\u001b[34m(self, mode, buffering, encoding, errors, newline)\u001b[39m\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1012\u001b[39m     encoding = io.text_encoding(encoding)\n\u001b[32m-> \u001b[39m\u001b[32m1013\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'd:\\\\codespace\\\\fhfeishi\\\\data_db\\\\settings.yaml'"
     ]
    }
   ],
   "source": [
    "# zzb  需要  settings.yaml\n",
    "from pathlib import Path\n",
    "from zzb import build_injector, IngestService\n",
    "\n",
    "inj = build_injector()\n",
    "svc = inj.get(IngestService)\n",
    "\n",
    "\n",
    "mdp = r\"D:\\codespace\\fhfeishi\\raga\\scripts\\PatentParser\\full.md\"\n",
    "docs = svc.ingest_file(mdp, Path(\"mdp\"))\n",
    "print(\"ingested:\", [d.doc_id for d in docs])\n",
    "\n",
    "print(\"list:\", svc.list_ingested()[:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "local_dir = r\"E:\\local_models\\huggingface\\cache\\hub\"\n",
    "\n",
    "embeds = HuggingFaceEmbedding(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    device=\"cpu\",                 # 建议放顶层\n",
    "    trust_remote_code=True,       # 建议放顶层\n",
    "    cache_folder=local_dir,\n",
    "    model_kwargs={\"local_files_only\": True},   # 允许联网 False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a98f258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py: d:\\developer\\miniconda\\envs\\langchain\\python.exe\n",
      "tf: 4.56.0 hub: 0.34.4 st: 5.0.0\n",
      "HF_HOME: E:\\local_models\\huggingface\\cache\n",
      "HF_ENDPOINT: https://mirrors.tuna.tsinghua.edu.cn/hugging-face/\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "\n",
    "import sys, transformers, huggingface_hub, sentence_transformers\n",
    "print(\"py:\", sys.executable)\n",
    "print(\"tf:\", transformers.__version__, \"hub:\", huggingface_hub.__version__, \"st:\", sentence_transformers.__version__)\n",
    "print(\"HF_HOME:\", os.getenv(\"HF_HOME\"))\n",
    "print(\"HF_ENDPOINT:\", os.getenv(\"HF_ENDPOINT\"))\n",
    "\n",
    "\n",
    "# —— 离线&缓存环境（Windows 下也OK）——\n",
    "os.environ[\"HF_HOME\"] = r\"E:\\local_models\\huggingface\\cache\"\n",
    "# os.environ[\"HF_HUB_OFFLINE\"] = \"0\"\n",
    "# os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "# os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"  # 关闭 symlink 警告\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://mirrors.tuna.tsinghua.edu.cn/hugging-face/\"\n",
    "\n",
    "\n",
    "name_or_path = \"Qwen/Qwen3-Embedding-0.6B\"  # 也可换成本地目录（见方案2）\n",
    "device = \"cpu\"  # 或 \"cuda:0\"（如果有GPU）\n",
    "\n",
    "# 第一次若缓存不全，把 local_files_only 改为 False 联网补齐一次\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    name_or_path, trust_remote_code=True, local_files_only=False,\n",
    "    cache_dir=os.environ[\"HF_HOME\"],\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    name_or_path, trust_remote_code=True, local_files_only=True,\n",
    "    cache_dir=os.environ[\"HF_HOME\"],\n",
    ").to(device).eval()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6416cff9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Transformer._load_model() got multiple values for argument 'cache_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:800\u001b[39m, in \u001b[36mSingletonScope.get\u001b[39m\u001b[34m(self, key, provider)\u001b[39m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m800\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: <class 'zzc.di.EmbeddingComponent'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m inj = build_injector()\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 先冒烟：仅加载嵌入模型，验证“缓存+离线”是否OK\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m _ = \u001b[43minj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEmbeddingComponent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m svc: IngestService = inj.get(IngestService)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# 路径：确保与当前运行 OS 一致\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:91\u001b[39m, in \u001b[36msynchronized.<locals>.outside_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(function)\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any) -> Any:\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m lock:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:976\u001b[39m, in \u001b[36mInjector.get\u001b[39m\u001b[34m(self, interface, scope)\u001b[39m\n\u001b[32m    971\u001b[39m scope_instance = scope_binding.provider.get(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    973\u001b[39m log.debug(\n\u001b[32m    974\u001b[39m     \u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33mInjector.get(\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m, scope=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m) using \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_prefix, interface, scope, binding.provider\n\u001b[32m    975\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m976\u001b[39m provider_instance = \u001b[43mscope_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinding\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    977\u001b[39m result = provider_instance.get(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    978\u001b[39m log.debug(\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m -> \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_prefix, result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:91\u001b[39m, in \u001b[36msynchronized.<locals>.outside_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(function)\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any) -> Any:\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m lock:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:802\u001b[39m, in \u001b[36mSingletonScope.get\u001b[39m\u001b[34m(self, key, provider)\u001b[39m\n\u001b[32m    800\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._context[key]\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m802\u001b[39m     instance = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minjector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    803\u001b[39m     provider = InstanceProvider(instance)\n\u001b[32m    804\u001b[39m     \u001b[38;5;28mself\u001b[39m._context[key] = provider\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:813\u001b[39m, in \u001b[36mSingletonScope._get_instance\u001b[39m\u001b[34m(self, key, provider, injector)\u001b[39m\n\u001b[32m    811\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (CallError, UnsatisfiedRequirement):\n\u001b[32m    812\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m813\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprovider\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43minjector\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:302\u001b[39m, in \u001b[36mCallableProvider.get\u001b[39m\u001b[34m(self, injector)\u001b[39m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, injector: \u001b[33m'\u001b[39m\u001b[33mInjector\u001b[39m\u001b[33m'\u001b[39m) -> T:\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minjector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_with_injection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_callable\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:1052\u001b[39m, in \u001b[36mInjector.call_with_injection\u001b[39m\u001b[34m(self, callable, self_, args, kwargs)\u001b[39m\n\u001b[32m   1050\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(*full_args, **dependencies)\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1052\u001b[39m     \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCallError\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdependencies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stack\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m     \u001b[38;5;66;03m# Needed because of a mypy-related issue (https://github.com/python/mypy/issues/8129).\u001b[39;00m\n\u001b[32m   1054\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33munreachable\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:190\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(original, exception, maximum_frames)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(frames) > maximum_frames:\n\u001b[32m    189\u001b[39m     exception = original\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception.with_traceback(tb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:1050\u001b[39m, in \u001b[36mInjector.call_with_injection\u001b[39m\u001b[34m(self, callable, self_, args, kwargs)\u001b[39m\n\u001b[32m   1047\u001b[39m dependencies.update(kwargs)\n\u001b[32m   1049\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1050\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdependencies\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1052\u001b[39m     reraise(e, CallError(self_, \u001b[38;5;28mcallable\u001b[39m, args, dependencies, e, \u001b[38;5;28mself\u001b[39m._stack))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\codespace\\fhfeishi\\raga\\data_db\\private_gpt_method\\zzc\\di.py:59\u001b[39m, in \u001b[36mPgmModule.provide_embedding_component\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;129m@singleton\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;129m@provider\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprovide_embedding_component\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> EmbeddingComponent:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEmbeddingComponent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\codespace\\fhfeishi\\raga\\data_db\\private_gpt_method\\zzc\\di.py:24\u001b[39m, in \u001b[36mEmbeddingComponent.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     22\u001b[39m     model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mcache_folder\u001b[39m\u001b[33m\"\u001b[39m] = cfg.cache_dir\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# llama-index 的 HF 封装内部会调用 transformers / sentence-transformers\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28mself\u001b[39m.embedding_model = \u001b[43mHuggingFaceEmbedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ↓ 传给 sentence-transformers/transformers 的参数\u001b[39;49;00m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal_files_only\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcache_dir\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# encode_kwargs 可根据模型需要扩展\u001b[39;49;00m\n\u001b[32m     34\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\llama_index\\embeddings\\huggingface\\base.py:160\u001b[39m, in \u001b[36mHuggingFaceEmbedding.__init__\u001b[39m\u001b[34m(self, model_name, tokenizer_name, pooling, max_length, query_instruction, text_instruction, normalize, model, tokenizer, embed_batch_size, cache_folder, trust_remote_code, device, callback_manager, parallel_process, target_devices, show_progress_bar, **model_kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe `model_name` argument must be provided.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m model = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_instruction\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_query_instruct_for_model_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_instruction\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_text_instruct_for_model_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_length:\n\u001b[32m    174\u001b[39m     model.max_seq_length = max_length\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:327\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    309\u001b[39m has_modules = is_sentence_transformer_model(\n\u001b[32m    310\u001b[39m     model_name_or_path,\n\u001b[32m    311\u001b[39m     token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    314\u001b[39m     local_files_only=local_files_only,\n\u001b[32m    315\u001b[39m )\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    317\u001b[39m     has_modules\n\u001b[32m    318\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_model_type(\n\u001b[32m   (...)\u001b[39m\u001b[32m    325\u001b[39m     == \u001b[38;5;28mself\u001b[39m._model_config[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    326\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    339\u001b[39m     modules = \u001b[38;5;28mself\u001b[39m._load_auto_model(\n\u001b[32m    340\u001b[39m         model_name_or_path,\n\u001b[32m    341\u001b[39m         token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    349\u001b[39m         has_modules=has_modules,\n\u001b[32m    350\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:2254\u001b[39m, in \u001b[36mSentenceTransformer._load_sbert_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[39m\n\u001b[32m   2249\u001b[39m         module = module_class.load(local_path)\n\u001b[32m   2251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2252\u001b[39m     \u001b[38;5;66;03m# Newer modules that support the new loading method are loaded with the new style\u001b[39;00m\n\u001b[32m   2253\u001b[39m     \u001b[38;5;66;03m# i.e. with many keyword arguments that can optionally be used by the modules\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2254\u001b[39m     module = \u001b[43mmodule_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2256\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Loading-specific keyword arguments\u001b[39;49;00m\n\u001b[32m   2257\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodule_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2262\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Module-specific keyword arguments\u001b[39;49;00m\n\u001b[32m   2263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2270\u001b[39m modules[module_config[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]] = module\n\u001b[32m   2271\u001b[39m module_kwargs[module_config[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]] = module_config.get(\u001b[33m\"\u001b[39m\u001b[33mkwargs\u001b[39m\u001b[33m\"\u001b[39m, [])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:541\u001b[39m, in \u001b[36mTransformer.load\u001b[39m\u001b[34m(cls, model_name_or_path, subfolder, token, cache_folder, revision, local_files_only, trust_remote_code, model_kwargs, tokenizer_kwargs, config_kwargs, backend, **kwargs)\u001b[39m\n\u001b[32m    510\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m    512\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    526\u001b[39m     **kwargs,\n\u001b[32m    527\u001b[39m ) -> Self:\n\u001b[32m    528\u001b[39m     init_kwargs = \u001b[38;5;28mcls\u001b[39m._load_init_kwargs(\n\u001b[32m    529\u001b[39m         model_name_or_path=model_name_or_path,\n\u001b[32m    530\u001b[39m         subfolder=subfolder,\n\u001b[32m   (...)\u001b[39m\u001b[32m    539\u001b[39m         backend=backend,\n\u001b[32m    540\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:88\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[39m\n\u001b[32m     85\u001b[39m     config_args = {}\n\u001b[32m     87\u001b[39m config, is_peft_model = \u001b[38;5;28mself\u001b[39m._load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_peft_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[32m     91\u001b[39m     tokenizer_args[\u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m] = max_seq_length\n",
      "\u001b[31mTypeError\u001b[39m: Transformer._load_model() got multiple values for argument 'cache_dir'"
     ]
    }
   ],
   "source": [
    "# test zzc\n",
    "import os\n",
    "from pathlib import Path\n",
    "from zzc import build_injector, IngestService\n",
    "from zzc.di import EmbeddingComponent  # 仅用于冒烟\n",
    "\n",
    "# --- 环境变量 ---\n",
    "os.environ['PGM_EMBED_MODEL'] = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "os.environ['PGM_DEVICE'] = \"cpu\"             # 无GPU就改 \"cpu\"\n",
    "os.environ['PGM_LOCAL_FILES_ONLY'] = \"0\"        # 首次跑若没缓存，改成 \"0\"\n",
    "os.environ['PGM_CHROMA_DIR'] = \"/data/pgm/chroma\"\n",
    "os.environ['PGM_CHROMA_COLLECTION'] = \"pgm_collection\"\n",
    "os.environ['PGM_PERSIST_DIR'] = \"/data/pgm/storage\"\n",
    "os.environ['PGM_SENT_WINDOW'] = \"3\"\n",
    "\n",
    "inj = build_injector()\n",
    "\n",
    "# 先冒烟：仅加载嵌入模型，验证“缓存+离线”是否OK\n",
    "_ = inj.get(EmbeddingComponent)\n",
    "\n",
    "svc: IngestService = inj.get(IngestService)\n",
    "\n",
    "# 路径：确保与当前运行 OS 一致\n",
    "mdp = r\"D:\\codespace\\fhfeishi\\raga\\scripts\\PatentParser\\full.md\"\n",
    "p = Path(mdp)\n",
    "assert p.exists(), f\"文件不存在：{p}（注意当前运行环境的操作系统路径）\"\n",
    "\n",
    "docs = svc.ingest_file(p.name, p)\n",
    "print([d.doc_id for d in docs])\n",
    "print(svc.list_ingested()[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a493a6a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Transformer._load_model() got multiple values for argument 'cache_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:800\u001b[39m, in \u001b[36mSingletonScope.get\u001b[39m\u001b[34m(self, key, provider)\u001b[39m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m800\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: <class 'zzc.ingest.IngestService'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:800\u001b[39m, in \u001b[36mSingletonScope.get\u001b[39m\u001b[34m(self, key, provider)\u001b[39m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m800\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: <class 'zzc.di.EmbeddingComponent'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mzzc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_injector, IngestService\n\u001b[32m     17\u001b[39m inj = build_injector()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m svc: IngestService = \u001b[43minj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIngestService\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m mdp = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mD:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mcodespace\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mfhfeishi\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mraga\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mscripts\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mPatentParser\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mfull.md\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m docs = svc.ingest_file(mdp, Path(mdp))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:91\u001b[39m, in \u001b[36msynchronized.<locals>.outside_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(function)\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any) -> Any:\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m lock:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:976\u001b[39m, in \u001b[36mInjector.get\u001b[39m\u001b[34m(self, interface, scope)\u001b[39m\n\u001b[32m    971\u001b[39m scope_instance = scope_binding.provider.get(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    973\u001b[39m log.debug(\n\u001b[32m    974\u001b[39m     \u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33mInjector.get(\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m, scope=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m) using \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_prefix, interface, scope, binding.provider\n\u001b[32m    975\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m976\u001b[39m provider_instance = \u001b[43mscope_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinding\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    977\u001b[39m result = provider_instance.get(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    978\u001b[39m log.debug(\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m -> \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_prefix, result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:91\u001b[39m, in \u001b[36msynchronized.<locals>.outside_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(function)\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any) -> Any:\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m lock:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:802\u001b[39m, in \u001b[36mSingletonScope.get\u001b[39m\u001b[34m(self, key, provider)\u001b[39m\n\u001b[32m    800\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._context[key]\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m802\u001b[39m     instance = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minjector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    803\u001b[39m     provider = InstanceProvider(instance)\n\u001b[32m    804\u001b[39m     \u001b[38;5;28mself\u001b[39m._context[key] = provider\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:813\u001b[39m, in \u001b[36mSingletonScope._get_instance\u001b[39m\u001b[34m(self, key, provider, injector)\u001b[39m\n\u001b[32m    811\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (CallError, UnsatisfiedRequirement):\n\u001b[32m    812\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m813\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprovider\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43minjector\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:264\u001b[39m, in \u001b[36mClassProvider.get\u001b[39m\u001b[34m(self, injector)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, injector: \u001b[33m'\u001b[39m\u001b[33mInjector\u001b[39m\u001b[33m'\u001b[39m) -> T:\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minjector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_object\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cls\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:1004\u001b[39m, in \u001b[36mInjector.create_object\u001b[39m\u001b[34m(self, cls, additional_kwargs)\u001b[39m\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1002\u001b[39m     \u001b[38;5;66;03m# Mypy says \"Cannot access \"__init__\" directly\"\u001b[39;00m\n\u001b[32m   1003\u001b[39m     init_function = instance.\u001b[34m__init__\u001b[39m.\u001b[34m__func__\u001b[39m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1004\u001b[39m     \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCallError\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stack\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m instance\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:190\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(original, exception, maximum_frames)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(frames) > maximum_frames:\n\u001b[32m    189\u001b[39m     exception = original\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception.with_traceback(tb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:1000\u001b[39m, in \u001b[36mInjector.create_object\u001b[39m\u001b[34m(self, cls, additional_kwargs)\u001b[39m\n\u001b[32m    998\u001b[39m init = \u001b[38;5;28mcls\u001b[39m.\u001b[34m__init__\u001b[39m\n\u001b[32m    999\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1000\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_with_injection\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_\u001b[49m\u001b[43m=\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1002\u001b[39m     \u001b[38;5;66;03m# Mypy says \"Cannot access \"__init__\" directly\"\u001b[39;00m\n\u001b[32m   1003\u001b[39m     init_function = instance.\u001b[34m__init__\u001b[39m.\u001b[34m__func__\u001b[39m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:1041\u001b[39m, in \u001b[36mInjector.call_with_injection\u001b[39m\u001b[34m(self, callable, self_, args, kwargs)\u001b[39m\n\u001b[32m   1035\u001b[39m bound_arguments = signature.bind_partial(*full_args)\n\u001b[32m   1037\u001b[39m needed = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m   1038\u001b[39m     (k, v) \u001b[38;5;28;01mfor\u001b[39;00m (k, v) \u001b[38;5;129;01min\u001b[39;00m bindings.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m bound_arguments.arguments\n\u001b[32m   1039\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1041\u001b[39m dependencies = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs_to_inject\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneeded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m    \u001b[49m\u001b[43mowner_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mself_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__module__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1047\u001b[39m dependencies.update(kwargs)\n\u001b[32m   1049\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:91\u001b[39m, in \u001b[36msynchronized.<locals>.outside_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(function)\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any) -> Any:\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m lock:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:1089\u001b[39m, in \u001b[36mInjector.args_to_inject\u001b[39m\u001b[34m(self, function, bindings, owner_key)\u001b[39m\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m arg, interface \u001b[38;5;129;01min\u001b[39;00m bindings.items():\n\u001b[32m   1088\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1089\u001b[39m         instance: Any = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m UnsatisfiedRequirement \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1091\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m e.owner:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:91\u001b[39m, in \u001b[36msynchronized.<locals>.outside_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(function)\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any) -> Any:\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m lock:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:976\u001b[39m, in \u001b[36mInjector.get\u001b[39m\u001b[34m(self, interface, scope)\u001b[39m\n\u001b[32m    971\u001b[39m scope_instance = scope_binding.provider.get(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    973\u001b[39m log.debug(\n\u001b[32m    974\u001b[39m     \u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33mInjector.get(\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m, scope=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m) using \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_prefix, interface, scope, binding.provider\n\u001b[32m    975\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m976\u001b[39m provider_instance = \u001b[43mscope_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinding\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    977\u001b[39m result = provider_instance.get(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    978\u001b[39m log.debug(\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m -> \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_prefix, result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:91\u001b[39m, in \u001b[36msynchronized.<locals>.outside_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(function)\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any) -> Any:\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m lock:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:802\u001b[39m, in \u001b[36mSingletonScope.get\u001b[39m\u001b[34m(self, key, provider)\u001b[39m\n\u001b[32m    800\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._context[key]\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m802\u001b[39m     instance = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minjector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    803\u001b[39m     provider = InstanceProvider(instance)\n\u001b[32m    804\u001b[39m     \u001b[38;5;28mself\u001b[39m._context[key] = provider\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:813\u001b[39m, in \u001b[36mSingletonScope._get_instance\u001b[39m\u001b[34m(self, key, provider, injector)\u001b[39m\n\u001b[32m    811\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (CallError, UnsatisfiedRequirement):\n\u001b[32m    812\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m813\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprovider\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43minjector\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:302\u001b[39m, in \u001b[36mCallableProvider.get\u001b[39m\u001b[34m(self, injector)\u001b[39m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, injector: \u001b[33m'\u001b[39m\u001b[33mInjector\u001b[39m\u001b[33m'\u001b[39m) -> T:\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minjector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_with_injection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_callable\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:1052\u001b[39m, in \u001b[36mInjector.call_with_injection\u001b[39m\u001b[34m(self, callable, self_, args, kwargs)\u001b[39m\n\u001b[32m   1050\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(*full_args, **dependencies)\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1052\u001b[39m     \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCallError\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdependencies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stack\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m     \u001b[38;5;66;03m# Needed because of a mypy-related issue (https://github.com/python/mypy/issues/8129).\u001b[39;00m\n\u001b[32m   1054\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33munreachable\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:190\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(original, exception, maximum_frames)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(frames) > maximum_frames:\n\u001b[32m    189\u001b[39m     exception = original\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception.with_traceback(tb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\injector\\__init__.py:1050\u001b[39m, in \u001b[36mInjector.call_with_injection\u001b[39m\u001b[34m(self, callable, self_, args, kwargs)\u001b[39m\n\u001b[32m   1047\u001b[39m dependencies.update(kwargs)\n\u001b[32m   1049\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1050\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdependencies\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1052\u001b[39m     reraise(e, CallError(self_, \u001b[38;5;28mcallable\u001b[39m, args, dependencies, e, \u001b[38;5;28mself\u001b[39m._stack))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\codespace\\fhfeishi\\raga\\data_db\\private_gpt_method\\zzc\\di.py:59\u001b[39m, in \u001b[36mPgmModule.provide_embedding_component\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;129m@singleton\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;129m@provider\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprovide_embedding_component\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> EmbeddingComponent:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEmbeddingComponent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\codespace\\fhfeishi\\raga\\data_db\\private_gpt_method\\zzc\\di.py:24\u001b[39m, in \u001b[36mEmbeddingComponent.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     22\u001b[39m     model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mcache_folder\u001b[39m\u001b[33m\"\u001b[39m] = cfg.cache_dir\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# llama-index 的 HF 封装内部会调用 transformers / sentence-transformers\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28mself\u001b[39m.embedding_model = \u001b[43mHuggingFaceEmbedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ↓ 传给 sentence-transformers/transformers 的参数\u001b[39;49;00m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal_files_only\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcache_dir\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# encode_kwargs 可根据模型需要扩展\u001b[39;49;00m\n\u001b[32m     34\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\llama_index\\embeddings\\huggingface\\base.py:160\u001b[39m, in \u001b[36mHuggingFaceEmbedding.__init__\u001b[39m\u001b[34m(self, model_name, tokenizer_name, pooling, max_length, query_instruction, text_instruction, normalize, model, tokenizer, embed_batch_size, cache_folder, trust_remote_code, device, callback_manager, parallel_process, target_devices, show_progress_bar, **model_kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe `model_name` argument must be provided.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m model = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_instruction\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_query_instruct_for_model_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_instruction\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_text_instruct_for_model_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_length:\n\u001b[32m    174\u001b[39m     model.max_seq_length = max_length\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:327\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    309\u001b[39m has_modules = is_sentence_transformer_model(\n\u001b[32m    310\u001b[39m     model_name_or_path,\n\u001b[32m    311\u001b[39m     token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    314\u001b[39m     local_files_only=local_files_only,\n\u001b[32m    315\u001b[39m )\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    317\u001b[39m     has_modules\n\u001b[32m    318\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_model_type(\n\u001b[32m   (...)\u001b[39m\u001b[32m    325\u001b[39m     == \u001b[38;5;28mself\u001b[39m._model_config[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    326\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    339\u001b[39m     modules = \u001b[38;5;28mself\u001b[39m._load_auto_model(\n\u001b[32m    340\u001b[39m         model_name_or_path,\n\u001b[32m    341\u001b[39m         token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    349\u001b[39m         has_modules=has_modules,\n\u001b[32m    350\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:2254\u001b[39m, in \u001b[36mSentenceTransformer._load_sbert_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[39m\n\u001b[32m   2249\u001b[39m         module = module_class.load(local_path)\n\u001b[32m   2251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2252\u001b[39m     \u001b[38;5;66;03m# Newer modules that support the new loading method are loaded with the new style\u001b[39;00m\n\u001b[32m   2253\u001b[39m     \u001b[38;5;66;03m# i.e. with many keyword arguments that can optionally be used by the modules\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2254\u001b[39m     module = \u001b[43mmodule_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2256\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Loading-specific keyword arguments\u001b[39;49;00m\n\u001b[32m   2257\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodule_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2262\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Module-specific keyword arguments\u001b[39;49;00m\n\u001b[32m   2263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2270\u001b[39m modules[module_config[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]] = module\n\u001b[32m   2271\u001b[39m module_kwargs[module_config[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]] = module_config.get(\u001b[33m\"\u001b[39m\u001b[33mkwargs\u001b[39m\u001b[33m\"\u001b[39m, [])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:541\u001b[39m, in \u001b[36mTransformer.load\u001b[39m\u001b[34m(cls, model_name_or_path, subfolder, token, cache_folder, revision, local_files_only, trust_remote_code, model_kwargs, tokenizer_kwargs, config_kwargs, backend, **kwargs)\u001b[39m\n\u001b[32m    510\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m    512\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    526\u001b[39m     **kwargs,\n\u001b[32m    527\u001b[39m ) -> Self:\n\u001b[32m    528\u001b[39m     init_kwargs = \u001b[38;5;28mcls\u001b[39m._load_init_kwargs(\n\u001b[32m    529\u001b[39m         model_name_or_path=model_name_or_path,\n\u001b[32m    530\u001b[39m         subfolder=subfolder,\n\u001b[32m   (...)\u001b[39m\u001b[32m    539\u001b[39m         backend=backend,\n\u001b[32m    540\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:88\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[39m\n\u001b[32m     85\u001b[39m     config_args = {}\n\u001b[32m     87\u001b[39m config, is_peft_model = \u001b[38;5;28mself\u001b[39m._load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_peft_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[32m     91\u001b[39m     tokenizer_args[\u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m] = max_seq_length\n",
      "\u001b[31mTypeError\u001b[39m: Transformer._load_model() got multiple values for argument 'cache_dir'"
     ]
    }
   ],
   "source": [
    "# test   zzc\n",
    "import os \n",
    "\n",
    "# environment variables\n",
    "os.environ['PGM_EMBED_MODEL'] = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "os.environ['PGM_DEVICE'] = \"cuda:0\"\n",
    "os.environ['PGM_LOCAL_FILES_ONLY'] = \"1\"    \n",
    "os.environ['PGM_CHROMA_DIR'] = \"/data/pgm/chroma\"\n",
    "os.environ['PGM_CHROMA_COLLECTION'] = \"pgm_collection\"\n",
    "os.environ['PGM_PERSIST_DIR'] = \"/data/pgm/storage\"\n",
    "os.environ['PGM_SENT_WINDOW'] = \"3\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from zzc import build_injector, IngestService\n",
    "\n",
    "inj = build_injector()\n",
    "svc: IngestService = inj.get(IngestService)\n",
    "\n",
    "mdp = r\"D:\\codespace\\fhfeishi\\raga\\scripts\\PatentParser\\full.md\"\n",
    "docs = svc.ingest_file(mdp, Path(mdp))\n",
    "print([d.doc_id for d in docs])\n",
    "\n",
    "print(svc.list_ingested()[:3])\n",
    "\n",
    "# 删除\n",
    "# svc.delete(\"demo.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5454c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f6b4727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d64d86b042412c9ddee0f9bf87a6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-Reranker-0.6B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "\n",
    "model_cache_root = r\"E:\\local_models\\huggingface\\cache\\hub\"\n",
    "# Qwen/Qwen3-1.7B  Qwen/Qwen3-Embedding-0.6B  Qwen/Qwen3-Reranker-0.6B\n",
    "\n",
    "# prompt\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "# chat \n",
    "from llama_index.llms.huggingface import HuggingFaceLLM \n",
    "chat_model = HuggingFaceLLM(\n",
    "    # cache_root可以省，model_name\n",
    "    # or  local_dir\n",
    "    model_name     = r\"Qwen/Qwen3-1.7B\",\n",
    "    tokenizer_name = r\"Qwen/Qwen3-1.7B\",\n",
    "    context_window = 3900,  \n",
    "    max_new_tokens = 640,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"top_k\": 30, \"top_p\": 0.95},\n",
    "    device_map     ='cpu' \n",
    ")\n",
    "\n",
    "# reranker\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model = \"Qwen/Qwen3-Reranker-0.6B\",\n",
    "    top_n = 4,\n",
    "    trust_remote_code=True,     \n",
    "    device= 'cpu',\n",
    ") \n",
    "\n",
    "# embedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embedding = HuggingFaceEmbedding(\n",
    "    # cache\n",
    "    model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    cache_folder=model_cache_root, \n",
    "    # local-dir\n",
    "    # model_name = r\"E:\\local_models\\huggingface\\local\\path_to_qwen3Embedding0.6b_load_dir\n",
    "    max_length=1024,\n",
    "    trust_remote_code=True,\n",
    "    model_kwargs={\"local_files_only\": True},   # 允许联网 False , 禁止联网 True\n",
    "    device='cpu',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57d9786f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "d:\\environment\\miniconda\\envs\\rag\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:  [0.9994982481002808, 0.9993619322776794]\n"
     ]
    }
   ],
   "source": [
    "# Requires transformers>=4.51.0   qwen-official\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def format_instruction(instruction, query, doc):\n",
    "    if instruction is None:\n",
    "        instruction = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "    output = \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(instruction=instruction,query=query, doc=doc)\n",
    "    return output\n",
    "\n",
    "def process_inputs(pairs):\n",
    "    inputs = tokenizer(\n",
    "        pairs, padding=False, truncation='longest_first',\n",
    "        return_attention_mask=False, max_length=max_length - len(prefix_tokens) - len(suffix_tokens)\n",
    "    )\n",
    "    for i, ele in enumerate(inputs['input_ids']):\n",
    "        inputs['input_ids'][i] = prefix_tokens + ele + suffix_tokens\n",
    "    inputs = tokenizer.pad(inputs, padding=True, return_tensors=\"pt\", max_length=max_length)\n",
    "    for key in inputs:\n",
    "        inputs[key] = inputs[key].to(model.device)\n",
    "    return inputs\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_logits(inputs, **kwargs):\n",
    "    batch_scores = model(**inputs).logits[:, -1, :]\n",
    "    true_vector = batch_scores[:, token_true_id]\n",
    "    false_vector = batch_scores[:, token_false_id]\n",
    "    batch_scores = torch.stack([false_vector, true_vector], dim=1)\n",
    "    batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n",
    "    scores = batch_scores[:, 1].exp().tolist()\n",
    "    return scores\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\").eval()\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").cuda().eval()\n",
    "token_false_id = tokenizer.convert_tokens_to_ids(\"no\")\n",
    "token_true_id = tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "max_length = 8192\n",
    "\n",
    "prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    "suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "prefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "suffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)\n",
    "        \n",
    "task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "\n",
    "queries = [\"What is the capital of China?\",\n",
    "    \"Explain gravity\",\n",
    "]\n",
    "\n",
    "documents = [\n",
    "    \"The capital of China is Beijing.\",\n",
    "    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n",
    "]\n",
    "\n",
    "pairs = [format_instruction(task, query, doc) for query, doc in zip(queries, documents)]\n",
    "\n",
    "# Tokenize the input texts\n",
    "inputs = process_inputs(pairs)\n",
    "scores = compute_logits(inputs)\n",
    "\n",
    "print(\"scores: \", scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0ebebd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "d:\\environment\\miniconda\\envs\\rag\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Qwen3 官方打分（高->低）===\n",
      "[1] score=1.0000 | LIPM（Linear Inverted Pendulum Model）将质心视为在常高平面上运动的倒立摆，常用于人形/双足步态规划与控制。\n",
      "[2] score=1.0000 | 在仿人机器人中，LIPM 常用于近似 ZMP 约束，从而生成可行的足底支持多边形内的质心轨迹。\n",
      "[3] score=0.0066 | SGLang 专注推理加速与 KV cache 管理，不涉及步态物理建模。\n",
      "[4] score=0.0021 | Qwen3 是一个大语言模型系列，与机器人动力学无关。\n"
     ]
    }
   ],
   "source": [
    "# qwen3_rerank_official.py\n",
    "# pip install \"transformers>=4.51.0\" torch\n",
    "\n",
    "\"\"\"  \n",
    "Qwen3-Reranker-0.6B 官方是按 CausalLM + yes/no 概率来打分的，\n",
    "不是 AutoModelForSequenceClassification 那一套。因此我们前面用分类头取 logits 会得到乱序分数。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def format_instruction(instruction, query, doc):\n",
    "    if instruction is None:\n",
    "        instruction = \"Given a web search query, retrieve relevant passages that answer the query\"\n",
    "    return f\"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\"\n",
    "\n",
    "def process_inputs(tokenizer, pairs, max_length=8192, prefix=None, suffix=None):\n",
    "    # 官方模板（来自模型卡）\n",
    "    if prefix is None:\n",
    "        prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    "    if suffix is None:\n",
    "        suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "\n",
    "    prefix_ids = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "    suffix_ids = tokenizer.encode(suffix, add_special_tokens=False)\n",
    "\n",
    "    enc = tokenizer(\n",
    "        pairs,\n",
    "        padding=False,\n",
    "        truncation=\"longest_first\",\n",
    "        return_attention_mask=False,\n",
    "        max_length=max_length - len(prefix_ids) - len(suffix_ids),\n",
    "    )\n",
    "    # 手动拼接 prefix / suffix，再统一 pad\n",
    "    for i, ids in enumerate(enc[\"input_ids\"]):\n",
    "        enc[\"input_ids\"][i] = prefix_ids + ids + suffix_ids\n",
    "\n",
    "    enc = tokenizer.pad(enc, padding=True, return_tensors=\"pt\", max_length=max_length)\n",
    "    return enc\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_scores(model, tokenizer, inputs):\n",
    "    # 只看最后一个位置对 \"yes\"/\"no\" 的对数几率\n",
    "    out = model(**{k: v.to(model.device) for k, v in inputs.items()})\n",
    "    last_token_logits = out.logits[:, -1, :]  # [N, vocab]\n",
    "    tok_yes = tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "    tok_no  = tokenizer.convert_tokens_to_ids(\"no\")\n",
    "    yes_logit = last_token_logits[:, tok_yes]\n",
    "    no_logit  = last_token_logits[:, tok_no]\n",
    "    # softmax 取 “yes” 概率\n",
    "    prob_yes = torch.softmax(torch.stack([no_logit, yes_logit], dim=1), dim=1)[:, 1]\n",
    "    return prob_yes.cpu().tolist()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"什么是 LIPM（线性倒立摆模型），它在仿人行走里有什么作用？\"\n",
    "    docs = [\n",
    "        \"Qwen3 是一个大语言模型系列，与机器人动力学无关。\",\n",
    "        \"LIPM（Linear Inverted Pendulum Model）将质心视为在常高平面上运动的倒立摆，常用于人形/双足步态规划与控制。\",\n",
    "        \"SGLang 专注推理加速与 KV cache 管理，不涉及步态物理建模。\",\n",
    "        \"在仿人机器人中，LIPM 常用于近似 ZMP 约束，从而生成可行的足底支持多边形内的质心轨迹。\"\n",
    "    ]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", padding_side=\"left\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\").eval()  # 有GPU可加 .cuda()\n",
    "\n",
    "    pairs = [format_instruction(None, query, d) for d in docs]\n",
    "    inputs = process_inputs(tokenizer, pairs)\n",
    "    scores = compute_scores(model, tokenizer, inputs)  # 分数越大越相关\n",
    "\n",
    "    ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "    print(\"=== Qwen3 官方打分（高->低）===\")\n",
    "    for i, (t, s) in enumerate(ranked, 1):\n",
    "        print(f\"[{i}] score={s:.4f} | {t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "045b1059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-Reranker-0.6B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TextNode' object has no attribute 'node'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     19\u001b[39m reranker = SentenceTransformerRerank(\n\u001b[32m     20\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen3-Reranker-0.6B\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m     top_n=\u001b[32m4\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# 3) 使用同步方法（注意：是 postprocess_nodes，不是 apostprocess_nodes）\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m reranked = \u001b[43mreranker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpostprocess_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mQueryBundle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# 4) 打印结果\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== LlamaIndex Rerank 结果（高->低）===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\environment\\miniconda\\envs\\rag\\Lib\\site-packages\\llama_index\\core\\postprocessor\\types.py:48\u001b[39m, in \u001b[36mBaseNodePostprocessor.postprocess_nodes\u001b[39m\u001b[34m(self, nodes, query_bundle, query_str)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_postprocess_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\environment\\miniconda\\envs\\rag\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:317\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    320\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\environment\\miniconda\\envs\\rag\\Lib\\site-packages\\llama_index\\core\\postprocessor\\sbert_rerank.py:75\u001b[39m, in \u001b[36mSentenceTransformerRerank._postprocess_nodes\u001b[39m\u001b[34m(self, nodes, query_bundle)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(nodes) == \u001b[32m0\u001b[39m:\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m     72\u001b[39m query_and_nodes = [\n\u001b[32m     73\u001b[39m     (\n\u001b[32m     74\u001b[39m         query_bundle.query_str,\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m         \u001b[43mnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnode\u001b[49m.get_content(metadata_mode=MetadataMode.EMBED),\n\u001b[32m     76\u001b[39m     )\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[32m     78\u001b[39m ]\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.event(\n\u001b[32m     81\u001b[39m     CBEventType.RERANKING,\n\u001b[32m     82\u001b[39m     payload={\n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m     },\n\u001b[32m     88\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[32m     89\u001b[39m     scores = \u001b[38;5;28mself\u001b[39m._model.predict(query_and_nodes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\environment\\miniconda\\envs\\rag\\Lib\\site-packages\\pydantic\\main.py:991\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'TextNode' object has no attribute 'node'"
     ]
    }
   ],
   "source": [
    "# demo_rerank_llamaindex.py\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core import QueryBundle\n",
    "\n",
    "# 1) 准备查询与候选段落\n",
    "query = \"什么是 LIPM（线性倒立摆模型），它在仿人行走里有什么作用？\"\n",
    "candidates = [\n",
    "    \"Qwen3 是一个大语言模型系列，与机器人动力学无关。\",\n",
    "    \"LIPM（Linear Inverted Pendulum Model）将质心视为在常高平面上运动的倒立摆，常用于人形/双足步态规划与控制。\",\n",
    "    \"SGLang 专注推理加速与 KV cache 管理，不涉及步态物理建模。\",\n",
    "    \"在仿人机器人中，LIPM 常用于近似 ZMP 约束，从而生成可行的足底支持多边形内的质心轨迹。\"\n",
    "]\n",
    "\n",
    "# 创建节点\n",
    "nodes = [TextNode(text=txt) for txt in candidates]\n",
    "\n",
    "# 2) 初始化 Reranker\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model=\"Qwen/Qwen3-Reranker-0.6B\",\n",
    "    top_n=4,\n",
    "    device=\"cpu\",  # 如果有 GPU 可用，改为 \"cuda\"\n",
    "    keep_retrieval_score=False,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# 3) 使用同步方法（注意：是 postprocess_nodes，不是 apostprocess_nodes）\n",
    "reranked = reranker.postprocess_nodes(\n",
    "    nodes=nodes,\n",
    "    query_bundle=QueryBundle(query)\n",
    ")\n",
    "\n",
    "# 4) 打印结果\n",
    "print(\"\\n=== LlamaIndex Rerank 结果（高->低）===\")\n",
    "for i, node in enumerate(reranked, 1):\n",
    "    score = getattr(node, 'score', None)\n",
    "    if score is not None:\n",
    "        print(f\"[{i}] score={score:.4f} | {node.get_content()}\")\n",
    "    else:\n",
    "        print(f\"[{i}] score=None | {node.get_content()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5160713",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for SentenceTransformerRerank\nmodel\n  Input should be a valid string [type=string_type, input_value=CrossEncoder(\n  (model): ...ivation_fn): Sigmoid()\n), input_type=CrossEncoder]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m ce = CrossEncoder(MODEL_NAME, max_length=\u001b[32m256\u001b[39m, device=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# 压短序列，省内存\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# 2) LlamaIndex 的 reranker（用我们自己创建的 CrossEncoder 实例）\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m reranker = \u001b[43mSentenceTransformerRerank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 3) 准备数据（尽量裁短文本，避免 OOM）\u001b[39;00m\n\u001b[32m     21\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33m什么是 LIPM（线性倒立摆模型），它在仿人行走里有什么作用？\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\environment\\miniconda\\envs\\rag\\Lib\\site-packages\\llama_index\\core\\postprocessor\\sbert_rerank.py:45\u001b[39m, in \u001b[36mSentenceTransformerRerank.__init__\u001b[39m\u001b[34m(self, top_n, model, device, keep_retrieval_score, trust_remote_code)\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     41\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot import sentence-transformers or torch package,\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     42\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mplease `pip install torch sentence-transformers`\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     43\u001b[39m     )\n\u001b[32m     44\u001b[39m device = infer_torch_device() \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m device\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_n\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_retrieval_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_retrieval_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28mself\u001b[39m._model = CrossEncoder(\n\u001b[32m     52\u001b[39m     model,\n\u001b[32m     53\u001b[39m     max_length=DEFAULT_SENTENCE_TRANSFORMER_MAX_LENGTH,\n\u001b[32m     54\u001b[39m     device=device,\n\u001b[32m     55\u001b[39m     trust_remote_code=trust_remote_code,\n\u001b[32m     56\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\environment\\miniconda\\envs\\rag\\Lib\\site-packages\\pydantic\\main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for SentenceTransformerRerank\nmodel\n  Input should be a valid string [type=string_type, input_value=CrossEncoder(\n  (model): ...ivation_fn): Sigmoid()\n), input_type=CrossEncoder]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type"
     ]
    }
   ],
   "source": [
    "# pip install -U sentence-transformers \"llama-index-core>=0.10.64\" transformers torch\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"   # 降低线程开销（笔记本/CPU友好）\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"              # 可选\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"              # 可选\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core import QueryBundle\n",
    "\n",
    "# 1) 更轻量的 reranker（推荐先用它验证链路）\n",
    "MODEL_NAME = \"BAAI/bge-reranker-base\"  # 也可尝试 \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "ce = CrossEncoder(MODEL_NAME, max_length=256, device=\"cpu\")  # 压短序列，省内存\n",
    "\n",
    "# 2) LlamaIndex 的 reranker（用我们自己创建的 CrossEncoder 实例）\n",
    "reranker = SentenceTransformerRerank(model=ce, top_n=3)\n",
    "\n",
    "# 3) 准备数据（尽量裁短文本，避免 OOM）\n",
    "query = \"什么是 LIPM（线性倒立摆模型），它在仿人行走里有什么作用？\"\n",
    "docs = [\n",
    "    \"Qwen3 是一个大语言模型系列，与机器人动力学无关。\",\n",
    "    \"LIPM（Linear Inverted Pendulum Model）将质心视为在常高平面上运动的倒立摆，常用于人形/双足步态规划与控制。\",\n",
    "    \"SGLang 专注推理加速与 KV cache 管理，不涉及步态物理建模。\",\n",
    "    \"在仿人机器人中，LIPM 常用于近似 ZMP 约束，从而生成可行的足底支持多边形内的质心轨迹。\"\n",
    "]\n",
    "nodes = [TextNode(text=t[:800]) for t in docs]  # 👈 文本裁短以保守内存\n",
    "\n",
    "# 4) 执行重排\n",
    "out = reranker.postprocess(nodes, QueryBundle(query))\n",
    "\n",
    "print(\"=== LlamaIndex + SentenceTransformerRerank（高->低）===\")\n",
    "for i, n in enumerate(out, 1):\n",
    "    score = float(getattr(n, \"score\", 0.0))\n",
    "    print(f\"[{i}] score={score:.4f} | {n.get_content()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbea010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e2e8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86384f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55c48ede3cc415aaebc69cea7d5ea9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b2bbc78f934ea9829d6ebbc73bef20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd7f43c7b9742c2b6dfc1ce4bd5cb46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24a45f8f13d4e6887d1e89ca81531c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a421b214e0b4249acb53e8f50d2070b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b200cee8364f6785d13e3d9a891887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.38G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== seq-cls 版打分（高->低）===\n",
      "[1] score=-0.8007 | SGLang 专注推理加速与 KV cache 管理，不涉及步态物理建模。\n",
      "[2] score=-1.0133 | LIPM（Linear Inverted Pendulum Model）将质心视为在常高平面上运动的倒立摆，常用于人形/双足步态规划与控制。\n",
      "[3] score=-1.0950 | 在仿人机器人中，LIPM 常用于近似 ZMP 约束，从而生成可行的足底支持多边形内的质心轨迹。\n",
      "[4] score=-1.2828 | Qwen3 是一个大语言模型系列，与机器人动力学无关。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "query = \"什么是 LIPM（线性倒立摆模型），它在仿人行走里有什么作用？\"\n",
    "docs = [\n",
    "    \"Qwen3 是一个大语言模型系列，与机器人动力学无关。\",\n",
    "    \"LIPM（Linear Inverted Pendulum Model）将质心视为在常高平面上运动的倒立摆，常用于人形/双足步态规划与控制。\",\n",
    "    \"SGLang 专注推理加速与 KV cache 管理，不涉及步态物理建模。\",\n",
    "    \"在仿人机器人中，LIPM 常用于近似 ZMP 约束，从而生成可行的足底支持多边形内的质心轨迹。\"\n",
    "]\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"tomaarsen/Qwen3-Reranker-0.6B-seq-cls\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"tomaarsen/Qwen3-Reranker-0.6B-seq-cls\").eval()\n",
    "\n",
    "tok.pad_token = tok.eos_token if tok.pad_token is None else tok.pad_token\n",
    "model.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "batch = tok([query]*len(docs), docs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    scores = model(**batch).logits.view(-1).tolist()  # 分数越大越相关\n",
    "\n",
    "ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "print(\"=== seq-cls 版打分（高->低）===\")\n",
    "for i, (t, s) in enumerate(ranked, 1):\n",
    "    print(f\"[{i}] score={s:.4f} | {t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e11c7273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-Reranker-0.6B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Transformers 直算得分（高->低）===\n",
      "[1] score=2.7103 | Qwen3 是一个大语言模型系列，与机器人动力学无关。\n",
      "[2] score=1.0633 | SGLang 专注推理加速与 KV cache 管理，不涉及步态物理建模。\n",
      "[3] score=-1.1071 | LIPM（Linear Inverted Pendulum Model）将质心视为在常高平面上运动的倒立摆，常用于人形/双足步态规划与控制。\n",
      "[4] score=-1.9902 | 在仿人机器人中，LIPM 常用于近似 ZMP 约束，从而生成可行的足底支持多边形内的质心轨迹。\n"
     ]
    }
   ],
   "source": [
    "# demo_rerank_transformers_fixed2.py\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "def ensure_pad_token(tok, model):\n",
    "    if tok.pad_token is None:\n",
    "        if tok.eos_token is not None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "        else:\n",
    "            tok.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "            model.resize_token_embeddings(len(tok))\n",
    "    model.config.pad_token_id = tok.pad_token_id\n",
    "    tok.padding_side = \"right\"\n",
    "\n",
    "query = \"什么是 LIPM（线性倒立摆模型），它在仿人行走里有什么作用？\"\n",
    "passages = [\n",
    "    \"Qwen3 是一个大语言模型系列，与机器人动力学无关。\",\n",
    "    \"LIPM（Linear Inverted Pendulum Model）将质心视为在常高平面上运动的倒立摆，常用于人形/双足步态规划与控制。\",\n",
    "    \"SGLang 专注推理加速与 KV cache 管理，不涉及步态物理建模。\",\n",
    "    \"在仿人机器人中，LIPM 常用于近似 ZMP 约束，从而生成可行的足底支持多边形内的质心轨迹。\"\n",
    "]\n",
    "\n",
    "device = torch.device(\"cpu\")  # 有 GPU 可改 \"cuda\"\n",
    "tok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", trust_remote_code=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"Qwen/Qwen3-Reranker-0.6B\", trust_remote_code=True\n",
    ").to(device).eval()\n",
    "\n",
    "ensure_pad_token(tok, model)\n",
    "\n",
    "pairs_q = [query] * len(passages)\n",
    "pairs_p = passages\n",
    "\n",
    "inputs = tok(\n",
    "    pairs_q,\n",
    "    pairs_p,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\",\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(**inputs)\n",
    "    # logits shape: [N, 1] → flatten to [N]\n",
    "    scores = out.logits.view(-1).cpu().tolist()  # 👈 强制展平成一维\n",
    "\n",
    "ranked = sorted(zip(passages, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n=== Transformers 直算得分（高->低）===\")\n",
    "for i, (text, s) in enumerate(ranked, 1):\n",
    "    print(f\"[{i}] score={s:.4f} | {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a86eb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-Reranker-0.6B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 人工智能的发展趋势\n",
      "\n",
      "Candidates:\n",
      "[0] 人工智能正在改变世界，深度学习、大模型推动技术进步。\n",
      "[1] 苹果公司发布了新款 iPhone，性能更强，摄像头更清晰。\n",
      "[2] 机器学习和神经网络在自然语言处理中广泛应用。\n",
      "[3] 天气预报说明天有雨，记得带伞。\n",
      "[4] 大模型如 Qwen、LLaMA 正在推动 AI 代理的发展。\n",
      "\n",
      "==================================================\n",
      "🔍 Reranking Results (Higher score = more relevant)\n",
      "==================================================\n",
      "Rank 1: [Score: +0.580]\n",
      "  人工智能正在改变世界，深度学习、大模型推动技术进步。\n",
      "Rank 2: [Score: -0.075]\n",
      "  大模型如 Qwen、LLaMA 正在推动 AI 代理的发展。\n",
      "Rank 3: [Score: -0.484]\n",
      "  机器学习和神经网络在自然语言处理中广泛应用。\n",
      "Rank 4: [Score: -0.544]\n",
      "  苹果公司发布了新款 iPhone，性能更强，摄像头更清晰。\n",
      "Rank 5: [Score: -0.694]\n",
      "  天气预报说明天有雨，记得带伞。\n"
     ]
    }
   ],
   "source": [
    "#  还可以吧 qwen\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from typing import List, Tuple\n",
    "\n",
    "# 设置设备\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 模型名称\n",
    "model_name = \"Qwen/Qwen3-Reranker-0.6B\"\n",
    "\n",
    "# 加载 tokenizer 和 模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
    ").to(device)\n",
    "\n",
    "# 示例数据\n",
    "query = \"人工智能的发展趋势\"\n",
    "candidates = [\n",
    "    \"人工智能正在改变世界，深度学习、大模型推动技术进步。\",\n",
    "    \"苹果公司发布了新款 iPhone，性能更强，摄像头更清晰。\",\n",
    "    \"机器学习和神经网络在自然语言处理中广泛应用。\",\n",
    "    \"天气预报说明天有雨，记得带伞。\",\n",
    "    \"大模型如 Qwen、LLaMA 正在推动 AI 代理的发展。\",\n",
    "]\n",
    "\n",
    "print(\"\\nQuery:\", query)\n",
    "print(\"\\nCandidates:\")\n",
    "for i, cand in enumerate(candidates):\n",
    "    print(f\"[{i}] {cand}\")\n",
    "\n",
    "# 构造输入并打分\n",
    "scores = []\n",
    "inputs_for_model = []\n",
    "\n",
    "with torch.no_grad():  # 推理阶段，关闭梯度\n",
    "    for i, doc in enumerate(candidates):\n",
    "        # 拼接 query 和 document\n",
    "        text = f\"Query: {query}\\nDoc: {doc}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = tokenizer(\n",
    "            text,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=8192\n",
    "        ).to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(**encoded)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # 👇 修复：根据 num_labels 提取正确分数\n",
    "        if logits.shape[-1] == 1:\n",
    "            score = logits.item()\n",
    "        else:\n",
    "            score = logits[0, 1].item()  # 取正类（相关）分数\n",
    "\n",
    "        scores.append(score)\n",
    "        inputs_for_model.append(encoded)\n",
    "\n",
    "# 排序：按分数从高到低\n",
    "ranked = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 输出排序结果\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🔍 Reranking Results (Higher score = more relevant)\")\n",
    "print(\"=\"*50)\n",
    "for rank, (idx, score) in enumerate(ranked, 1):\n",
    "    print(f\"Rank {rank}: [Score: {score:+.3f}]\")\n",
    "    print(f\"  {candidates[idx][:100]}{'...' if len(candidates[idx]) > 100 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1701995d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77cc0ac57fb4546ac1d410b48047fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320dfca8c4134bf2aa6438c7777ce61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5526d171e35b4a89a9279219442f3789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41fc8c7395f9467f9a8e53ea7cf1cc5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08999888122b480d8dc7c15d5ca28721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3dd766a5b04564ac673422442e135c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2e3316314a44f6bf8f293e624a9d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始打分 ---\n",
      "0.9943  北京是中国的首都，也是政治中心。\n",
      "0.2503  上海是中国最大的经济中心。\n",
      "0.0185  广州位于南方，气候温暖。\n",
      "0.6185  北京有故宫、天安门等著名景点。\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TextNode' object has no attribute 'node'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     27\u001b[39m nodes = [TextNode(text=c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m candidates]\n\u001b[32m     28\u001b[39m reranker = SentenceTransformerRerank(\n\u001b[32m     29\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mBAAI/bge-reranker-base\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     30\u001b[39m     top_n=\u001b[32m3\u001b[39m,           \u001b[38;5;66;03m# 只保留前 3\u001b[39;00m\n\u001b[32m     31\u001b[39m     device=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     32\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m ranked_nodes = \u001b[43mreranker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpostprocess_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- llama-index 重排后（top_n=3）---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ranked_nodes, \u001b[32m1\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\environment\\miniconda\\envs\\rag\\Lib\\site-packages\\llama_index\\core\\postprocessor\\types.py:48\u001b[39m, in \u001b[36mBaseNodePostprocessor.postprocess_nodes\u001b[39m\u001b[34m(self, nodes, query_bundle, query_str)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_postprocess_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\environment\\miniconda\\envs\\rag\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:317\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    320\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\environment\\miniconda\\envs\\rag\\Lib\\site-packages\\llama_index\\core\\postprocessor\\sbert_rerank.py:75\u001b[39m, in \u001b[36mSentenceTransformerRerank._postprocess_nodes\u001b[39m\u001b[34m(self, nodes, query_bundle)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(nodes) == \u001b[32m0\u001b[39m:\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m     72\u001b[39m query_and_nodes = [\n\u001b[32m     73\u001b[39m     (\n\u001b[32m     74\u001b[39m         query_bundle.query_str,\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m         \u001b[43mnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnode\u001b[49m.get_content(metadata_mode=MetadataMode.EMBED),\n\u001b[32m     76\u001b[39m     )\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[32m     78\u001b[39m ]\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.event(\n\u001b[32m     81\u001b[39m     CBEventType.RERANKING,\n\u001b[32m     82\u001b[39m     payload={\n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m     },\n\u001b[32m     88\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[32m     89\u001b[39m     scores = \u001b[38;5;28mself\u001b[39m._model.predict(query_and_nodes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\environment\\miniconda\\envs\\rag\\Lib\\site-packages\\pydantic\\main.py:991\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'TextNode' object has no attribute 'node'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "验证 bge-reranker-base 能否正常打分并重排\n",
    "\"\"\"\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "# 0. 造几条假设的候选文本\n",
    "query = \"中国的首都在哪？\"\n",
    "candidates = [\n",
    "    \"北京是中国的首都，也是政治中心。\",\n",
    "    \"上海是中国最大的经济中心。\",\n",
    "    \"广州位于南方，气候温暖。\",\n",
    "    \"北京有故宫、天安门等著名景点。\",\n",
    "]\n",
    "\n",
    "# maidalun/bce-reranker-base_v1  中文优化\n",
    "# BAAI/bge-reranker-base         中英双语\n",
    "# BAAI/bge-reranker-large        large\n",
    "\n",
    "\n",
    "# 1. 直接用 sentence-transformers 打原始分\n",
    "model = CrossEncoder(\"BAAI/bge-reranker-base\", device=\"cpu\")  \n",
    "pairs = [(query, c) for c in candidates]\n",
    "raw_scores = model.predict(pairs)\n",
    "print(\"--- 原始打分 ---\")\n",
    "for c, s in zip(candidates, raw_scores):\n",
    "    print(f\"{s:.4f}  {c}\")\n",
    "\n",
    "# 2. 用 llama-index 的 reranker 包装，再跑一次\n",
    "nodes = [TextNode(text=c) for c in candidates]\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model=\"BAAI/bge-reranker-base\",\n",
    "    top_n=3,           # 只保留前 3\n",
    "    device=\"cpu\"\n",
    ")\n",
    "ranked_nodes = reranker.postprocess_nodes(\n",
    "    nodes=nodes,\n",
    "    query_str=query\n",
    ")\n",
    "\n",
    "print(\"\\n--- llama-index 重排后（top_n=3）---\")\n",
    "for i, n in enumerate(ranked_nodes, 1):\n",
    "    print(f\"{i}. {n.score:.4f}  {n.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb73ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f4f686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
