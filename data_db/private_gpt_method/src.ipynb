{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b4fa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zzb  éœ€è¦  settings.yaml\n",
    "from pathlib import Path\n",
    "from zzb import build_injector, IngestService\n",
    "\n",
    "inj = build_injector()\n",
    "svc = inj.get(IngestService)\n",
    "\n",
    "\n",
    "mdp = r\"D:\\codespace\\fhfeishi\\raga\\scripts\\PatentParser\\full.md\"\n",
    "docs = svc.ingest_file(mdp, Path(\"mdp\"))\n",
    "print(\"ingested:\", [d.doc_id for d in docs])\n",
    "\n",
    "print(\"list:\", svc.list_ingested()[:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# api  \n",
    "\n",
    "local_dir = r\"E:\\local_models\\huggingface\\cache\\hub\"\n",
    "\n",
    "embeds = HuggingFaceEmbedding(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    device=\"cpu\",                 # å»ºè®®æ”¾é¡¶å±‚\n",
    "    trust_remote_code=True,       # å»ºè®®æ”¾é¡¶å±‚\n",
    "    cache_folder=local_dir,\n",
    "    model_kwargs={\"local_files_only\": False},   # å…è®¸è”ç½‘ False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a98f258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "\n",
    "import sys, transformers, huggingface_hub, sentence_transformers\n",
    "print(\"py:\", sys.executable)\n",
    "print(\"tf:\", transformers.__version__, \"hub:\", huggingface_hub.__version__, \"st:\", sentence_transformers.__version__)\n",
    "print(\"HF_HOME:\", os.getenv(\"HF_HOME\"))\n",
    "print(\"HF_ENDPOINT:\", os.getenv(\"HF_ENDPOINT\"))\n",
    "\n",
    "\n",
    "# â€”â€” ç¦»çº¿&ç¼“å­˜ç¯å¢ƒï¼ˆWindows ä¸‹ä¹ŸOKï¼‰â€”â€”\n",
    "os.environ[\"HF_HOME\"] = r\"E:\\local_models\\huggingface\\cache\"\n",
    "# os.environ[\"HF_HUB_OFFLINE\"] = \"0\"\n",
    "# os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "# os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"  # å…³é—­ symlink è­¦å‘Š\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://mirrors.tuna.tsinghua.edu.cn/hugging-face/\"\n",
    "\n",
    "\n",
    "name_or_path = \"Qwen/Qwen3-Embedding-0.6B\"  # ä¹Ÿå¯æ¢æˆæœ¬åœ°ç›®å½•ï¼ˆè§æ–¹æ¡ˆ2ï¼‰\n",
    "device = \"cpu\"  # æˆ– \"cuda:0\"ï¼ˆå¦‚æœæœ‰GPUï¼‰\n",
    "\n",
    "# ç¬¬ä¸€æ¬¡è‹¥ç¼“å­˜ä¸å…¨ï¼ŒæŠŠ local_files_only æ”¹ä¸º False è”ç½‘è¡¥é½ä¸€æ¬¡\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    name_or_path, trust_remote_code=True, local_files_only=False,\n",
    "    cache_dir=os.environ[\"HF_HOME\"],\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    name_or_path, trust_remote_code=True, local_files_only=True,\n",
    "    cache_dir=os.environ[\"HF_HOME\"],\n",
    ").to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6416cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test zzc\n",
    "import os\n",
    "from pathlib import Path\n",
    "from zzc import build_injector, IngestService\n",
    "from zzc.di import EmbeddingComponent  # ä»…ç”¨äºå†’çƒŸ\n",
    "\n",
    "# --- ç¯å¢ƒå˜é‡ ---\n",
    "os.environ['PGM_EMBED_MODEL'] = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "os.environ['PGM_DEVICE'] = \"cpu\"             # æ— GPUå°±æ”¹ \"cpu\"\n",
    "os.environ['PGM_LOCAL_FILES_ONLY'] = \"0\"        # é¦–æ¬¡è·‘è‹¥æ²¡ç¼“å­˜ï¼Œæ”¹æˆ \"0\"\n",
    "os.environ['PGM_CHROMA_DIR'] = \"/data/pgm/chroma\"\n",
    "os.environ['PGM_CHROMA_COLLECTION'] = \"pgm_collection\"\n",
    "os.environ['PGM_PERSIST_DIR'] = \"/data/pgm/storage\"\n",
    "os.environ['PGM_SENT_WINDOW'] = \"3\"\n",
    "\n",
    "inj = build_injector()\n",
    "\n",
    "# å…ˆå†’çƒŸï¼šä»…åŠ è½½åµŒå…¥æ¨¡å‹ï¼ŒéªŒè¯â€œç¼“å­˜+ç¦»çº¿â€æ˜¯å¦OK\n",
    "_ = inj.get(EmbeddingComponent)\n",
    "\n",
    "svc: IngestService = inj.get(IngestService)\n",
    "\n",
    "# è·¯å¾„ï¼šç¡®ä¿ä¸å½“å‰è¿è¡Œ OS ä¸€è‡´\n",
    "mdp = r\"D:\\codespace\\fhfeishi\\raga\\scripts\\PatentParser\\full.md\"\n",
    "p = Path(mdp)\n",
    "assert p.exists(), f\"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{p}ï¼ˆæ³¨æ„å½“å‰è¿è¡Œç¯å¢ƒçš„æ“ä½œç³»ç»Ÿè·¯å¾„ï¼‰\"\n",
    "\n",
    "docs = svc.ingest_file(p.name, p)\n",
    "print([d.doc_id for d in docs])\n",
    "print(svc.list_ingested()[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a493a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test   zzc\n",
    "import os \n",
    "\n",
    "# environment variables\n",
    "os.environ['PGM_EMBED_MODEL'] = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "os.environ['PGM_DEVICE'] = \"cuda:0\"\n",
    "os.environ['PGM_LOCAL_FILES_ONLY'] = \"1\"    \n",
    "os.environ['PGM_CHROMA_DIR'] = \"/data/pgm/chroma\"\n",
    "os.environ['PGM_CHROMA_COLLECTION'] = \"pgm_collection\"\n",
    "os.environ['PGM_PERSIST_DIR'] = \"/data/pgm/storage\"\n",
    "os.environ['PGM_SENT_WINDOW'] = \"3\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from zzc import build_injector, IngestService\n",
    "\n",
    "inj = build_injector()\n",
    "svc: IngestService = inj.get(IngestService)\n",
    "\n",
    "mdp = r\"D:\\codespace\\fhfeishi\\raga\\scripts\\PatentParser\\full.md\"\n",
    "docs = svc.ingest_file(mdp, Path(mdp))\n",
    "print([d.doc_id for d in docs])\n",
    "\n",
    "print(svc.list_ingested()[:3])\n",
    "\n",
    "# åˆ é™¤\n",
    "# svc.delete(\"demo.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5454c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "\n",
    "\n",
    "# ollama\n",
    "\n",
    "\n",
    "model_cache_root = r\"E:\\local_models\\huggingface\\cache\\hub\"\n",
    "# Qwen/Qwen3-1.7B  Qwen/Qwen3-Embedding-0.6B  Qwen/Qwen3-Reranker-0.6B\n",
    "\n",
    "# prompt\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "# chat \n",
    "from llama_index.llms.huggingface import HuggingFaceLLM \n",
    "chat_model = HuggingFaceLLM(\n",
    "    # cache_rootå¯ä»¥çœï¼Œmodel_name\n",
    "    # or  local_dir\n",
    "    model_name     = r\"Qwen/Qwen3-1.7B\",\n",
    "    tokenizer_name = r\"Qwen/Qwen3-1.7B\",\n",
    "    context_window = 3900,  \n",
    "    max_new_tokens = 640,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"top_k\": 30, \"top_p\": 0.95},\n",
    "    device_map     ='cpu' \n",
    ")\n",
    "\n",
    "# reranker  \n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model = \"Qwen/Qwen3-Reranker-0.6B\",\n",
    "    top_n = 4,\n",
    "    trust_remote_code=True,     \n",
    "    device= 'cpu',\n",
    ") \n",
    "\n",
    "# embedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embedding = HuggingFaceEmbedding(\n",
    "    # cache\n",
    "    model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    cache_folder=model_cache_root, \n",
    "    # local-dir\n",
    "    # model_name = r\"E:\\local_models\\huggingface\\local\\path_to_qwen3Embedding0.6b_load_dir\n",
    "    max_length=1024,\n",
    "    trust_remote_code=True,\n",
    "    model_kwargs={\"local_files_only\": True},   # å…è®¸è”ç½‘ False , ç¦æ­¢è”ç½‘ True\n",
    "    device='cpu',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d9786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires transformers>=4.51.0   qwen-official\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def format_instruction(instruction, query, doc):\n",
    "    if instruction is None:\n",
    "        instruction = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "    output = \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(instruction=instruction,query=query, doc=doc)\n",
    "    return output\n",
    "\n",
    "def process_inputs(pairs):\n",
    "    inputs = tokenizer(\n",
    "        pairs, padding=False, truncation='longest_first',\n",
    "        return_attention_mask=False, max_length=max_length - len(prefix_tokens) - len(suffix_tokens)\n",
    "    )\n",
    "    for i, ele in enumerate(inputs['input_ids']):\n",
    "        inputs['input_ids'][i] = prefix_tokens + ele + suffix_tokens\n",
    "    inputs = tokenizer.pad(inputs, padding=True, return_tensors=\"pt\", max_length=max_length)\n",
    "    for key in inputs:\n",
    "        inputs[key] = inputs[key].to(model.device)\n",
    "    return inputs\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_logits(inputs, **kwargs):\n",
    "    batch_scores = model(**inputs).logits[:, -1, :]\n",
    "    true_vector = batch_scores[:, token_true_id]\n",
    "    false_vector = batch_scores[:, token_false_id]\n",
    "    batch_scores = torch.stack([false_vector, true_vector], dim=1)\n",
    "    batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n",
    "    scores = batch_scores[:, 1].exp().tolist()\n",
    "    return scores\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\").eval()\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").cuda().eval()\n",
    "token_false_id = tokenizer.convert_tokens_to_ids(\"no\")\n",
    "token_true_id = tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "max_length = 8192\n",
    "\n",
    "prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    "suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "prefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "suffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)\n",
    "        \n",
    "task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "\n",
    "queries = [\"What is the capital of China?\",\n",
    "    \"Explain gravity\",\n",
    "]\n",
    "\n",
    "documents = [\n",
    "    \"The capital of China is Beijing.\",\n",
    "    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n",
    "]\n",
    "\n",
    "pairs = [format_instruction(task, query, doc) for query, doc in zip(queries, documents)]\n",
    "\n",
    "# Tokenize the input texts\n",
    "inputs = process_inputs(pairs)\n",
    "scores = compute_logits(inputs)\n",
    "\n",
    "print(\"scores: \", scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ebebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen3_rerank_official.py\n",
    "# pip install \"transformers>=4.51.0\" torch\n",
    "\n",
    "\"\"\"  \n",
    "Qwen3-Reranker-0.6B å®˜æ–¹æ˜¯æŒ‰ CausalLM + yes/no æ¦‚ç‡æ¥æ‰“åˆ†çš„ï¼Œ\n",
    "ä¸æ˜¯ AutoModelForSequenceClassification é‚£ä¸€å¥—ã€‚å› æ­¤æˆ‘ä»¬å‰é¢ç”¨åˆ†ç±»å¤´å– logits ä¼šå¾—åˆ°ä¹±åºåˆ†æ•°ã€‚\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def format_instruction(instruction, query, doc):\n",
    "    if instruction is None:\n",
    "        instruction = \"Given a web search query, retrieve relevant passages that answer the query\"\n",
    "    return f\"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\"\n",
    "\n",
    "def process_inputs(tokenizer, pairs, max_length=8192, prefix=None, suffix=None):\n",
    "    # å®˜æ–¹æ¨¡æ¿ï¼ˆæ¥è‡ªæ¨¡å‹å¡ï¼‰\n",
    "    if prefix is None:\n",
    "        prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    "    if suffix is None:\n",
    "        suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "\n",
    "    prefix_ids = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "    suffix_ids = tokenizer.encode(suffix, add_special_tokens=False)\n",
    "\n",
    "    enc = tokenizer(\n",
    "        pairs,\n",
    "        padding=False,\n",
    "        truncation=\"longest_first\",\n",
    "        return_attention_mask=False,\n",
    "        max_length=max_length - len(prefix_ids) - len(suffix_ids),\n",
    "    )\n",
    "    # æ‰‹åŠ¨æ‹¼æ¥ prefix / suffixï¼Œå†ç»Ÿä¸€ pad\n",
    "    for i, ids in enumerate(enc[\"input_ids\"]):\n",
    "        enc[\"input_ids\"][i] = prefix_ids + ids + suffix_ids\n",
    "\n",
    "    enc = tokenizer.pad(enc, padding=True, return_tensors=\"pt\", max_length=max_length)\n",
    "    return enc\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_scores(model, tokenizer, inputs):\n",
    "    # åªçœ‹æœ€åä¸€ä¸ªä½ç½®å¯¹ \"yes\"/\"no\" çš„å¯¹æ•°å‡ ç‡\n",
    "    out = model(**{k: v.to(model.device) for k, v in inputs.items()})\n",
    "    last_token_logits = out.logits[:, -1, :]  # [N, vocab]\n",
    "    tok_yes = tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "    tok_no  = tokenizer.convert_tokens_to_ids(\"no\")\n",
    "    yes_logit = last_token_logits[:, tok_yes]\n",
    "    no_logit  = last_token_logits[:, tok_no]\n",
    "    # softmax å– â€œyesâ€ æ¦‚ç‡\n",
    "    prob_yes = torch.softmax(torch.stack([no_logit, yes_logit], dim=1), dim=1)[:, 1]\n",
    "    return prob_yes.cpu().tolist()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"ä»€ä¹ˆæ˜¯ LIPMï¼ˆçº¿æ€§å€’ç«‹æ‘†æ¨¡å‹ï¼‰ï¼Œå®ƒåœ¨ä»¿äººè¡Œèµ°é‡Œæœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ\"\n",
    "    docs = [\n",
    "        \"Qwen3 æ˜¯ä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹ç³»åˆ—ï¼Œä¸æœºå™¨äººåŠ¨åŠ›å­¦æ— å…³ã€‚\",\n",
    "        \"LIPMï¼ˆLinear Inverted Pendulum Modelï¼‰å°†è´¨å¿ƒè§†ä¸ºåœ¨å¸¸é«˜å¹³é¢ä¸Šè¿åŠ¨çš„å€’ç«‹æ‘†ï¼Œå¸¸ç”¨äºäººå½¢/åŒè¶³æ­¥æ€è§„åˆ’ä¸æ§åˆ¶ã€‚\",\n",
    "        \"SGLang ä¸“æ³¨æ¨ç†åŠ é€Ÿä¸ KV cache ç®¡ç†ï¼Œä¸æ¶‰åŠæ­¥æ€ç‰©ç†å»ºæ¨¡ã€‚\",\n",
    "        \"åœ¨ä»¿äººæœºå™¨äººä¸­ï¼ŒLIPM å¸¸ç”¨äºè¿‘ä¼¼ ZMP çº¦æŸï¼Œä»è€Œç”Ÿæˆå¯è¡Œçš„è¶³åº•æ”¯æŒå¤šè¾¹å½¢å†…çš„è´¨å¿ƒè½¨è¿¹ã€‚\"\n",
    "    ]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", padding_side=\"left\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\").eval()  # æœ‰GPUå¯åŠ  .cuda()\n",
    "\n",
    "    pairs = [format_instruction(None, query, d) for d in docs]\n",
    "    inputs = process_inputs(tokenizer, pairs)\n",
    "    scores = compute_scores(model, tokenizer, inputs)  # åˆ†æ•°è¶Šå¤§è¶Šç›¸å…³\n",
    "\n",
    "    ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "    print(\"=== Qwen3 å®˜æ–¹æ‰“åˆ†ï¼ˆé«˜->ä½ï¼‰===\")\n",
    "    for i, (t, s) in enumerate(ranked, 1):\n",
    "        print(f\"[{i}] score={s:.4f} | {t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b1059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_rerank_llamaindex.py\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core import QueryBundle\n",
    "\n",
    "# 1) å‡†å¤‡æŸ¥è¯¢ä¸å€™é€‰æ®µè½\n",
    "query = \"ä»€ä¹ˆæ˜¯ LIPMï¼ˆçº¿æ€§å€’ç«‹æ‘†æ¨¡å‹ï¼‰ï¼Œå®ƒåœ¨ä»¿äººè¡Œèµ°é‡Œæœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ\"\n",
    "candidates = [\n",
    "    \"Qwen3 æ˜¯ä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹ç³»åˆ—ï¼Œä¸æœºå™¨äººåŠ¨åŠ›å­¦æ— å…³ã€‚\",\n",
    "    \"LIPMï¼ˆLinear Inverted Pendulum Modelï¼‰å°†è´¨å¿ƒè§†ä¸ºåœ¨å¸¸é«˜å¹³é¢ä¸Šè¿åŠ¨çš„å€’ç«‹æ‘†ï¼Œå¸¸ç”¨äºäººå½¢/åŒè¶³æ­¥æ€è§„åˆ’ä¸æ§åˆ¶ã€‚\",\n",
    "    \"SGLang ä¸“æ³¨æ¨ç†åŠ é€Ÿä¸ KV cache ç®¡ç†ï¼Œä¸æ¶‰åŠæ­¥æ€ç‰©ç†å»ºæ¨¡ã€‚\",\n",
    "    \"åœ¨ä»¿äººæœºå™¨äººä¸­ï¼ŒLIPM å¸¸ç”¨äºè¿‘ä¼¼ ZMP çº¦æŸï¼Œä»è€Œç”Ÿæˆå¯è¡Œçš„è¶³åº•æ”¯æŒå¤šè¾¹å½¢å†…çš„è´¨å¿ƒè½¨è¿¹ã€‚\"\n",
    "]\n",
    "\n",
    "# åˆ›å»ºèŠ‚ç‚¹\n",
    "nodes = [TextNode(text=txt) for txt in candidates]\n",
    "\n",
    "# 2) åˆå§‹åŒ– Reranker\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model=\"Qwen/Qwen3-Reranker-0.6B\",\n",
    "    top_n=4,\n",
    "    device=\"cpu\",  # å¦‚æœæœ‰ GPU å¯ç”¨ï¼Œæ”¹ä¸º \"cuda\"\n",
    "    keep_retrieval_score=False,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# 3) ä½¿ç”¨åŒæ­¥æ–¹æ³•ï¼ˆæ³¨æ„ï¼šæ˜¯ postprocess_nodesï¼Œä¸æ˜¯ apostprocess_nodesï¼‰\n",
    "reranked = reranker.postprocess_nodes(\n",
    "    nodes=nodes,\n",
    "    query_bundle=QueryBundle(query)\n",
    ")\n",
    "\n",
    "# 4) æ‰“å°ç»“æœ\n",
    "print(\"\\n=== LlamaIndex Rerank ç»“æœï¼ˆé«˜->ä½ï¼‰===\")\n",
    "for i, node in enumerate(reranked, 1):\n",
    "    score = getattr(node, 'score', None)\n",
    "    if score is not None:\n",
    "        print(f\"[{i}] score={score:.4f} | {node.get_content()}\")\n",
    "    else:\n",
    "        print(f\"[{i}] score=None | {node.get_content()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5160713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U sentence-transformers \"llama-index-core>=0.10.64\" transformers torch\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"   # é™ä½çº¿ç¨‹å¼€é”€ï¼ˆç¬”è®°æœ¬/CPUå‹å¥½ï¼‰\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"              # å¯é€‰\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"              # å¯é€‰\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core import QueryBundle\n",
    "\n",
    "# 1) æ›´è½»é‡çš„ rerankerï¼ˆæ¨èå…ˆç”¨å®ƒéªŒè¯é“¾è·¯ï¼‰\n",
    "MODEL_NAME = \"BAAI/bge-reranker-base\"  # ä¹Ÿå¯å°è¯• \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "ce = CrossEncoder(MODEL_NAME, max_length=256, device=\"cpu\")  # å‹çŸ­åºåˆ—ï¼Œçœå†…å­˜\n",
    "\n",
    "# 2) LlamaIndex çš„ rerankerï¼ˆç”¨æˆ‘ä»¬è‡ªå·±åˆ›å»ºçš„ CrossEncoder å®ä¾‹ï¼‰\n",
    "reranker = SentenceTransformerRerank(model=ce, top_n=3)\n",
    "\n",
    "# 3) å‡†å¤‡æ•°æ®ï¼ˆå°½é‡è£çŸ­æ–‡æœ¬ï¼Œé¿å… OOMï¼‰\n",
    "query = \"ä»€ä¹ˆæ˜¯ LIPMï¼ˆçº¿æ€§å€’ç«‹æ‘†æ¨¡å‹ï¼‰ï¼Œå®ƒåœ¨ä»¿äººè¡Œèµ°é‡Œæœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ\"\n",
    "docs = [\n",
    "    \"Qwen3 æ˜¯ä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹ç³»åˆ—ï¼Œä¸æœºå™¨äººåŠ¨åŠ›å­¦æ— å…³ã€‚\",\n",
    "    \"LIPMï¼ˆLinear Inverted Pendulum Modelï¼‰å°†è´¨å¿ƒè§†ä¸ºåœ¨å¸¸é«˜å¹³é¢ä¸Šè¿åŠ¨çš„å€’ç«‹æ‘†ï¼Œå¸¸ç”¨äºäººå½¢/åŒè¶³æ­¥æ€è§„åˆ’ä¸æ§åˆ¶ã€‚\",\n",
    "    \"SGLang ä¸“æ³¨æ¨ç†åŠ é€Ÿä¸ KV cache ç®¡ç†ï¼Œä¸æ¶‰åŠæ­¥æ€ç‰©ç†å»ºæ¨¡ã€‚\",\n",
    "    \"åœ¨ä»¿äººæœºå™¨äººä¸­ï¼ŒLIPM å¸¸ç”¨äºè¿‘ä¼¼ ZMP çº¦æŸï¼Œä»è€Œç”Ÿæˆå¯è¡Œçš„è¶³åº•æ”¯æŒå¤šè¾¹å½¢å†…çš„è´¨å¿ƒè½¨è¿¹ã€‚\"\n",
    "]\n",
    "nodes = [TextNode(text=t[:800]) for t in docs]  # ğŸ‘ˆ æ–‡æœ¬è£çŸ­ä»¥ä¿å®ˆå†…å­˜\n",
    "\n",
    "# 4) æ‰§è¡Œé‡æ’\n",
    "out = reranker.postprocess(nodes, QueryBundle(query))\n",
    "\n",
    "print(\"=== LlamaIndex + SentenceTransformerRerankï¼ˆé«˜->ä½ï¼‰===\")\n",
    "for i, n in enumerate(out, 1):\n",
    "    score = float(getattr(n, \"score\", 0.0))\n",
    "    print(f\"[{i}] score={score:.4f} | {n.get_content()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbea010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e2e8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86384f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "query = \"ä»€ä¹ˆæ˜¯ LIPMï¼ˆçº¿æ€§å€’ç«‹æ‘†æ¨¡å‹ï¼‰ï¼Œå®ƒåœ¨ä»¿äººè¡Œèµ°é‡Œæœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ\"\n",
    "docs = [\n",
    "    \"Qwen3 æ˜¯ä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹ç³»åˆ—ï¼Œä¸æœºå™¨äººåŠ¨åŠ›å­¦æ— å…³ã€‚\",\n",
    "    \"LIPMï¼ˆLinear Inverted Pendulum Modelï¼‰å°†è´¨å¿ƒè§†ä¸ºåœ¨å¸¸é«˜å¹³é¢ä¸Šè¿åŠ¨çš„å€’ç«‹æ‘†ï¼Œå¸¸ç”¨äºäººå½¢/åŒè¶³æ­¥æ€è§„åˆ’ä¸æ§åˆ¶ã€‚\",\n",
    "    \"SGLang ä¸“æ³¨æ¨ç†åŠ é€Ÿä¸ KV cache ç®¡ç†ï¼Œä¸æ¶‰åŠæ­¥æ€ç‰©ç†å»ºæ¨¡ã€‚\",\n",
    "    \"åœ¨ä»¿äººæœºå™¨äººä¸­ï¼ŒLIPM å¸¸ç”¨äºè¿‘ä¼¼ ZMP çº¦æŸï¼Œä»è€Œç”Ÿæˆå¯è¡Œçš„è¶³åº•æ”¯æŒå¤šè¾¹å½¢å†…çš„è´¨å¿ƒè½¨è¿¹ã€‚\"\n",
    "]\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"tomaarsen/Qwen3-Reranker-0.6B-seq-cls\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"tomaarsen/Qwen3-Reranker-0.6B-seq-cls\").eval()\n",
    "\n",
    "tok.pad_token = tok.eos_token if tok.pad_token is None else tok.pad_token\n",
    "model.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "batch = tok([query]*len(docs), docs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    scores = model(**batch).logits.view(-1).tolist()  # åˆ†æ•°è¶Šå¤§è¶Šç›¸å…³\n",
    "\n",
    "ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "print(\"=== seq-cls ç‰ˆæ‰“åˆ†ï¼ˆé«˜->ä½ï¼‰===\")\n",
    "for i, (t, s) in enumerate(ranked, 1):\n",
    "    print(f\"[{i}] score={s:.4f} | {t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c7273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_rerank_transformers_fixed2.py\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "def ensure_pad_token(tok, model):\n",
    "    if tok.pad_token is None:\n",
    "        if tok.eos_token is not None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "        else:\n",
    "            tok.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "            model.resize_token_embeddings(len(tok))\n",
    "    model.config.pad_token_id = tok.pad_token_id\n",
    "    tok.padding_side = \"right\"\n",
    "\n",
    "query = \"ä»€ä¹ˆæ˜¯ LIPMï¼ˆçº¿æ€§å€’ç«‹æ‘†æ¨¡å‹ï¼‰ï¼Œå®ƒåœ¨ä»¿äººè¡Œèµ°é‡Œæœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ\"\n",
    "passages = [\n",
    "    \"Qwen3 æ˜¯ä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹ç³»åˆ—ï¼Œä¸æœºå™¨äººåŠ¨åŠ›å­¦æ— å…³ã€‚\",\n",
    "    \"LIPMï¼ˆLinear Inverted Pendulum Modelï¼‰å°†è´¨å¿ƒè§†ä¸ºåœ¨å¸¸é«˜å¹³é¢ä¸Šè¿åŠ¨çš„å€’ç«‹æ‘†ï¼Œå¸¸ç”¨äºäººå½¢/åŒè¶³æ­¥æ€è§„åˆ’ä¸æ§åˆ¶ã€‚\",\n",
    "    \"SGLang ä¸“æ³¨æ¨ç†åŠ é€Ÿä¸ KV cache ç®¡ç†ï¼Œä¸æ¶‰åŠæ­¥æ€ç‰©ç†å»ºæ¨¡ã€‚\",\n",
    "    \"åœ¨ä»¿äººæœºå™¨äººä¸­ï¼ŒLIPM å¸¸ç”¨äºè¿‘ä¼¼ ZMP çº¦æŸï¼Œä»è€Œç”Ÿæˆå¯è¡Œçš„è¶³åº•æ”¯æŒå¤šè¾¹å½¢å†…çš„è´¨å¿ƒè½¨è¿¹ã€‚\"\n",
    "]\n",
    "\n",
    "device = torch.device(\"cpu\")  # æœ‰ GPU å¯æ”¹ \"cuda\"\n",
    "tok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", trust_remote_code=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"Qwen/Qwen3-Reranker-0.6B\", trust_remote_code=True\n",
    ").to(device).eval()\n",
    "\n",
    "ensure_pad_token(tok, model)\n",
    "\n",
    "pairs_q = [query] * len(passages)\n",
    "pairs_p = passages\n",
    "\n",
    "inputs = tok(\n",
    "    pairs_q,\n",
    "    pairs_p,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\",\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(**inputs)\n",
    "    # logits shape: [N, 1] â†’ flatten to [N]\n",
    "    scores = out.logits.view(-1).cpu().tolist()  # ğŸ‘ˆ å¼ºåˆ¶å±•å¹³æˆä¸€ç»´\n",
    "\n",
    "ranked = sorted(zip(passages, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n=== Transformers ç›´ç®—å¾—åˆ†ï¼ˆé«˜->ä½ï¼‰===\")\n",
    "for i, (text, s) in enumerate(ranked, 1):\n",
    "    print(f\"[{i}] score={s:.4f} | {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a86eb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  è¿˜å¯ä»¥å§ qwen\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from typing import List, Tuple\n",
    "\n",
    "# è®¾ç½®è®¾å¤‡\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# æ¨¡å‹åç§°\n",
    "model_name = \"Qwen/Qwen3-Reranker-0.6B\"\n",
    "\n",
    "# åŠ è½½ tokenizer å’Œ æ¨¡å‹\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
    ").to(device)\n",
    "\n",
    "# ç¤ºä¾‹æ•°æ®\n",
    "query = \"äººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿\"\n",
    "candidates = [\n",
    "    \"äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œï¼Œæ·±åº¦å­¦ä¹ ã€å¤§æ¨¡å‹æ¨åŠ¨æŠ€æœ¯è¿›æ­¥ã€‚\",\n",
    "    \"è‹¹æœå…¬å¸å‘å¸ƒäº†æ–°æ¬¾ iPhoneï¼Œæ€§èƒ½æ›´å¼ºï¼Œæ‘„åƒå¤´æ›´æ¸…æ™°ã€‚\",\n",
    "    \"æœºå™¨å­¦ä¹ å’Œç¥ç»ç½‘ç»œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­å¹¿æ³›åº”ç”¨ã€‚\",\n",
    "    \"å¤©æ°”é¢„æŠ¥è¯´æ˜å¤©æœ‰é›¨ï¼Œè®°å¾—å¸¦ä¼ã€‚\",\n",
    "    \"å¤§æ¨¡å‹å¦‚ Qwenã€LLaMA æ­£åœ¨æ¨åŠ¨ AI ä»£ç†çš„å‘å±•ã€‚\",\n",
    "]\n",
    "\n",
    "print(\"\\nQuery:\", query)\n",
    "print(\"\\nCandidates:\")\n",
    "for i, cand in enumerate(candidates):\n",
    "    print(f\"[{i}] {cand}\")\n",
    "\n",
    "# æ„é€ è¾“å…¥å¹¶æ‰“åˆ†\n",
    "scores = []\n",
    "inputs_for_model = []\n",
    "\n",
    "with torch.no_grad():  # æ¨ç†é˜¶æ®µï¼Œå…³é—­æ¢¯åº¦\n",
    "    for i, doc in enumerate(candidates):\n",
    "        # æ‹¼æ¥ query å’Œ document\n",
    "        text = f\"Query: {query}\\nDoc: {doc}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = tokenizer(\n",
    "            text,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=8192\n",
    "        ).to(device)\n",
    "        \n",
    "        # å‰å‘ä¼ æ’­\n",
    "        outputs = model(**encoded)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # ğŸ‘‡ ä¿®å¤ï¼šæ ¹æ® num_labels æå–æ­£ç¡®åˆ†æ•°\n",
    "        if logits.shape[-1] == 1:\n",
    "            score = logits.item()\n",
    "        else:\n",
    "            score = logits[0, 1].item()  # å–æ­£ç±»ï¼ˆç›¸å…³ï¼‰åˆ†æ•°\n",
    "\n",
    "        scores.append(score)\n",
    "        inputs_for_model.append(encoded)\n",
    "\n",
    "# æ’åºï¼šæŒ‰åˆ†æ•°ä»é«˜åˆ°ä½\n",
    "ranked = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# è¾“å‡ºæ’åºç»“æœ\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ” Reranking Results (Higher score = more relevant)\")\n",
    "print(\"=\"*50)\n",
    "for rank, (idx, score) in enumerate(ranked, 1):\n",
    "    print(f\"Rank {rank}: [Score: {score:+.3f}]\")\n",
    "    print(f\"  {candidates[idx][:100]}{'...' if len(candidates[idx]) > 100 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1701995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "éªŒè¯ bge-reranker-base èƒ½å¦æ­£å¸¸æ‰“åˆ†å¹¶é‡æ’\n",
    "\"\"\"\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "# 0. é€ å‡ æ¡å‡è®¾çš„å€™é€‰æ–‡æœ¬\n",
    "query = \"ä¸­å›½çš„é¦–éƒ½åœ¨å“ªï¼Ÿ\"\n",
    "candidates = [\n",
    "    \"åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ï¼Œä¹Ÿæ˜¯æ”¿æ²»ä¸­å¿ƒã€‚\",\n",
    "    \"ä¸Šæµ·æ˜¯ä¸­å›½æœ€å¤§çš„ç»æµä¸­å¿ƒã€‚\",\n",
    "    \"å¹¿å·ä½äºå—æ–¹ï¼Œæ°”å€™æ¸©æš–ã€‚\",\n",
    "    \"åŒ—äº¬æœ‰æ•…å®«ã€å¤©å®‰é—¨ç­‰è‘—åæ™¯ç‚¹ã€‚\",\n",
    "]\n",
    "\n",
    "# maidalun/bce-reranker-base_v1  ä¸­æ–‡ä¼˜åŒ–\n",
    "# BAAI/bge-reranker-base         ä¸­è‹±åŒè¯­\n",
    "# BAAI/bge-reranker-large        large\n",
    "\n",
    "\n",
    "# 1. ç›´æ¥ç”¨ sentence-transformers æ‰“åŸå§‹åˆ†\n",
    "model = CrossEncoder(\"BAAI/bge-reranker-base\", device=\"cpu\")  \n",
    "pairs = [(query, c) for c in candidates]\n",
    "raw_scores = model.predict(pairs)\n",
    "print(\"--- åŸå§‹æ‰“åˆ† ---\")\n",
    "for c, s in zip(candidates, raw_scores):\n",
    "    print(f\"{s:.4f}  {c}\")\n",
    "\n",
    "# 2. ç”¨ llama-index çš„ reranker åŒ…è£…ï¼Œå†è·‘ä¸€æ¬¡\n",
    "nodes = [TextNode(text=c) for c in candidates]\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model=\"BAAI/bge-reranker-base\",\n",
    "    top_n=3,           # åªä¿ç•™å‰ 3\n",
    "    device=\"cpu\"\n",
    ")\n",
    "ranked_nodes = reranker.postprocess_nodes(\n",
    "    nodes=nodes,\n",
    "    query_str=query\n",
    ")\n",
    "\n",
    "print(\"\\n--- llama-index é‡æ’åï¼ˆtop_n=3ï¼‰---\")\n",
    "for i, n in enumerate(ranked_nodes, 1):\n",
    "    print(f\"{i}. {n.score:.4f}  {n.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb73ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f4f686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
