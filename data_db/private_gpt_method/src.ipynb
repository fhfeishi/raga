{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b4fa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zzb  需要  settings.yaml\n",
    "from pathlib import Path\n",
    "from zzb import build_injector, IngestService\n",
    "\n",
    "inj = build_injector()\n",
    "svc = inj.get(IngestService)\n",
    "\n",
    "\n",
    "mdp = r\"D:\\codespace\\fhfeishi\\raga\\scripts\\PatentParser\\full.md\"\n",
    "docs = svc.ingest_file(mdp, Path(\"mdp\"))\n",
    "print(\"ingested:\", [d.doc_id for d in docs])\n",
    "\n",
    "print(\"list:\", svc.list_ingested()[:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# api  \n",
    "\n",
    "local_dir = r\"E:\\local_models\\huggingface\\cache\\hub\"\n",
    "\n",
    "embeds = HuggingFaceEmbedding(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    device=\"cpu\",                 # 建议放顶层\n",
    "    trust_remote_code=True,       # 建议放顶层\n",
    "    cache_folder=local_dir,\n",
    "    model_kwargs={\"local_files_only\": False},   # 允许联网 False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a98f258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "\n",
    "import sys, transformers, huggingface_hub, sentence_transformers\n",
    "print(\"py:\", sys.executable)\n",
    "print(\"tf:\", transformers.__version__, \"hub:\", huggingface_hub.__version__, \"st:\", sentence_transformers.__version__)\n",
    "print(\"HF_HOME:\", os.getenv(\"HF_HOME\"))\n",
    "print(\"HF_ENDPOINT:\", os.getenv(\"HF_ENDPOINT\"))\n",
    "\n",
    "\n",
    "# —— 离线&缓存环境（Windows 下也OK）——\n",
    "os.environ[\"HF_HOME\"] = r\"E:\\local_models\\huggingface\\cache\"\n",
    "# os.environ[\"HF_HUB_OFFLINE\"] = \"0\"\n",
    "# os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "# os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"  # 关闭 symlink 警告\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://mirrors.tuna.tsinghua.edu.cn/hugging-face/\"\n",
    "\n",
    "\n",
    "name_or_path = \"Qwen/Qwen3-Embedding-0.6B\"  # 也可换成本地目录（见方案2）\n",
    "device = \"cpu\"  # 或 \"cuda:0\"（如果有GPU）\n",
    "\n",
    "# 第一次若缓存不全，把 local_files_only 改为 False 联网补齐一次\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    name_or_path, trust_remote_code=True, local_files_only=False,\n",
    "    cache_dir=os.environ[\"HF_HOME\"],\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    name_or_path, trust_remote_code=True, local_files_only=True,\n",
    "    cache_dir=os.environ[\"HF_HOME\"],\n",
    ").to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6416cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test zzc\n",
    "import os\n",
    "from pathlib import Path\n",
    "from zzc import build_injector, IngestService\n",
    "from zzc.di import EmbeddingComponent  # 仅用于冒烟\n",
    "\n",
    "# --- 环境变量 ---\n",
    "os.environ['PGM_EMBED_MODEL'] = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "os.environ['PGM_DEVICE'] = \"cpu\"             # 无GPU就改 \"cpu\"\n",
    "os.environ['PGM_LOCAL_FILES_ONLY'] = \"0\"        # 首次跑若没缓存，改成 \"0\"\n",
    "os.environ['PGM_CHROMA_DIR'] = \"/data/pgm/chroma\"\n",
    "os.environ['PGM_CHROMA_COLLECTION'] = \"pgm_collection\"\n",
    "os.environ['PGM_PERSIST_DIR'] = \"/data/pgm/storage\"\n",
    "os.environ['PGM_SENT_WINDOW'] = \"3\"\n",
    "\n",
    "inj = build_injector()\n",
    "\n",
    "# 先冒烟：仅加载嵌入模型，验证“缓存+离线”是否OK\n",
    "_ = inj.get(EmbeddingComponent)\n",
    "\n",
    "svc: IngestService = inj.get(IngestService)\n",
    "\n",
    "# 路径：确保与当前运行 OS 一致\n",
    "mdp = r\"D:\\codespace\\fhfeishi\\raga\\scripts\\PatentParser\\full.md\"\n",
    "p = Path(mdp)\n",
    "assert p.exists(), f\"文件不存在：{p}（注意当前运行环境的操作系统路径）\"\n",
    "\n",
    "docs = svc.ingest_file(p.name, p)\n",
    "print([d.doc_id for d in docs])\n",
    "print(svc.list_ingested()[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a493a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test   zzc\n",
    "import os \n",
    "\n",
    "# environment variables\n",
    "os.environ['PGM_EMBED_MODEL'] = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "os.environ['PGM_DEVICE'] = \"cuda:0\"\n",
    "os.environ['PGM_LOCAL_FILES_ONLY'] = \"1\"    \n",
    "os.environ['PGM_CHROMA_DIR'] = \"/data/pgm/chroma\"\n",
    "os.environ['PGM_CHROMA_COLLECTION'] = \"pgm_collection\"\n",
    "os.environ['PGM_PERSIST_DIR'] = \"/data/pgm/storage\"\n",
    "os.environ['PGM_SENT_WINDOW'] = \"3\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from zzc import build_injector, IngestService\n",
    "\n",
    "inj = build_injector()\n",
    "svc: IngestService = inj.get(IngestService)\n",
    "\n",
    "mdp = r\"D:\\codespace\\fhfeishi\\raga\\scripts\\PatentParser\\full.md\"\n",
    "docs = svc.ingest_file(mdp, Path(mdp))\n",
    "print([d.doc_id for d in docs])\n",
    "\n",
    "print(svc.list_ingested()[:3])\n",
    "\n",
    "# 删除\n",
    "# svc.delete(\"demo.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5454c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "\n",
    "\n",
    "# ollama\n",
    "\n",
    "\n",
    "model_cache_root = r\"E:\\local_models\\huggingface\\cache\\hub\"\n",
    "# Qwen/Qwen3-1.7B  Qwen/Qwen3-Embedding-0.6B  Qwen/Qwen3-Reranker-0.6B\n",
    "\n",
    "# prompt\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "# chat \n",
    "from llama_index.llms.huggingface import HuggingFaceLLM \n",
    "chat_model = HuggingFaceLLM(\n",
    "    # cache_root可以省，model_name\n",
    "    # or  local_dir\n",
    "    model_name     = r\"Qwen/Qwen3-1.7B\",\n",
    "    tokenizer_name = r\"Qwen/Qwen3-1.7B\",\n",
    "    context_window = 3900,  \n",
    "    max_new_tokens = 640,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"top_k\": 30, \"top_p\": 0.95},\n",
    "    device_map     ='cpu' \n",
    ")\n",
    "\n",
    "# reranker  \n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model = \"Qwen/Qwen3-Reranker-0.6B\",\n",
    "    top_n = 4,\n",
    "    trust_remote_code=True,     \n",
    "    device= 'cpu',\n",
    ") \n",
    "\n",
    "# embedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embedding = HuggingFaceEmbedding(\n",
    "    # cache\n",
    "    model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    cache_folder=model_cache_root, \n",
    "    # local-dir\n",
    "    # model_name = r\"E:\\local_models\\huggingface\\local\\path_to_qwen3Embedding0.6b_load_dir\n",
    "    max_length=1024,\n",
    "    trust_remote_code=True,\n",
    "    model_kwargs={\"local_files_only\": True},   # 允许联网 False , 禁止联网 True\n",
    "    device='cpu',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d9786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires transformers>=4.51.0   qwen-official\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def format_instruction(instruction, query, doc):\n",
    "    if instruction is None:\n",
    "        instruction = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "    output = \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(instruction=instruction,query=query, doc=doc)\n",
    "    return output\n",
    "\n",
    "def process_inputs(pairs):\n",
    "    inputs = tokenizer(\n",
    "        pairs, padding=False, truncation='longest_first',\n",
    "        return_attention_mask=False, max_length=max_length - len(prefix_tokens) - len(suffix_tokens)\n",
    "    )\n",
    "    for i, ele in enumerate(inputs['input_ids']):\n",
    "        inputs['input_ids'][i] = prefix_tokens + ele + suffix_tokens\n",
    "    inputs = tokenizer.pad(inputs, padding=True, return_tensors=\"pt\", max_length=max_length)\n",
    "    for key in inputs:\n",
    "        inputs[key] = inputs[key].to(model.device)\n",
    "    return inputs\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_logits(inputs, **kwargs):\n",
    "    batch_scores = model(**inputs).logits[:, -1, :]\n",
    "    true_vector = batch_scores[:, token_true_id]\n",
    "    false_vector = batch_scores[:, token_false_id]\n",
    "    batch_scores = torch.stack([false_vector, true_vector], dim=1)\n",
    "    batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n",
    "    scores = batch_scores[:, 1].exp().tolist()\n",
    "    return scores\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\").eval()\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").cuda().eval()\n",
    "token_false_id = tokenizer.convert_tokens_to_ids(\"no\")\n",
    "token_true_id = tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "max_length = 8192\n",
    "\n",
    "prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    "suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "prefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "suffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)\n",
    "        \n",
    "task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "\n",
    "queries = [\"What is the capital of China?\",\n",
    "    \"Explain gravity\",\n",
    "]\n",
    "\n",
    "documents = [\n",
    "    \"The capital of China is Beijing.\",\n",
    "    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n",
    "]\n",
    "\n",
    "pairs = [format_instruction(task, query, doc) for query, doc in zip(queries, documents)]\n",
    "\n",
    "# Tokenize the input texts\n",
    "inputs = process_inputs(pairs)\n",
    "scores = compute_logits(inputs)\n",
    "\n",
    "print(\"scores: \", scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ebebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen3_rerank_official.py\n",
    "# pip install \"transformers>=4.51.0\" torch\n",
    "\n",
    "\"\"\"  \n",
    "Qwen3-Reranker-0.6B 官方是按 CausalLM + yes/no 概率来打分的，\n",
    "不是 AutoModelForSequenceClassification 那一套。因此我们前面用分类头取 logits 会得到乱序分数。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def format_instruction(instruction, query, doc):\n",
    "    if instruction is None:\n",
    "        instruction = \"Given a web search query, retrieve relevant passages that answer the query\"\n",
    "    return f\"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\"\n",
    "\n",
    "def process_inputs(tokenizer, pairs, max_length=8192, prefix=None, suffix=None):\n",
    "    # 官方模板（来自模型卡）\n",
    "    if prefix is None:\n",
    "        prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    "    if suffix is None:\n",
    "        suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "\n",
    "    prefix_ids = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "    suffix_ids = tokenizer.encode(suffix, add_special_tokens=False)\n",
    "\n",
    "    enc = tokenizer(\n",
    "        pairs,\n",
    "        padding=False,\n",
    "        truncation=\"longest_first\",\n",
    "        return_attention_mask=False,\n",
    "        max_length=max_length - len(prefix_ids) - len(suffix_ids),\n",
    "    )\n",
    "    # 手动拼接 prefix / suffix，再统一 pad\n",
    "    for i, ids in enumerate(enc[\"input_ids\"]):\n",
    "        enc[\"input_ids\"][i] = prefix_ids + ids + suffix_ids\n",
    "\n",
    "    enc = tokenizer.pad(enc, padding=True, return_tensors=\"pt\", max_length=max_length)\n",
    "    return enc\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_scores(model, tokenizer, inputs):\n",
    "    # 只看最后一个位置对 \"yes\"/\"no\" 的对数几率\n",
    "    out = model(**{k: v.to(model.device) for k, v in inputs.items()})\n",
    "    last_token_logits = out.logits[:, -1, :]  # [N, vocab]\n",
    "    tok_yes = tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "    tok_no  = tokenizer.convert_tokens_to_ids(\"no\")\n",
    "    yes_logit = last_token_logits[:, tok_yes]\n",
    "    no_logit  = last_token_logits[:, tok_no]\n",
    "    # softmax 取 “yes” 概率\n",
    "    prob_yes = torch.softmax(torch.stack([no_logit, yes_logit], dim=1), dim=1)[:, 1]\n",
    "    return prob_yes.cpu().tolist()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"什么是 LIPM（线性倒立摆模型），它在仿人行走里有什么作用？\"\n",
    "    docs = [\n",
    "        \"Qwen3 是一个大语言模型系列，与机器人动力学无关。\",\n",
    "        \"LIPM（Linear Inverted Pendulum Model）将质心视为在常高平面上运动的倒立摆，常用于人形/双足步态规划与控制。\",\n",
    "        \"SGLang 专注推理加速与 KV cache 管理，不涉及步态物理建模。\",\n",
    "        \"在仿人机器人中，LIPM 常用于近似 ZMP 约束，从而生成可行的足底支持多边形内的质心轨迹。\"\n",
    "    ]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", padding_side=\"left\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\").eval()  # 有GPU可加 .cuda()\n",
    "\n",
    "    pairs = [format_instruction(None, query, d) for d in docs]\n",
    "    inputs = process_inputs(tokenizer, pairs)\n",
    "    scores = compute_scores(model, tokenizer, inputs)  # 分数越大越相关\n",
    "\n",
    "    ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "    print(\"=== Qwen3 官方打分（高->低）===\")\n",
    "    for i, (t, s) in enumerate(ranked, 1):\n",
    "        print(f\"[{i}] score={s:.4f} | {t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b1059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_rerank_llamaindex.py\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core import QueryBundle\n",
    "\n",
    "# 1) 准备查询与候选段落\n",
    "query = \"什么是 LIPM（线性倒立摆模型），它在仿人行走里有什么作用？\"\n",
    "candidates = [\n",
    "    \"Qwen3 是一个大语言模型系列，与机器人动力学无关。\",\n",
    "    \"LIPM（Linear Inverted Pendulum Model）将质心视为在常高平面上运动的倒立摆，常用于人形/双足步态规划与控制。\",\n",
    "    \"SGLang 专注推理加速与 KV cache 管理，不涉及步态物理建模。\",\n",
    "    \"在仿人机器人中，LIPM 常用于近似 ZMP 约束，从而生成可行的足底支持多边形内的质心轨迹。\"\n",
    "]\n",
    "\n",
    "# 创建节点\n",
    "nodes = [TextNode(text=txt) for txt in candidates]\n",
    "\n",
    "# 2) 初始化 Reranker\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model=\"Qwen/Qwen3-Reranker-0.6B\",\n",
    "    top_n=4,\n",
    "    device=\"cpu\",  # 如果有 GPU 可用，改为 \"cuda\"\n",
    "    keep_retrieval_score=False,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# 3) 使用同步方法（注意：是 postprocess_nodes，不是 apostprocess_nodes）\n",
    "reranked = reranker.postprocess_nodes(\n",
    "    nodes=nodes,\n",
    "    query_bundle=QueryBundle(query)\n",
    ")\n",
    "\n",
    "# 4) 打印结果\n",
    "print(\"\\n=== LlamaIndex Rerank 结果（高->低）===\")\n",
    "for i, node in enumerate(reranked, 1):\n",
    "    score = getattr(node, 'score', None)\n",
    "    if score is not None:\n",
    "        print(f\"[{i}] score={score:.4f} | {node.get_content()}\")\n",
    "    else:\n",
    "        print(f\"[{i}] score=None | {node.get_content()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5160713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U sentence-transformers \"llama-index-core>=0.10.64\" transformers torch\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"   # 降低线程开销（笔记本/CPU友好）\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"              # 可选\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"              # 可选\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core import QueryBundle\n",
    "\n",
    "# 1) 更轻量的 reranker（推荐先用它验证链路）\n",
    "MODEL_NAME = \"BAAI/bge-reranker-base\"  # 也可尝试 \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "ce = CrossEncoder(MODEL_NAME, max_length=256, device=\"cpu\")  # 压短序列，省内存\n",
    "\n",
    "# 2) LlamaIndex 的 reranker（用我们自己创建的 CrossEncoder 实例）\n",
    "reranker = SentenceTransformerRerank(model=ce, top_n=3)\n",
    "\n",
    "# 3) 准备数据（尽量裁短文本，避免 OOM）\n",
    "query = \"什么是 LIPM（线性倒立摆模型），它在仿人行走里有什么作用？\"\n",
    "docs = [\n",
    "    \"Qwen3 是一个大语言模型系列，与机器人动力学无关。\",\n",
    "    \"LIPM（Linear Inverted Pendulum Model）将质心视为在常高平面上运动的倒立摆，常用于人形/双足步态规划与控制。\",\n",
    "    \"SGLang 专注推理加速与 KV cache 管理，不涉及步态物理建模。\",\n",
    "    \"在仿人机器人中，LIPM 常用于近似 ZMP 约束，从而生成可行的足底支持多边形内的质心轨迹。\"\n",
    "]\n",
    "nodes = [TextNode(text=t[:800]) for t in docs]  # 👈 文本裁短以保守内存\n",
    "\n",
    "# 4) 执行重排\n",
    "out = reranker.postprocess(nodes, QueryBundle(query))\n",
    "\n",
    "print(\"=== LlamaIndex + SentenceTransformerRerank（高->低）===\")\n",
    "for i, n in enumerate(out, 1):\n",
    "    score = float(getattr(n, \"score\", 0.0))\n",
    "    print(f\"[{i}] score={score:.4f} | {n.get_content()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbea010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e2e8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86384f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "query = \"什么是 LIPM（线性倒立摆模型），它在仿人行走里有什么作用？\"\n",
    "docs = [\n",
    "    \"Qwen3 是一个大语言模型系列，与机器人动力学无关。\",\n",
    "    \"LIPM（Linear Inverted Pendulum Model）将质心视为在常高平面上运动的倒立摆，常用于人形/双足步态规划与控制。\",\n",
    "    \"SGLang 专注推理加速与 KV cache 管理，不涉及步态物理建模。\",\n",
    "    \"在仿人机器人中，LIPM 常用于近似 ZMP 约束，从而生成可行的足底支持多边形内的质心轨迹。\"\n",
    "]\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"tomaarsen/Qwen3-Reranker-0.6B-seq-cls\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"tomaarsen/Qwen3-Reranker-0.6B-seq-cls\").eval()\n",
    "\n",
    "tok.pad_token = tok.eos_token if tok.pad_token is None else tok.pad_token\n",
    "model.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "batch = tok([query]*len(docs), docs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    scores = model(**batch).logits.view(-1).tolist()  # 分数越大越相关\n",
    "\n",
    "ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "print(\"=== seq-cls 版打分（高->低）===\")\n",
    "for i, (t, s) in enumerate(ranked, 1):\n",
    "    print(f\"[{i}] score={s:.4f} | {t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c7273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_rerank_transformers_fixed2.py\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "def ensure_pad_token(tok, model):\n",
    "    if tok.pad_token is None:\n",
    "        if tok.eos_token is not None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "        else:\n",
    "            tok.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "            model.resize_token_embeddings(len(tok))\n",
    "    model.config.pad_token_id = tok.pad_token_id\n",
    "    tok.padding_side = \"right\"\n",
    "\n",
    "query = \"什么是 LIPM（线性倒立摆模型），它在仿人行走里有什么作用？\"\n",
    "passages = [\n",
    "    \"Qwen3 是一个大语言模型系列，与机器人动力学无关。\",\n",
    "    \"LIPM（Linear Inverted Pendulum Model）将质心视为在常高平面上运动的倒立摆，常用于人形/双足步态规划与控制。\",\n",
    "    \"SGLang 专注推理加速与 KV cache 管理，不涉及步态物理建模。\",\n",
    "    \"在仿人机器人中，LIPM 常用于近似 ZMP 约束，从而生成可行的足底支持多边形内的质心轨迹。\"\n",
    "]\n",
    "\n",
    "device = torch.device(\"cpu\")  # 有 GPU 可改 \"cuda\"\n",
    "tok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", trust_remote_code=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"Qwen/Qwen3-Reranker-0.6B\", trust_remote_code=True\n",
    ").to(device).eval()\n",
    "\n",
    "ensure_pad_token(tok, model)\n",
    "\n",
    "pairs_q = [query] * len(passages)\n",
    "pairs_p = passages\n",
    "\n",
    "inputs = tok(\n",
    "    pairs_q,\n",
    "    pairs_p,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\",\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(**inputs)\n",
    "    # logits shape: [N, 1] → flatten to [N]\n",
    "    scores = out.logits.view(-1).cpu().tolist()  # 👈 强制展平成一维\n",
    "\n",
    "ranked = sorted(zip(passages, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n=== Transformers 直算得分（高->低）===\")\n",
    "for i, (text, s) in enumerate(ranked, 1):\n",
    "    print(f\"[{i}] score={s:.4f} | {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a86eb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  还可以吧 qwen\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from typing import List, Tuple\n",
    "\n",
    "# 设置设备\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 模型名称\n",
    "model_name = \"Qwen/Qwen3-Reranker-0.6B\"\n",
    "\n",
    "# 加载 tokenizer 和 模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
    ").to(device)\n",
    "\n",
    "# 示例数据\n",
    "query = \"人工智能的发展趋势\"\n",
    "candidates = [\n",
    "    \"人工智能正在改变世界，深度学习、大模型推动技术进步。\",\n",
    "    \"苹果公司发布了新款 iPhone，性能更强，摄像头更清晰。\",\n",
    "    \"机器学习和神经网络在自然语言处理中广泛应用。\",\n",
    "    \"天气预报说明天有雨，记得带伞。\",\n",
    "    \"大模型如 Qwen、LLaMA 正在推动 AI 代理的发展。\",\n",
    "]\n",
    "\n",
    "print(\"\\nQuery:\", query)\n",
    "print(\"\\nCandidates:\")\n",
    "for i, cand in enumerate(candidates):\n",
    "    print(f\"[{i}] {cand}\")\n",
    "\n",
    "# 构造输入并打分\n",
    "scores = []\n",
    "inputs_for_model = []\n",
    "\n",
    "with torch.no_grad():  # 推理阶段，关闭梯度\n",
    "    for i, doc in enumerate(candidates):\n",
    "        # 拼接 query 和 document\n",
    "        text = f\"Query: {query}\\nDoc: {doc}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = tokenizer(\n",
    "            text,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=8192\n",
    "        ).to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(**encoded)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # 👇 修复：根据 num_labels 提取正确分数\n",
    "        if logits.shape[-1] == 1:\n",
    "            score = logits.item()\n",
    "        else:\n",
    "            score = logits[0, 1].item()  # 取正类（相关）分数\n",
    "\n",
    "        scores.append(score)\n",
    "        inputs_for_model.append(encoded)\n",
    "\n",
    "# 排序：按分数从高到低\n",
    "ranked = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 输出排序结果\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🔍 Reranking Results (Higher score = more relevant)\")\n",
    "print(\"=\"*50)\n",
    "for rank, (idx, score) in enumerate(ranked, 1):\n",
    "    print(f\"Rank {rank}: [Score: {score:+.3f}]\")\n",
    "    print(f\"  {candidates[idx][:100]}{'...' if len(candidates[idx]) > 100 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1701995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "验证 bge-reranker-base 能否正常打分并重排\n",
    "\"\"\"\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "# 0. 造几条假设的候选文本\n",
    "query = \"中国的首都在哪？\"\n",
    "candidates = [\n",
    "    \"北京是中国的首都，也是政治中心。\",\n",
    "    \"上海是中国最大的经济中心。\",\n",
    "    \"广州位于南方，气候温暖。\",\n",
    "    \"北京有故宫、天安门等著名景点。\",\n",
    "]\n",
    "\n",
    "# maidalun/bce-reranker-base_v1  中文优化\n",
    "# BAAI/bge-reranker-base         中英双语\n",
    "# BAAI/bge-reranker-large        large\n",
    "\n",
    "\n",
    "# 1. 直接用 sentence-transformers 打原始分\n",
    "model = CrossEncoder(\"BAAI/bge-reranker-base\", device=\"cpu\")  \n",
    "pairs = [(query, c) for c in candidates]\n",
    "raw_scores = model.predict(pairs)\n",
    "print(\"--- 原始打分 ---\")\n",
    "for c, s in zip(candidates, raw_scores):\n",
    "    print(f\"{s:.4f}  {c}\")\n",
    "\n",
    "# 2. 用 llama-index 的 reranker 包装，再跑一次\n",
    "nodes = [TextNode(text=c) for c in candidates]\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model=\"BAAI/bge-reranker-base\",\n",
    "    top_n=3,           # 只保留前 3\n",
    "    device=\"cpu\"\n",
    ")\n",
    "ranked_nodes = reranker.postprocess_nodes(\n",
    "    nodes=nodes,\n",
    "    query_str=query\n",
    ")\n",
    "\n",
    "print(\"\\n--- llama-index 重排后（top_n=3）---\")\n",
    "for i, n in enumerate(ranked_nodes, 1):\n",
    "    print(f\"{i}. {n.score:.4f}  {n.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb73ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f4f686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
