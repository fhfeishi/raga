{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80fd6911",
   "metadata": {},
   "source": [
    "### ingest  private-gpt  解析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9631f752",
   "metadata": {},
   "source": [
    "```python\n",
    "#### components/ingest_helper.py\n",
    "# IngestionHelper  加载files，-> [Document] filter-node-metadata\n",
    "\"\"\"  \n",
    "BaseNode (抽象基类)\n",
    " ├─ TextNode\n",
    " │    └─ Document   ← 你最常用/读写的入口（整份文件或一页/一段）\n",
    " ├─ ImageNode\n",
    " ├─ IndexNode\n",
    " └─ ...（其他模态/复合节点）\n",
    "\n",
    "- BaseNode：所有“可被索引/检索”的最小通用接口。\n",
    "- TextNode：带 text 的文本节点。\n",
    "- Document：实质上就是一个“整份文档/页”的 TextNode（许多版本里 Document 直接继承 TextNode）。读文件的 reader 通常返回 list[Document] 作为初始输入。\n",
    "-  IngestionHelper.transform_file_into_documents() 里做的事：把各种文件读成 List[Document]，再做 metadata 规范化与清洗。\n",
    "\n",
    "\n",
    "- BaseNode：统一抽象（ID、metadata、关系、可被索引/检索/持久化）。\n",
    "- Document：读入阶段的“整文/整页文本节点”，是最自然的入口；从它向下切块得到更多 TextNode。\n",
    "- Node（TextNode 等）：最小检索单元，被向量化、被重排、被拼成上下文回给 LLM。\n",
    "\n",
    "# BaseNode 是什么（共同的骨架）\n",
    "任何节点（文本、图片、索引引用…）都具备这些“基因”——常见属性/行为（名字可能随版本有细微差别，但概念稳定）：\n",
    "id_：节点的唯一 ID（一个 chunk 一条 ID）。\n",
    "metadata: dict：任意键值对（如 file_name, page_label, source 等）。\n",
    "relationships：前后文/层级联系（如 PREVIOUS/NEXT/PARENT/SOURCE 等关系，用于重建上下文或来源）。\n",
    "hash：内容哈希，便于幂等/变更检测。\n",
    "excluded_embed_metadata_keys：做向量化时不带入的 metadata（减少噪声）。\n",
    "excluded_llm_metadata_keys：喂给 LLM 上下文时不带入的 metadata（避免 prompt 污染）。\n",
    "（有的版本）ref_doc_id 或通过关系把节点映射回原始文档，用于 delete_ref_doc 之类操作。\n",
    "作用：统一“索引/存储/检索”的数据接口——向量库只关心它的文本与 embedding、LLM 只关心它的文本与必要 metadata、文档层级/前后窗口用 relationships 还原。\n",
    "\n",
    "# TextNode：带文本的通用节点\n",
    "在 BaseNode 基础上，TextNode提供：\n",
    "text: str：节点文本（chunk 的正文）。\n",
    "（常见）start_char_idx/end_char_idx：文本在原文中的切片位置（用于追踪来源、展示命中高亮）。\n",
    "其他用于格式化/模板的字段（不同版本略有差异）。\n",
    "作用：凡是“可被切块后的文本单元”，基本就是 TextNode（或其子类）。\n",
    "\n",
    "\n",
    "# Document：入口文档，也是文本节点\n",
    "Document 是 TextNode 的一个“角色化”版本，语义是“一份原始文档（或 PDF 的一页、或 HTML 的一个段落）”。在多数 Reader 里，读取一个文件会返回若干个 Document（PDF 常常是“每页一个 Document”）。\n",
    "为什么工程里用它当入口？\n",
    "Reader 输出：reader.load_data(path) → list[Document]\n",
    "携带原始语义：你可以在 metadata 放 file_name, page_label, source, mtime 等，后面切块时会自动继承。\n",
    "支持删除/追踪：很多删除操作（delete_ref_doc）是按“文档维度”生效的：你把文档的 doc_id 记录到每个 chunk（node）的 ref_doc 关系里，后面就能通过 doc_id 一键删掉这个文档对应的所有节点与向量。\n",
    "\n",
    "document.metadata[\"file_name\"] = file_name\n",
    "document.metadata[\"doc_id\"] = document.doc_id\n",
    "document.excluded_embed_metadata_keys = [\"doc_id\"]\n",
    "document.excluded_llm_metadata_keys = [\"file_name\", \"doc_id\", \"page_label\"]\n",
    "给 Document 附上来源；\n",
    "用 doc_id 当“文档级别的主键”，供后续删除/查询；\n",
    "设置 excluded_*：避免把无关 metadata 污染 embedding/LLM。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# components/ingest_component.py\n",
    "\"\"\"\n",
    "interface:\n",
    "# llama_index.core.schema\n",
    "    - Document：文本文档节点（继承自 TextNode/Node）；reader 输出的基本单位；携带 text/metadata/doc_id。\n",
    "    - BaseNode：所有节点的抽象基类；insert_nodes() 接口处理的对象类型。\n",
    "    - TransformComponent：变换组件协议（如 splitter/embedding）；run_transformations 的元素类型。\n",
    "# llama_index.core.ingestion\n",
    "    - run_transformations(documents, transformations, ...)：按顺序执行一串变换（典型是 [node_parser/splitter, embedder]），将 Document 变成带向量的 BaseNode 列表。\n",
    "# llama_index.core.indices\n",
    "    - VectorStoreIndex：RAG 常用的索引类型；支持 from_documents()、insert(document)、insert_nodes(nodes)、delete_ref_doc(doc_id, ...)。\n",
    "    - load_index_from_storage(storage_context, ...)：从持久化存储中加载索引（与 StorageContext 配合）。\n",
    "# llama_index.core.indices.base\n",
    "    - BaseIndex：索引抽象基类（这里主要用作类型注解）。\n",
    "# llama_index.core.data_structs\n",
    "    - IndexDict：某些索引的内部数据结构类型参数（用于 BaseIndex[IndexDict] 的类型标注）。\n",
    "# llama_index.core.storage\n",
    "    - StorageContext：封装 vector_store / docstore / index_store 的上下文；负责持久化、加载、协调三者。\n",
    "# llama_index.core.embeddings.utils\n",
    "    - EmbedType：对“可用作嵌入器”的类型别名（抽象成一个联合/协议），便于函数签名与类型提示。\n",
    "工程内自有部分\n",
    "# IngestionHelper：文件→Documents 的解析与元数据规范化。\n",
    "# local_data_path：存储持久化位置。\n",
    "# Settings：工程配置（决定 ingest_mode / workers 等）。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# settings.py\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c65992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54795f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84f3e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90d98769",
   "metadata": {},
   "source": [
    "#### 按需import的方法\n",
    "\n",
    "哪个先导入成功就立刻返回对应的类；只有当所有路径都失败时才抛出最后一次的异常。 (吞掉所有的异常)\n",
    "\n",
    "tgt_clss = _import([\"from a import tgt_clss\",\n",
    "                    \"from a.sub import tgt_clss\",\n",
    "                    \"from a.core import tgt_clss\",\n",
    "                    \"from a.new import tgt_clss\"])\n",
    "\n",
    "\n",
    "```python\n",
    "# ---------- 工厂：延迟导入，适配不同版本的 llama-index ----------\n",
    "def _import(path_variants: list[str]):\n",
    "    \"\"\"尝试多条导入路径，适配 llama-index 不同版本目录结构。\"\"\"\n",
    "    last_err = None\n",
    "    for p in path_variants:\n",
    "        try:\n",
    "            module_path, name = p.rsplit(\".\", 1)\n",
    "            mod = __import__(module_path, fromlist=[name])\n",
    "            return getattr(mod, name)\n",
    "        except Exception as e:  # pragma: no cover\n",
    "            last_err = e\n",
    "    raise last_err\n",
    "``` \n",
    "\n",
    "优化版本，有个error-log\n",
    "tgt_clss = safe_import([\"a.tgt_clss\",\n",
    "                        \"a.sub.tgt_clss\",\n",
    "                        \"a.core.tgt_clss\",\n",
    "                        \"a.new.tgt_clss\"])\n",
    "\n",
    "```python\n",
    "from importlib import import_module\n",
    "\n",
    "def safe_import(path_variants: list[str]):\n",
    "    errors = []\n",
    "    for p in path_variants:\n",
    "        try:\n",
    "            module_path, name = p.rsplit(\".\", 1)\n",
    "            mod = import_module(module_path)\n",
    "            return getattr(mod, name)\n",
    "        except (ImportError, ModuleNotFoundError, AttributeError) as e:\n",
    "            errors.append(f\"{p} -> {e}\")\n",
    "            continue\n",
    "    raise ImportError(\"All import variants failed:\\n\" + \"\\n\".join(errors))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614292db",
   "metadata": {},
   "source": [
    "#### 注入器：  \n",
    "- `cfg.yaml --- 通过接口更新 ---> Settings ---注入器--->  工程代码实例`\n",
    "- `对于一个具体的 Settings  --- 调用 --> 实例化  --> 工程代码  就足够`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19d7a43",
   "metadata": {},
   "source": [
    "##### llama-index official-reranker-interface: class`SentenceTransformerRerank`\n",
    "```python\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022ce383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-Reranker-0.6B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "####  reranker  \n",
    "\n",
    "# official-api: \n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model=\"Qwen/Qwen3-Reranker-0.6B\",\n",
    "    top_n=3,\n",
    "    device=\"cpu\",\n",
    "    trust_remote_code=True,\n",
    "    keep_retrieval_score=False,\n",
    ")   \n",
    "# DEFAULT_SENTENCE_TRANSFORMER_MAX_LENGTH = 512\n",
    "\n",
    "\"\"\" \n",
    "# sbert_rerank.py\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "from llama_index.core.bridge.pydantic import Field, PrivateAttr\n",
    "from llama_index.core.callbacks import CBEventType, EventPayload\n",
    "from llama_index.core.postprocessor.types import BaseNodePostprocessor\n",
    "from llama_index.core.schema import MetadataMode, NodeWithScore, QueryBundle\n",
    "from llama_index.core.utils import infer_torch_device\n",
    "\n",
    "DEFAULT_SENTENCE_TRANSFORMER_MAX_LENGTH = 512\n",
    "\n",
    "\n",
    "class SentenceTransformerRerank(BaseNodePostprocessor):\n",
    "    model: str = Field(description=\"Sentence transformer model name.\")\n",
    "    top_n: int = Field(description=\"Number of nodes to return sorted by score.\")\n",
    "    device: str = Field(\n",
    "        default=\"cpu\",\n",
    "        description=\"Device to use for sentence transformer.\",\n",
    "    )\n",
    "    keep_retrieval_score: bool = Field(\n",
    "        default=False,\n",
    "        description=\"Whether to keep the retrieval score in metadata.\",\n",
    "    )\n",
    "    trust_remote_code: bool = Field(\n",
    "        default=False,\n",
    "        description=\"Whether to trust remote code.\",\n",
    "    )\n",
    "    _model: Any = PrivateAttr()\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        top_n: int = 2,\n",
    "        model: str = \"cross-encoder/stsb-distilroberta-base\",\n",
    "        device: Optional[str] = None,\n",
    "        keep_retrieval_score: bool = False,\n",
    "        trust_remote_code: bool = True,\n",
    "    ):\n",
    "        try:\n",
    "            from sentence_transformers import CrossEncoder  # pants: no-infer-dep\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Cannot import sentence-transformers or torch package,\",\n",
    "                \"please `pip install torch sentence-transformers`\",\n",
    "            )\n",
    "        device = infer_torch_device() if device is None else device\n",
    "        super().__init__(\n",
    "            top_n=top_n,\n",
    "            model=model,\n",
    "            device=device,\n",
    "            keep_retrieval_score=keep_retrieval_score,\n",
    "        )\n",
    "        self._model = CrossEncoder(\n",
    "            model,\n",
    "            max_length=DEFAULT_SENTENCE_TRANSFORMER_MAX_LENGTH,\n",
    "            device=device,\n",
    "            trust_remote_code=trust_remote_code,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        return \"SentenceTransformerRerank\"\n",
    "\n",
    "    def _postprocess_nodes(\n",
    "        self,\n",
    "        nodes: List[NodeWithScore],\n",
    "        query_bundle: Optional[QueryBundle] = None,\n",
    "    ) -> List[NodeWithScore]:\n",
    "        if query_bundle is None:\n",
    "            raise ValueError(\"Missing query bundle in extra info.\")\n",
    "        if len(nodes) == 0:\n",
    "            return []\n",
    "\n",
    "        query_and_nodes = [\n",
    "            (\n",
    "                query_bundle.query_str,\n",
    "                node.node.get_content(metadata_mode=MetadataMode.EMBED),\n",
    "            )\n",
    "            for node in nodes\n",
    "        ]\n",
    "\n",
    "        with self.callback_manager.event(\n",
    "            CBEventType.RERANKING,\n",
    "            payload={\n",
    "                EventPayload.NODES: nodes,\n",
    "                EventPayload.MODEL_NAME: self.model,\n",
    "                EventPayload.QUERY_STR: query_bundle.query_str,\n",
    "                EventPayload.TOP_K: self.top_n,\n",
    "            },\n",
    "        ) as event:\n",
    "            scores = self._model.predict(query_and_nodes)\n",
    "\n",
    "            assert len(scores) == len(nodes)\n",
    "\n",
    "            for node, score in zip(nodes, scores):\n",
    "                if self.keep_retrieval_score:\n",
    "                    # keep the retrieval score in metadata\n",
    "                    node.node.metadata[\"retrieval_score\"] = node.score\n",
    "                node.score = score\n",
    "\n",
    "            new_nodes = sorted(nodes, key=lambda x: -x.score if x.score else 0)[\n",
    "                : self.top_n\n",
    "            ]\n",
    "            event.on_end(payload={EventPayload.NODES: new_nodes})\n",
    "\n",
    "        return new_nodes\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"  \n",
    "# Reranker在什么时候有用？ \n",
    "- 常见做法是 先 用向量检索（bi-encoder embedding）拿到 top_k 候选（比如 50～200 条），再 用 cross-encoder 做 rerank，把前 top_n（比如 5～10）按相关性精排输出。\n",
    "- 为什么通常能提升效果：cross-encoder 每次把「查询 + 文本」一起喂给模型，做交互式编码，能捕捉词序、约束关系、否定词等细节，P@k 和 nDCG 往往显著提升，尤其是长句/多条件/歧义查询。\n",
    "- 什么时候“感觉不太有用”：\n",
    "    - similarity_top_k 太小（前段召回没把真相关召回来），reranker 再“精排”也白搭。\n",
    "    - 文本很短/关键词检索足够（如 FAQ 单句），embedding 已经很稳\n",
    "    - top_n 设太小 → 损失召回。一般建议：先召回 50～200，再 rerank 到 5～10。\n",
    "- 代价：cross-encoder 需要对每个候选“成对”打分，比单纯向量检索慢很多；长文本还受最大长度限制\n",
    "\n",
    "\n",
    "\n",
    "# 不同 reranker 实现“流派”\n",
    "- Pair-wise CrossEncoder（本类）：对每个 (q, d) 单独打分，稳定、常用。代表：BAAI/bge-reranker-*、jinaai/jina-reranker-* 等。\n",
    "- List-wise Reranker：一次看全体候选，建模文档间相对顺序，质量可更高，但接口不同、显存开销大。\n",
    "- LLM Reranker（生成式/打分式）：用小/中型 LLM 读 “查询+候选文本”，按模板让它“给分/排序”。在 LlamaIndex 里通常用 LLM 的 postprocessor（例如 LLMRerank、自定义 BaseNodePostprocessor），不是 SentenceTransformerRerank。\n",
    "- 托管 API：如 Cohere/Vectara 等，也有各自的 LlamaIndex 后处理器（CohereRerank 等）。\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278cffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import span_marker \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
