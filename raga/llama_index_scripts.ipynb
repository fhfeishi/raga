{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5632732",
   "metadata": {},
   "source": [
    "### RAG  llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b415a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file-parser\n",
    "from typing import List, Dict, Any   \n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm \n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings \n",
    "from llama_index.core.node_parser import SentenceSplitter, TokenTextSplitter, MarkdownNodeParser \n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.extractors import TitleExtractor   # 需要接入llm\n",
    "\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# embedding\n",
    "embedding = HuggingFaceEmbedding(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    device=\"cpu\",                 # 建议放顶层\n",
    "    cache_folder=r\"E:\\local_models\\huggingface\\cache\\hub\",\n",
    "    trust_remote_code=True,       # 建议放顶层\n",
    "    model_kwargs={\"local_files_only\": False},   # 允许联网 False\n",
    ")\n",
    "Settings.embed_model = embedding\n",
    "\n",
    "# full_split.md      figs_MetaDict.json  \n",
    "data_root = r\"../.log/SimplePDF\"\n",
    "mdfs: str = str(next(Path(data_root).rglob('full_split.md'), None))\n",
    "assert  Path(mdfs).exists()\n",
    "\n",
    "# 元数据 ++\n",
    "# 保存图片信息字段的json文件在mdfs的同级目录下 文件名 figs_MetaDict.json\n",
    "def load_jsf(mdp: Path) -> Dict[str, str | List]:\n",
    "    jsp = str(next(Path(mdp).parent.glob(\"figs_MetaDict.json\"),None))\n",
    "    if not jsp:\n",
    "        raise TypeError(\"jsp is NoneType\")\n",
    "    # assert jsp\n",
    "    \n",
    "    with open(jsp, \"r\", encoding='utf-8') as jf:\n",
    "        ims_metadata = json.load(jf)\n",
    "    return ims_metadata\n",
    "\n",
    "\"\"\" figs_metadict.json  \n",
    "{\n",
    "    \"im_abs\": [\n",
    "        \"摘要图\",\n",
    "        \"path/to/im_abs\"\n",
    "    ],\n",
    "    \"lines_ims\": [                    \n",
    "        \"图1为本实用新型的爆炸图\",\n",
    "        \"图2为本实用新型的侧面示图\",\n",
    "        ...\n",
    "    ],\n",
    "    \"annos_ims\": \"图中：1、驱动板；2、转子编码器芯片；...\",\n",
    "    \"im_1\": [\n",
    "        \"图1为本实用新型的爆炸图\",\n",
    "        \"path/to/im_1\"\n",
    "    ],\n",
    "    \"im_2\": [\n",
    "        \"图2为本实用新型的侧面示图\",\n",
    "        \"path/to/im_2\"\n",
    "    ],\n",
    "    ...\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# load_data\n",
    "documents = SimpleDirectoryReader(input_files=[mdfs]).load_data()\n",
    "# # figs_metadict  \n",
    "figs_dict = load_jsf(mdfs)   \n",
    "\n",
    "# spliter\n",
    "text_spliter = SentenceSplitter(chunk_size=700, chunk_overlap=100)\n",
    "\n",
    "# 节点后处理器  Postprocessor        \n",
    "# 相应合成器    Response Synthesizer    # todo.\n",
    "from llama_index.core.postprocessor import KeywordNodePostprocessor, LongContextReorder #  \n",
    "# pipeline \n",
    "pipeline = IngestionPipeline(transformations=[text_spliter])\n",
    "\n",
    "# nodes\n",
    "nodes = pipeline.run(documents=documents)\n",
    "\n",
    "# 给每个 node 记录 doc_id/figs_path，后面好取图，图的描述语也是需要一同提取出来的，图中的标记或许也需要展示/输出出来\n",
    "doc_id = Path(mdfs).parent.name\n",
    "for n in nodes:\n",
    "    n.metadata[\"doc_id\"] = doc_id\n",
    "    n.metadata[\"figs_path\"] = str(Path(mdfs).with_name(\"figs_MetaDict.json\"))\n",
    "\n",
    "\n",
    "# index\n",
    "nodes_idx = VectorStoreIndex(nodes=nodes)\n",
    "\n",
    "# 3) 纯检索：不走 LLM，只取 SourceNodes\n",
    "retriever = nodes_idx.as_retriever(similarity_top_k=3)\n",
    "query = \"介绍一下这是什么专利\"\n",
    "source_nodes = retriever.retrieve(query)\n",
    "\n",
    "# 4) 关联图片（最简单规则：chunk 文本里找“图N”）\n",
    "import re\n",
    "def pick_figs_for_text(text: str, figs: Dict[str, Any], top_k:int=2):\n",
    "    out = []\n",
    "    nums = [int(m.group(1)) for m in re.finditer(r'图\\s*(\\d+)', text)]\n",
    "    seen = set()\n",
    "    for n in nums:\n",
    "        key = f\"im_{n}\"\n",
    "        if key in figs and n not in seen:\n",
    "            cap, path = figs[key]\n",
    "            out.append({\"no\": n, \"caption\": cap, \"url\": path})\n",
    "            seen.add(n)\n",
    "            if len(out) >= top_k: break\n",
    "    # 补一个摘要图兜底\n",
    "    if len(out) == 0 and \"im_abs\" in figs:\n",
    "        cap, path = figs[\"im_abs\"]\n",
    "        out.append({\"no\": 0, \"caption\": cap, \"url\": path})\n",
    "    return out\n",
    "\n",
    "payload = []\n",
    "for sn in source_nodes:\n",
    "    figs = figs_dict  # 单文档就直接用；多文档可按 sn.node.metadata[\"doc_id\"] 选择\n",
    "    figs_pick = pick_figs_for_text(sn.node.get_text(), figs, top_k=2)\n",
    "    payload.append({\n",
    "        \"score\": sn.score,\n",
    "        \"text\": sn.node.get_text(),\n",
    "        \"figures\": figs_pick,\n",
    "        \"annos\": figs.get(\"annos_ims\", \"\")\n",
    "    })\n",
    "\n",
    "# 打印/返回给前端\n",
    "for i, item in enumerate(payload, 1):\n",
    "    print(f\"\\n=== 命中 {i} (score={item['score']:.3f}) ===\")\n",
    "    print(item[\"text\"], \"...\")\n",
    "    if item[\"figures\"]:\n",
    "        print(\"相关图：\", [f\"图{f['no']}\" for f in item[\"figures\"]])\n",
    "    if item[\"annos\"]:\n",
    "        print(\"附图标记：\", item[\"annos\"][:120], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483a2d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ims-info-dict   txts-str \n",
    "\n",
    "# ims-info-dict.keys(): im_abs lines_ims annos_ims im_n\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c45b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# agents.ipynb\n",
    "\"\"\"   \n",
    "#### AgentWorkflow的多智能体系统\n",
    "# AgentWorkflow 单个智能体、多智能体系统。 多智能体系统中多个智能体协作完成任务，并在需要时将控制权互相移交\n",
    "# - ResearchAgent  : 它将搜索网络一查找给定主题的信息\n",
    "# - WriteAgent     : 将用ResearchAgent检索到的信息来撰写报告\n",
    "# - ReviewAgent    : 将审查报告并提供反馈\n",
    "## 需要用到的工具\n",
    "# web_search工具，  Tavily\n",
    "# record_notes工具，将网络搜索道德研究保存到状态中（AgentWorkflow使用一个名为state的Context变量），然后其他工具就可以使用它\n",
    "# write_repot工具，使用ResearchAgent检索到的信息撰写报告\n",
    "# review_report工具，审查报告和提供反馈\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"   \n",
    "# HuggingFaceLLM 不支持函数调用/工具选择   许多本地小模型都不支持函数调用、工具支持，\n",
    "\n",
    "想用本地/离线模型，但又要多智能体与工具调用，有两条路：\n",
    "  1. 在本地模型上再套一层包装器（       -- 简单方法， \"智能体\"  模型自己决定是否使用工具/函数、然后本地机器调用工具/函数\n",
    "- 跑一个 OpenAI-兼容网关（如 vLLM + OpenAI 接口、LiteLLM 等），把本地模型“挂成” OpenAI-style API，再用 OpenAI 封装；\n",
    "  2. 模型只负责生成文本，不负责选工具   --复杂方法\n",
    "- 放弃“由模型自己决定何时调工具”，改为工作流显式编排：由你在 Python 中决定先搜→再记笔记→再写→再审（模型只负责生成文本，不负责选工具）。\n",
    "\"\"\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from llama_index.core.agent.workflow import AgentWorkflow \n",
    "from llama_index.core.workflow import Context \n",
    "from llama_index.core.agent.workflow import (\n",
    "    AgentOutput,\n",
    "    ToolCall,\n",
    "    ToolCallResult,\n",
    ")\n",
    "from llama_index.tools.tavily_research import TavilyToolSpec\n",
    "from llama_index.core.agent.workflow import FunctionAgent \n",
    "import os \n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM \n",
    "llm = HuggingFaceLLM(\n",
    "    model_name     = r\"Qwen/Qwen3-1.7B\",\n",
    "    tokenizer_name = r\"Qwen/Qwen3-1.7B\",\n",
    "    context_window = 3900,  \n",
    "    max_new_tokens = 640,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"top_k\": 30, \"top_p\": 0.95},\n",
    "    device_map     ='cpu' \n",
    ")\n",
    "# HuggingFaceLLM 不支持函数调用/工具选择\n",
    "\n",
    "tavily_tool = TavilyToolSpec(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "search_web = tavily_tool.to_tool_list()[0]\n",
    "\n",
    "# ---- 工具函数   \n",
    "async def record_notes(ctx: Context, notes: str, notes_title: str) -> str:\n",
    "    \"\"\" useful for recording notes on a gaven topic.\"\"\"\n",
    "    current_state = await ctx.store.get(\"state\")\n",
    "    if \"research_notes\" not in current_state:\n",
    "        current_state[\"research_notes\"] = {}\n",
    "    current_state[\"research_notes\"][notes_title] = notes\n",
    "    await ctx.store.set(\"state\", current_state)\n",
    "    return \"Notes recorded.\"\n",
    "\n",
    "async def write_report(ctx:Context, report_content:str) -> str:\n",
    "    \"\"\" useful write a report on a gaven topic.\"\"\"\n",
    "    current_state = ctx.store.get(\"state\")\n",
    "    current_state[\"report_content\"] = report_content\n",
    "    await ctx.store.set(\"state\", current_state)\n",
    "    return \"Report written\"\n",
    "    \n",
    "async def review_report(ctx:Context, review:str) -> str:\n",
    "    \"\"\"useful for reviewing a report and providing feedbacks\"\"\"\n",
    "    current_state = ctx.store.get(\"state\")\n",
    "    current_state[\"review\"] = review\n",
    "    await ctx.store.set([\"state\"],current_state)\n",
    "    return \"Report reviewed\" \n",
    "\n",
    "# ------------- agent\n",
    "research_agent = FunctionAgent(\n",
    "    name=\"ResearchAgent\",\n",
    "    description=\"Useful for searching the web for information on a given topic and recording notes on the topic.\",\n",
    "    system_prompt=(\n",
    "        \"You are the ResearchAgent that can search the web for information on a given topic and record notes on the topic. \"\n",
    "        \"Once notes are recorded and you are satisfied, you should hand off control to the WriteAgent to write a report on the topic.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[search_web, record_notes],\n",
    "    can_handoff_to=[\"WriteAgent\"],\n",
    ")\n",
    "\n",
    "write_agent = FunctionAgent(\n",
    "    name=\"WriteAgent\",\n",
    "    description=\"Useful for writing a report on a given topic.\",\n",
    "    system_prompt=(\n",
    "        \"You are the WriteAgent that can write a report on a given topic. \"\n",
    "        \"Your report should be in a markdown format. The content should be grounded in the research notes. \"\n",
    "        \"Once the report is written, you should get feedback at least once from the ReviewAgent.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[write_report],\n",
    "    can_handoff_to=[\"ReviewAgent\", \"ResearchAgent\"],\n",
    ")\n",
    "\n",
    "review_agent = FunctionAgent(\n",
    "    name=\"ReviewAgent\",\n",
    "    description=\"Useful for reviewing a report and providing feedback.\",\n",
    "    system_prompt=(\n",
    "        \"You are the ReviewAgent that can review a report and provide feedback. \"\n",
    "        \"Your feedback should either approve the current report or request changes for the WriteAgent to implement.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[review_report],\n",
    "    can_handoff_to=[\"WriteAgent\"],\n",
    ")\n",
    "\n",
    "agent_workflow = AgentWorkflow(\n",
    "    agents=[research_agent, write_agent, review_agent],\n",
    "    root_agent=research_agent.name,\n",
    "    initial_state={\n",
    "        \"research_notes\": {},\n",
    "        \"report_content\": \"Not written yet.\",\n",
    "        \"review\": \"Review required.\",\n",
    "    },\n",
    ")\n",
    "\n",
    "async def main():\n",
    "    handler = agent_workflow.run(user_msg=\"\"\"\n",
    "        Write me a report on the history of the web. Briefly describe the history \n",
    "        of the world wide web, including the development of the internet and the \n",
    "        development of the web, including 21st century developments.\n",
    "    \"\"\")\n",
    "    \n",
    "    current_agent = None \n",
    "    current_tool_calls = \"\" \n",
    "    async for event in handler.stream_events():\n",
    "        if (\n",
    "            hasattr(event, \"current_agent_name\")\n",
    "            and event.current_agent_name != current_agent\n",
    "        ):\n",
    "            current_agent = event.current_agent_name\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"🤖 Agent: {current_agent}\")\n",
    "            print(f\"{'='*50}\\n\")\n",
    "        elif isinstance(event, AgentOutput):\n",
    "            if event.response.content:\n",
    "                print(\"📤 Output:\", event.response.content)\n",
    "            if event.tool_calls:\n",
    "                print(\n",
    "                    \"🛠️  Planning to use tools:\",\n",
    "                    [call.tool_name for call in event.tool_calls],\n",
    "                )\n",
    "        elif isinstance(event, ToolCallResult):\n",
    "            print(f\"🔧 Tool Result ({event.tool_name}):\")\n",
    "            print(f\"  Arguments: {event.tool_kwargs}\")\n",
    "            print(f\"  Output: {event.tool_output}\")\n",
    "        elif isinstance(event, ToolCall):\n",
    "            print(f\"🔨 Calling Tool: {event.tool_name}\")\n",
    "            print(f\"  With arguments: {event.tool_kwargs}\")\n",
    "\n",
    "\"\"\"   .py\n",
    "if __name__ == '__main__':\n",
    "    import asyncio \n",
    "    asyncio.run(main())\n",
    "\"\"\"\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f0051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示工作流安排， 因为现在我的本地hf-llm不支持工具/函数调用（无function-calling）  -- runs ok  \n",
    "# ### 缺点： 中间信息复用的代码逻辑不好写，所以还是用llama-index（Context）\n",
    "# 本地hf-llm只生成文本（通过提示词控制 生成文本 这个行为），\n",
    "# 其余的我来\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os, json, re, textwrap, requests\n",
    "\n",
    "# 1) 本地/离线 LLM（不需要工具调用协议）\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name     = r\"Qwen/Qwen3-1.7B\",\n",
    "    tokenizer_name = r\"Qwen/Qwen3-1.7B\",\n",
    "    context_window = 3900,\n",
    "    max_new_tokens = 640,\n",
    "    generate_kwargs={\"temperature\": 0.5, \"top_k\": 30, \"top_p\": 0.95},\n",
    "    device_map     = \"cpu\",\n",
    ")\n",
    "\n",
    "# 2) Tavily 搜索（直接 REST；不依赖 LlamaIndex 的 Tool）\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "def tavily_search(query: str, max_results: int = 6, depth: str = \"advanced\"):\n",
    "    \"\"\"显式调用 Tavily 的 REST API。\"\"\"\n",
    "    url = \"https://api.tavily.com/search\"\n",
    "    payload = {\n",
    "        \"api_key\": TAVILY_API_KEY,\n",
    "        \"query\": query,\n",
    "        \"search_depth\": depth,           # \"basic\" / \"advanced\"\n",
    "        \"include_images\": False,\n",
    "        \"include_answer\": False,\n",
    "        \"max_results\": max_results,\n",
    "    }\n",
    "    r = requests.post(url, json=payload, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    # 期望 data[\"results\"] 是 [{title, url, content, score?}, ...]\n",
    "    return data.get(\"results\", [])\n",
    "\n",
    "def _trim(s: str, n=8000):\n",
    "    return s if len(s) <= n else (s[:n] + \" ...[truncated]\")\n",
    "\n",
    "# --------------------- 显式“工作流”步骤 ---------------------\n",
    "\n",
    "def step_research(topic: str) -> dict:\n",
    "    \"\"\"Step-1: 搜索 + 生成结构化笔记（显式调用 Tavily 与本地LLM）\"\"\"\n",
    "    results = tavily_search(topic, max_results=6, depth=\"advanced\")\n",
    "    sources_md = \"\\n\".join(\n",
    "        f\"- [{it.get('title','(no title)')}]({it.get('url','')}) — {it.get('content','')[:240].replace('\\n', ' ')}\"\n",
    "        for it in results\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "你是研究助理。基于下列检索到的资料，为“{topic}”生成不超过 10 条的要点式研究笔记（Markdown）。\n",
    "要求：\n",
    "- 覆盖关键时间线/里程碑/核心人物或机构\n",
    "- 突出 21 世纪的发展（若相关）\n",
    "- 尽量引用来源（以 [编号] 形式对照下方“资料列表”序号）\n",
    "\n",
    "资料列表（按顺序标号）：\n",
    "{sources_md}\n",
    "\n",
    "请输出：\n",
    "### 笔记\n",
    "1. ...\n",
    "2. ...\n",
    "3. ...\n",
    "### 引用来源\n",
    "- [1] 标题（链接）\n",
    "- [2] ...\n",
    "\"\"\"\n",
    "    notes = llm.complete(prompt).text\n",
    "    state = {\n",
    "        \"topic\": topic,\n",
    "        \"search_results\": results,\n",
    "        \"notes_md\": notes,\n",
    "        \"sources_md\": sources_md,\n",
    "    }\n",
    "    return state\n",
    "\n",
    "def step_write_report(state: dict) -> dict:\n",
    "    \"\"\"Step-2: 写报告（显式把上一步的笔记传入 LLM）\"\"\"\n",
    "    topic = state[\"topic\"]\n",
    "    notes_md = state[\"notes_md\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "你是一名技术写作者。请基于下方“研究笔记”为主题“{topic}”撰写一篇**结构化 Markdown 报告**。\n",
    "要求：\n",
    "- 结构示例：# 概览 / ## 早期发展 / ## 1990s / ## 2000s / ## 2010s-2020s / ## 参考资料\n",
    "- 内容必须**紧密依赖**研究笔记，不要编造\n",
    "- 语言简洁，段落短小\n",
    "\n",
    "--- 研究笔记 ---\n",
    "{notes_md}\n",
    "\"\"\"\n",
    "    report_md = llm.complete(prompt).text\n",
    "    state[\"report_md\"] = report_md\n",
    "    return state\n",
    "\n",
    "def _extract_json_block(text: str) -> dict:\n",
    "    \"\"\"从模型输出中尽量提取 JSON（稳健解析）\"\"\"\n",
    "    # 先找 ```json ... ``` 包裹\n",
    "    m = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", text, flags=re.S)\n",
    "    if not m:\n",
    "        # 退化：找第一个 { ... } 块\n",
    "        m = re.search(r\"(\\{.*\\})\", text, flags=re.S)\n",
    "    if not m:\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(m.group(1))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def step_review(state: dict, strict: bool = False) -> dict:\n",
    "    \"\"\"Step-3: 审稿（输出 JSON：approved/changes）\"\"\"\n",
    "    report_md = state[\"report_md\"]\n",
    "    prompt = f\"\"\"\n",
    "你是审稿人。请仅以 JSON 方式给出审稿结论。\n",
    "规则：\n",
    "- 字段：approved (bool), summary (string), changes (string[])\n",
    "- 若 approved=false，请给出 3-6 条具体修改建议\n",
    "- 输出必须放在一个 ```json 块中，不要出现多余文字\n",
    "\n",
    "--- 待审报告 ---\n",
    "{_trim(report_md, 7000)}\n",
    "\"\"\"\n",
    "    review_raw = llm.complete(prompt).text\n",
    "    parsed = _extract_json_block(review_raw)\n",
    "    # 容错：默认通过\n",
    "    approved = bool(parsed.get(\"approved\", True))\n",
    "    changes = parsed.get(\"changes\", [])\n",
    "    summary = parsed.get(\"summary\", \"OK\")\n",
    "\n",
    "    state[\"review\"] = {\n",
    "        \"raw\": review_raw,\n",
    "        \"parsed\": parsed,\n",
    "        \"approved\": approved,\n",
    "        \"changes\": changes,\n",
    "        \"summary\": summary,\n",
    "    }\n",
    "    return state\n",
    "\n",
    "def step_revise_if_needed(state: dict, max_rounds: int = 1) -> dict:\n",
    "    \"\"\"可选：根据审稿意见进行 0~N 轮修订\"\"\"\n",
    "    rounds = 0\n",
    "    while rounds < max_rounds and not state[\"review\"][\"approved\"]:\n",
    "        changes = state[\"review\"][\"changes\"]\n",
    "        report_md = state[\"report_md\"]\n",
    "        topic = state[\"topic\"]\n",
    "        notes_md = state[\"notes_md\"]\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "根据以下审稿意见修订报告《{topic}》。必须遵循审稿条目逐条修改：\n",
    "- 审稿意见：\n",
    "{json.dumps(changes, ensure_ascii=False, indent=2)}\n",
    "\n",
    "限制：\n",
    "- 仍需严格依赖“研究笔记”，不要编造\n",
    "- 保持 Markdown 结构清晰\n",
    "\n",
    "--- 研究笔记 ---\n",
    "{notes_md}\n",
    "\n",
    "--- 旧版报告 ---\n",
    "{_trim(report_md, 7000)}\n",
    "\"\"\"\n",
    "        new_report = llm.complete(prompt).text\n",
    "        state[\"report_md\"] = new_report\n",
    "        # 重新审稿\n",
    "        state = step_review(state)\n",
    "        rounds += 1\n",
    "    return state\n",
    "\n",
    "# --------------------- 顶层 orchestrator ---------------------\n",
    "\n",
    "def run_explicit_workflow(user_request: str, revise_rounds: int = 1) -> dict:\n",
    "    # 1) 研究\n",
    "    state = step_research(user_request)\n",
    "    print(\"✅ Research done.\")\n",
    "\n",
    "    # 2) 写作\n",
    "    state = step_write_report(state)\n",
    "    print(\"✅ Draft written.\")\n",
    "\n",
    "    # 3) 审稿\n",
    "    state = step_review(state)\n",
    "    print(f\"✅ Review: approved={state['review']['approved']}\")\n",
    "\n",
    "    # 4) 需要的话，修订若干轮\n",
    "    state = step_revise_if_needed(state, max_rounds=revise_rounds)\n",
    "    print(f\"✅ Final approved={state['review']['approved']}\")\n",
    "\n",
    "    return state\n",
    "\n",
    "# --------------------- 运行示例 ---------------------\n",
    "if __name__ == \"__main__\":\n",
    "    topic = \"History of the World Wide Web and key 21st-century developments\"\n",
    "    final_state = run_explicit_workflow(topic, revise_rounds=1)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"# 最终报告（截断展示）\\n\")\n",
    "    print(_trim(final_state[\"report_md\"], 4000))\n",
    "    print(\"\\n# 审稿解析：\", final_state[\"review\"][\"parsed\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用 LlamaIndex 的 Workflow（仍是显式路径，不让模型挑工具）\n",
    "# explicit_workflow_llamaindex.py  —— 显式编排，无 function-calling\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os, json, re, requests\n",
    "from typing import Union\n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.workflow import (\n",
    "    Workflow, StartEvent, StopEvent, step, Event\n",
    ")\n",
    "\n",
    "# ================== 基础组件 ==================\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name     = r\"Qwen/Qwen3-1.7B\",\n",
    "    tokenizer_name = r\"Qwen/Qwen3-1.7B\",\n",
    "    context_window = 3900,\n",
    "    max_new_tokens = 640,\n",
    "    generate_kwargs={\"temperature\": 0.5, \"top_k\": 30, \"top_p\": 0.95},\n",
    "    device_map     = \"cpu\",\n",
    ")\n",
    "\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "def tavily_search(query: str, max_results: int = 6):\n",
    "    url = \"https://api.tavily.com/search\"\n",
    "    payload = {\n",
    "        \"api_key\": TAVILY_API_KEY,\n",
    "        \"query\": query,\n",
    "        \"search_depth\": \"advanced\",\n",
    "        \"max_results\": max_results\n",
    "    }\n",
    "    resp = requests.post(url, json=payload, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json().get(\"results\", [])\n",
    "\n",
    "def _extract_json_block(text: str) -> dict:\n",
    "    m = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", text, flags=re.S) or re.search(r\"(\\{.*\\})\", text, flags=re.S)\n",
    "    if not m:\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(m.group(1))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "# ================== 事件类型（显式定义） ==================\n",
    "class ResearchDone(Event):\n",
    "    topic: str\n",
    "    sources_md: str\n",
    "    notes_md: str\n",
    "\n",
    "class DraftWritten(Event):\n",
    "    topic: str\n",
    "    notes_md: str\n",
    "    report_md: str\n",
    "\n",
    "class ReviewApproved(Event):\n",
    "    topic: str\n",
    "    notes_md: str\n",
    "    report_md: str\n",
    "    review: dict  # {\"approved\": True, \"summary\": \"...\", \"changes\": []}\n",
    "\n",
    "class ReviewChangesNeeded(Event):\n",
    "    topic: str\n",
    "    notes_md: str\n",
    "    report_md: str\n",
    "    review: dict  # {\"approved\": False, \"summary\": \"...\", \"changes\": [...]}\n",
    "\n",
    "# ================== Workflow & Steps ==================\n",
    "class wf(Workflow):\n",
    "    # 因为是从前往后，单线， 所以不用Context也可以\n",
    "    @step\n",
    "    def research(ev: StartEvent) -> ResearchDone:\n",
    "        topic = ev.input[\"topic\"]\n",
    "        results = tavily_search(topic, max_results=6)\n",
    "        sources_md = \"\\n\".join(\n",
    "            f\"- [{it.get('title','(no title)')}]({it.get('url','')}) — {it.get('content','')[:240].replace('\\n',' ')}\"\n",
    "            for it in results\n",
    "        )\n",
    "        prompt = f\"请基于下列资料为“{topic}”生成 6-10 条要点式 Markdown 笔记：\\n{sources_md}\"\n",
    "        notes_md = llm.complete(prompt).text\n",
    "        return ResearchDone(topic=topic, sources_md=sources_md, notes_md=notes_md)\n",
    "\n",
    "    @step\n",
    "    def write(ev: ResearchDone) -> DraftWritten:\n",
    "        topic, notes_md = ev.topic, ev.notes_md\n",
    "        prompt = f\"基于研究笔记为“{topic}”写一篇结构化 Markdown 报告：\\n{notes_md}\"\n",
    "        report_md = llm.complete(prompt).text\n",
    "        return DraftWritten(topic=topic, notes_md=notes_md, report_md=report_md)\n",
    "\n",
    "    @step\n",
    "    def review(ev: DraftWritten) -> Union[ReviewApproved, ReviewChangesNeeded]:\n",
    "        report_md = ev.report_md\n",
    "        prompt = f\"\"\"你是审稿人，仅以 JSON 回答：{{\"approved\": true/false, \"summary\": \"...\", \"changes\": [\"...\"]}}。\n",
    "    报告：\n",
    "    {report_md}\n",
    "    \"\"\"\n",
    "        parsed = _extract_json_block(llm.complete(prompt).text)\n",
    "        if not parsed:\n",
    "            parsed = {\"approved\": True, \"summary\": \"OK\", \"changes\": []}\n",
    "\n",
    "        if bool(parsed.get(\"approved\", True)):\n",
    "            return ReviewApproved(\n",
    "                topic=ev.topic, notes_md=ev.notes_md, report_md=report_md, review=parsed\n",
    "            )\n",
    "        else:\n",
    "            return ReviewChangesNeeded(\n",
    "                topic=ev.topic, notes_md=ev.notes_md, report_md=report_md, review=parsed\n",
    "            )\n",
    "\n",
    "    @step\n",
    "    def revise(ev: ReviewChangesNeeded) -> DraftWritten:\n",
    "        \"\"\"按审稿意见修订一轮后，回到 review。\"\"\"\n",
    "        changes = ev.review.get(\"changes\", [])\n",
    "        prompt = (\n",
    "            \"根据以下审稿意见修订报告（保持 Markdown 结构，逐条落实）：\\n\"\n",
    "            f\"{json.dumps(changes, ensure_ascii=False, indent=2)}\\n\"\n",
    "            \"---\\n旧版：\\n\"\n",
    "            f\"{ev.report_md}\"\n",
    "        )\n",
    "        new_report = llm.complete(prompt).text\n",
    "        return DraftWritten(topic=ev.topic, notes_md=ev.notes_md, report_md=new_report)\n",
    "\n",
    "    @step\n",
    "    def end(ev: ReviewApproved) -> StopEvent:\n",
    "        \"\"\"终止：返回最终报告与审稿结果。\"\"\"\n",
    "        return StopEvent(result={\"report_md\": ev.report_md, \"review\": ev.review})\n",
    "\n",
    "\n",
    "# =============== 运行与打印 ===============\n",
    "handler = wf.run(input={\"topic\": \"History of the World Wide Web and 21st-century developments\"})\n",
    "\n",
    "# 可选：打印事件流\n",
    "for ev in handler.stream_events():\n",
    "    name = type(ev).__name__\n",
    "    payload = getattr(ev, \"result\", None)\n",
    "    if payload:\n",
    "        print(f\"[{name}] result keys: {list(payload) if isinstance(payload, dict) else str(payload)[:60]}\")\n",
    "    else:\n",
    "        print(f\"[{name}]\")\n",
    "\n",
    "final = handler.get()  # StopEvent.result\n",
    "print(\"\\n=== FINAL REPORT (snippet) ===\\n\")\n",
    "print(final[\"report_md\"][:2000])\n",
    "print(\"\\n=== REVIEW ===\\n\", json.dumps(final[\"review\"], ensure_ascii=False, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea779dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file \n",
    "\n",
    "from pathlib import Path \n",
    "\n",
    "pdf_root = r\"./log/simplePDF\"    # ~root/subdir/ ..data\n",
    "imPDF_root = r\"./log/ImagePDF\"   # ~root/subdir/ ..data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031680bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd70887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "project_root = Path(__file__).parent.parent\n",
    "file_md = r\"\"\n",
    "file_pdf = r\"\"\n",
    "\n",
    "\n",
    "file_md_im = r\"\"\n",
    "file_pdf_im = r\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15b7e3",
   "metadata": {},
   "source": [
    "### 部署  llama_deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2254e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
