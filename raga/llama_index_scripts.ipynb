{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5632732",
   "metadata": {},
   "source": [
    "### RAG  llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b415a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 命中 1 (score=0.423) ===\n",
      "# (19)中华人民共和国国家知识产权局\n",
      "\n",
      "# (12)实用新型专利\n",
      "\n",
      "(10)授权公告号CN207225508U(45)授权公告日2018.04.13\n",
      "\n",
      "(21)申请号201721328994.5\n",
      "\n",
      "(22)申请日2017.10.16\n",
      "\n",
      "(73)专利权人杭州宇树科技有限公司地址310051浙江省杭州市滨江区聚业路26号金绣国际科技中心B座106室\n",
      "\n",
      "(72)发明人王兴兴 杨知雨\n",
      "\n",
      "(74)专利代理机构浙江翔隆专利事务所（普通合伙）33206\n",
      "\n",
      "代理人许守金\n",
      "\n",
      "(51)Int.Cl. ...\n",
      "相关图： ['图0']\n",
      "附图标记： 附图标记说明：1：保护罩总成，2：足基座总成，3：足垫，2-1：传感部件，2-2：应变片，2-3：发光件，2-4：测控单元，2-5：足基座，2-1- 1：敏感部，2-1- 1-1：平行平面区域 ...\n",
      "\n",
      "=== 命中 2 (score=0.304) ===\n",
      "B62D57/032(2006.01) G01L1/18(2006.01) G01C21/10(2006.01)\n",
      "\n",
      "权利要求书1页 说明书4页 附图4页\n",
      "\n",
      "# (54)实用新型名称\n",
      "\n",
      "一种机器人足端结构\n",
      "\n",
      "# (57)摘要\n",
      "\n",
      "本实用新型公开了一种机器人足端结构，属于机器人设备技术领域。现有技术的足端结构较复杂，体积及重量大，内置的力传感器可靠性低。一种机器人足端结构，包括用于和机器人腿部连杆相连接的足基座总成、用于缓冲传递冲击力的足垫和保护罩总成，所述足基座总成内的传感部件上设有能产生较大变形的敏感部，所述敏感部由高屈服强度材料制成，所述敏感部表面粘贴有应变桥；所述足垫传递足受到的支撑力到足基座总成，使敏感部产生较大变形，从而通过应变桥检测到足端支撑力的大小。本实用新型集力传感器与足端为一体，零部件少，结构简单，能够有效减轻机器人足端的重量与体积，便于生产制造，力传感器过载能力强不易失效，制造成本低。\n",
      "\n",
      "\n",
      "1. ...\n",
      "相关图： ['图4']\n",
      "附图标记： 附图标记说明：1：保护罩总成，2：足基座总成，3：足垫，2-1：传感部件，2-2：应变片，2-3：发光件，2-4：测控单元，2-5：足基座，2-1- 1：敏感部，2-1- 1-1：平行平面区域 ...\n",
      "\n",
      "=== 命中 3 (score=0.296) ===\n",
      "[0016] 进一步，本实用新型的应变片黏贴于敏感部上，能够准确感应传感部件的形变位移，把支撑力的大小转化为自身电阻变化的大小，进而实现对足端受力的测量并且测量精度高。\n",
      "\n",
      "# 具体实施方式\n",
      "\n",
      "[0025] 为了使本实用新型的目的、技术方案及优点更加清楚明白，以下结合附图及实施例，对本实用新型进行进一步详细说明。应当理解，此处所描述的具体实施例仅仅用以解释本实用新型，并不用于限定本实用新型。\n",
      "\n",
      "[0026] 相反，本实用新型涵盖任何由权利要求定义的在本实用新型的精髓和范围上做的替代、修改、等效方法以及方案。进一步，为了使公众对本实用新型有更好的了解，在下文对本实用新型的细节描述中，详尽描述了一些特定的细节部分。对本领域技术人员来说没有这些细节部分的描述也可以完全理解本实用新型。\n",
      "\n",
      "[0027] 需要说明的是，当元件被称为“固定于”另一个元件，它可以直接在另一个元件上或者也可以存在居中的元件。当一个元件被认为是“连接”另一个元件，它可以是直接连接到另一个元件或者可能同时存在居中元件。相反，当元件被称作“直接在”另一元件“上”时，不存在中间元件。本文所使用的术语“上”、“下”以及类似的表述只是为了说明的目的。\n",
      "\n",
      "[0028] 除非另有定义，本文所使用的所有技术和科学术语与属于本实用新型的技术领域的技术人员通常理解的含义相同。本文所使用的术语只是为了描述具体的实施例的目的，不是旨在限制本实用新型。 ...\n",
      "相关图： ['图0']\n",
      "附图标记： 附图标记说明：1：保护罩总成，2：足基座总成，3：足垫，2-1：传感部件，2-2：应变片，2-3：发光件，2-4：测控单元，2-5：足基座，2-1- 1：敏感部，2-1- 1-1：平行平面区域 ...\n"
     ]
    }
   ],
   "source": [
    "# file-parser\n",
    "from typing import List, Dict, Any   \n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm \n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings \n",
    "from llama_index.core.node_parser import SentenceSplitter, TokenTextSplitter, MarkdownNodeParser \n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.extractors import TitleExtractor   # 需要接入llm\n",
    "\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# embedding\n",
    "embedding = HuggingFaceEmbedding(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    device=\"cpu\",                 # 建议放顶层\n",
    "    cache_folder=r\"E:\\local_models\\huggingface\\cache\\hub\",\n",
    "    trust_remote_code=True,       # 建议放顶层\n",
    "    model_kwargs={\"local_files_only\": False},   # 允许联网 False\n",
    ")\n",
    "Settings.embed_model = embedding\n",
    "\n",
    "# full_split.md      figs_MetaDict.json  \n",
    "data_root = r\"../.log/SimplePDF\"\n",
    "mdfs: str = str(next(Path(data_root).rglob('full_split.md'), None))\n",
    "assert  Path(mdfs).exists()\n",
    "\n",
    "# 元数据 ++\n",
    "# 保存图片信息字段的json文件在mdfs的同级目录下 文件名 figs_MetaDict.json\n",
    "def load_jsf(mdp: Path) -> Dict[str, str | List]:\n",
    "    jsp = str(next(Path(mdp).parent.glob(\"figs_MetaDict.json\"),None))\n",
    "    if not jsp:\n",
    "        raise TypeError(\"jsp is NoneType\")\n",
    "    # assert jsp\n",
    "    \n",
    "    with open(jsp, \"r\", encoding='utf-8') as jf:\n",
    "        ims_metadata = json.load(jf)\n",
    "    return ims_metadata\n",
    "\n",
    "\"\"\" figs_metadict.json  \n",
    "{\n",
    "    \"im_abs\": [\n",
    "        \"摘要图\",\n",
    "        \"path/to/im_abs\"\n",
    "    ],\n",
    "    \"lines_ims\": [                    \n",
    "        \"图1为本实用新型的爆炸图\",\n",
    "        \"图2为本实用新型的侧面示图\",\n",
    "        ...\n",
    "    ],\n",
    "    \"annos_ims\": \"图中：1、驱动板；2、转子编码器芯片；...\",\n",
    "    \"im_1\": [\n",
    "        \"图1为本实用新型的爆炸图\",\n",
    "        \"path/to/im_1\"\n",
    "    ],\n",
    "    \"im_2\": [\n",
    "        \"图2为本实用新型的侧面示图\",\n",
    "        \"path/to/im_2\"\n",
    "    ],\n",
    "    ...\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# load_data\n",
    "documents = SimpleDirectoryReader(input_files=[mdfs]).load_data()\n",
    "# # figs_metadict  \n",
    "figs_dict = load_jsf(mdfs)   \n",
    "\n",
    "# spliter\n",
    "text_spliter = SentenceSplitter(chunk_size=700, chunk_overlap=100)\n",
    "\n",
    "# 节点后处理器  Postprocessor        \n",
    "# 相应合成器    Response Synthesizer    # todo.\n",
    "from llama_index.core.postprocessor import KeywordNodePostprocessor, LongContextReorder #  \n",
    "# pipeline \n",
    "pipeline = IngestionPipeline(transformations=[text_spliter])\n",
    "\n",
    "# nodes\n",
    "nodes = pipeline.run(documents=documents)\n",
    "\n",
    "# 给每个 node 记录 doc_id/figs_path，后面好取图，图的描述语也是需要一同提取出来的，图中的标记或许也需要展示/输出出来\n",
    "doc_id = Path(mdfs).parent.name\n",
    "for n in nodes:\n",
    "    n.metadata[\"doc_id\"] = doc_id\n",
    "    n.metadata[\"figs_path\"] = str(Path(mdfs).with_name(\"figs_MetaDict.json\"))\n",
    "\n",
    "\n",
    "# index\n",
    "nodes_idx = VectorStoreIndex(nodes=nodes)\n",
    "\n",
    "# 3) 纯检索：不走 LLM，只取 SourceNodes\n",
    "retriever = nodes_idx.as_retriever(similarity_top_k=3)\n",
    "query = \"介绍一下这是什么专利\"\n",
    "source_nodes = retriever.retrieve(query)\n",
    "\n",
    "# 4) 关联图片（最简单规则：chunk 文本里找“图N”）\n",
    "import re\n",
    "def pick_figs_for_text(text: str, figs: Dict[str, Any], top_k:int=2):\n",
    "    out = []\n",
    "    nums = [int(m.group(1)) for m in re.finditer(r'图\\s*(\\d+)', text)]\n",
    "    seen = set()\n",
    "    for n in nums:\n",
    "        key = f\"im_{n}\"\n",
    "        if key in figs and n not in seen:\n",
    "            cap, path = figs[key]\n",
    "            out.append({\"no\": n, \"caption\": cap, \"url\": path})\n",
    "            seen.add(n)\n",
    "            if len(out) >= top_k: break\n",
    "    # 补一个摘要图兜底\n",
    "    if len(out) == 0 and \"im_abs\" in figs:\n",
    "        cap, path = figs[\"im_abs\"]\n",
    "        out.append({\"no\": 0, \"caption\": cap, \"url\": path})\n",
    "    return out\n",
    "\n",
    "payload = []\n",
    "for sn in source_nodes:\n",
    "    figs = figs_dict  # 单文档就直接用；多文档可按 sn.node.metadata[\"doc_id\"] 选择\n",
    "    figs_pick = pick_figs_for_text(sn.node.get_text(), figs, top_k=2)\n",
    "    payload.append({\n",
    "        \"score\": sn.score,\n",
    "        \"text\": sn.node.get_text(),\n",
    "        \"figures\": figs_pick,\n",
    "        \"annos\": figs.get(\"annos_ims\", \"\")\n",
    "    })\n",
    "\n",
    "# 打印/返回给前端\n",
    "for i, item in enumerate(payload, 1):\n",
    "    print(f\"\\n=== 命中 {i} (score={item['score']:.3f}) ===\")\n",
    "    print(item[\"text\"], \"...\")\n",
    "    if item[\"figures\"]:\n",
    "        print(\"相关图：\", [f\"图{f['no']}\" for f in item[\"figures\"]])\n",
    "    if item[\"annos\"]:\n",
    "        print(\"附图标记：\", item[\"annos\"][:120], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483a2d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ims-info-dict   txts-str \n",
    "\n",
    "# ims-info-dict.keys(): im_abs lines_ims annos_ims im_n\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c45b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0c1240f00b48f3a24fea03d8c6f58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# agents.ipynb\n",
    "\"\"\"   \n",
    "#### AgentWorkflow的多智能体系统\n",
    "# AgentWorkflow 单个智能体、多智能体系统。 多智能体系统中多个智能体协作完成任务，并在需要时将控制权互相移交\n",
    "# - ResearchAgent  : 它将搜索网络一查找给定主题的信息\n",
    "# - WriteAgent     : 将用ResearchAgent检索到的信息来撰写报告\n",
    "# - ReviewAgent    : 将审查报告并提供反馈\n",
    "## 需要用到的工具\n",
    "# web_search工具，  Tavily\n",
    "# record_notes工具，将网络搜索道德研究保存到状态中（AgentWorkflow使用一个名为state的Context变量），然后其他工具就可以使用它\n",
    "# write_repot工具，使用ResearchAgent检索到的信息撰写报告\n",
    "# review_report工具，审查报告和提供反馈\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"   \n",
    "# HuggingFaceLLM 不支持函数调用/工具选择   许多本地小模型都不支持函数调用、工具支持，\n",
    "\n",
    "想用本地/离线模型，但又要多智能体与工具调用，有两条路：\n",
    "  1. 在本地模型上再套一层包装器（       -- 简单方法， \"智能体\"  模型自己决定是否使用工具/函数、然后本地机器调用工具/函数\n",
    "- 跑一个 OpenAI-兼容网关（如 vLLM + OpenAI 接口、LiteLLM 等），把本地模型“挂成” OpenAI-style API，再用 OpenAI 封装；\n",
    "  2. 模型只负责生成文本，不负责选工具   --复杂方法\n",
    "- 放弃“由模型自己决定何时调工具”，改为工作流显式编排：由你在 Python 中决定先搜→再记笔记→再写→再审（模型只负责生成文本，不负责选工具）。\n",
    "\"\"\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from llama_index.core.agent.workflow import AgentWorkflow \n",
    "from llama_index.core.workflow import Context \n",
    "from llama_index.core.agent.workflow import (\n",
    "    AgentOutput,\n",
    "    ToolCall,\n",
    "    ToolCallResult,\n",
    ")\n",
    "from llama_index.tools.tavily_research import TavilyToolSpec\n",
    "from llama_index.core.agent.workflow import FunctionAgent \n",
    "import os \n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM \n",
    "llm = HuggingFaceLLM(\n",
    "    model_name     = r\"Qwen/Qwen3-1.7B\",\n",
    "    tokenizer_name = r\"Qwen/Qwen3-1.7B\",\n",
    "    context_window = 3900,  \n",
    "    max_new_tokens = 640,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"top_k\": 30, \"top_p\": 0.95},\n",
    "    device_map     ='cpu' \n",
    ")\n",
    "# HuggingFaceLLM 不支持函数调用/工具选择\n",
    "\n",
    "tavily_tool = TavilyToolSpec(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "search_web = tavily_tool.to_tool_list()[0]\n",
    "\n",
    "# ---- 工具函数   \n",
    "async def record_notes(ctx: Context, notes: str, notes_title: str) -> str:\n",
    "    \"\"\" useful for recording notes on a gaven topic.\"\"\"\n",
    "    current_state = await ctx.store.get(\"state\")\n",
    "    if \"research_notes\" not in current_state:\n",
    "        current_state[\"research_notes\"] = {}\n",
    "    current_state[\"research_notes\"][notes_title] = notes\n",
    "    await ctx.store.set(\"state\", current_state)\n",
    "    return \"Notes recorded.\"\n",
    "\n",
    "async def write_report(ctx:Context, report_content:str) -> str:\n",
    "    \"\"\" useful write a report on a gaven topic.\"\"\"\n",
    "    current_state = ctx.store.get(\"state\")\n",
    "    current_state[\"report_content\"] = report_content\n",
    "    await ctx.store.set(\"state\", current_state)\n",
    "    return \"Report written\"\n",
    "    \n",
    "async def review_report(ctx:Context, review:str) -> str:\n",
    "    \"\"\"useful for reviewing a report and providing feedbacks\"\"\"\n",
    "    current_state = ctx.store.get(\"state\")\n",
    "    current_state[\"review\"] = review\n",
    "    await ctx.store.set([\"state\"],current_state)\n",
    "    return \"Report reviewed\" \n",
    "\n",
    "# ------------- agent\n",
    "research_agent = FunctionAgent(\n",
    "    name=\"ResearchAgent\",\n",
    "    description=\"Useful for searching the web for information on a given topic and recording notes on the topic.\",\n",
    "    system_prompt=(\n",
    "        \"You are the ResearchAgent that can search the web for information on a given topic and record notes on the topic. \"\n",
    "        \"Once notes are recorded and you are satisfied, you should hand off control to the WriteAgent to write a report on the topic.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[search_web, record_notes],\n",
    "    can_handoff_to=[\"WriteAgent\"],\n",
    ")\n",
    "\n",
    "write_agent = FunctionAgent(\n",
    "    name=\"WriteAgent\",\n",
    "    description=\"Useful for writing a report on a given topic.\",\n",
    "    system_prompt=(\n",
    "        \"You are the WriteAgent that can write a report on a given topic. \"\n",
    "        \"Your report should be in a markdown format. The content should be grounded in the research notes. \"\n",
    "        \"Once the report is written, you should get feedback at least once from the ReviewAgent.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[write_report],\n",
    "    can_handoff_to=[\"ReviewAgent\", \"ResearchAgent\"],\n",
    ")\n",
    "\n",
    "review_agent = FunctionAgent(\n",
    "    name=\"ReviewAgent\",\n",
    "    description=\"Useful for reviewing a report and providing feedback.\",\n",
    "    system_prompt=(\n",
    "        \"You are the ReviewAgent that can review a report and provide feedback. \"\n",
    "        \"Your feedback should either approve the current report or request changes for the WriteAgent to implement.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[review_report],\n",
    "    can_handoff_to=[\"WriteAgent\"],\n",
    ")\n",
    "\n",
    "agent_workflow = AgentWorkflow(\n",
    "    agents=[research_agent, write_agent, review_agent],\n",
    "    root_agent=research_agent.name,\n",
    "    initial_state={\n",
    "        \"research_notes\": {},\n",
    "        \"report_content\": \"Not written yet.\",\n",
    "        \"review\": \"Review required.\",\n",
    "    },\n",
    ")\n",
    "\n",
    "async def main():\n",
    "    handler = agent_workflow.run(user_msg=\"\"\"\n",
    "        Write me a report on the history of the web. Briefly describe the history \n",
    "        of the world wide web, including the development of the internet and the \n",
    "        development of the web, including 21st century developments.\n",
    "    \"\"\")\n",
    "    \n",
    "    current_agent = None \n",
    "    current_tool_calls = \"\" \n",
    "    async for event in handler.stream_events():\n",
    "        if (\n",
    "            hasattr(event, \"current_agent_name\")\n",
    "            and event.current_agent_name != current_agent\n",
    "        ):\n",
    "            current_agent = event.current_agent_name\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"🤖 Agent: {current_agent}\")\n",
    "            print(f\"{'='*50}\\n\")\n",
    "        elif isinstance(event, AgentOutput):\n",
    "            if event.response.content:\n",
    "                print(\"📤 Output:\", event.response.content)\n",
    "            if event.tool_calls:\n",
    "                print(\n",
    "                    \"🛠️  Planning to use tools:\",\n",
    "                    [call.tool_name for call in event.tool_calls],\n",
    "                )\n",
    "        elif isinstance(event, ToolCallResult):\n",
    "            print(f\"🔧 Tool Result ({event.tool_name}):\")\n",
    "            print(f\"  Arguments: {event.tool_kwargs}\")\n",
    "            print(f\"  Output: {event.tool_output}\")\n",
    "        elif isinstance(event, ToolCall):\n",
    "            print(f\"🔨 Calling Tool: {event.tool_name}\")\n",
    "            print(f\"  With arguments: {event.tool_kwargs}\")\n",
    "\n",
    "\"\"\"   .py\n",
    "if __name__ == '__main__':\n",
    "    import asyncio \n",
    "    asyncio.run(main())\n",
    "\"\"\"\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f0051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3319b4baaa30474e81ebdbeff9eb7e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Research done.\n",
      "✅ Draft written.\n",
      "✅ Review: approved=False\n",
      "✅ Final approved=False\n",
      "\n",
      "================================================================================\n",
      "# 最终报告（截断展示）\n",
      "\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合要求，不包含任何额外信息。\n",
      "### 请确保输出的每个要点都符合\n",
      "\n",
      "# 审稿解析： {'approved': False, 'summary': 'The manuscript requires significant revisions to enhance clarity, depth of analysis, and experimental details.', 'changes': ['Add detailed experimental procedures for the key methodologies used in the study.', 'Improve the statistical analysis by including more robust methods and reporting effect sizes.', 'Expand the discussion section to address potential limitations and alternative interpretations of the results.', 'Clarify the significance of the findings in the context of existing literature.', 'Refine the figure legends and add additional figures to support the main conclusions.']}\n"
     ]
    }
   ],
   "source": [
    "# 显示工作流安排， 因为现在我的本地hf-llm不支持工具/函数调用（无function-calling）  -- runs ok  \n",
    "# ### 缺点： 中间信息复用的代码逻辑不好写，所以还是用llama-index（Context）\n",
    "# 本地hf-llm只生成文本（通过提示词控制 生成文本 这个行为），\n",
    "# 其余的我来\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os, json, re, textwrap, requests\n",
    "\n",
    "# 1) 本地/离线 LLM（不需要工具调用协议）\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name     = r\"Qwen/Qwen3-1.7B\",\n",
    "    tokenizer_name = r\"Qwen/Qwen3-1.7B\",\n",
    "    context_window = 3900,\n",
    "    max_new_tokens = 640,\n",
    "    generate_kwargs={\"temperature\": 0.5, \"top_k\": 30, \"top_p\": 0.95},\n",
    "    device_map     = \"cpu\",\n",
    ")\n",
    "\n",
    "# 2) Tavily 搜索（直接 REST；不依赖 LlamaIndex 的 Tool）\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "def tavily_search(query: str, max_results: int = 6, depth: str = \"advanced\"):\n",
    "    \"\"\"显式调用 Tavily 的 REST API。\"\"\"\n",
    "    url = \"https://api.tavily.com/search\"\n",
    "    payload = {\n",
    "        \"api_key\": TAVILY_API_KEY,\n",
    "        \"query\": query,\n",
    "        \"search_depth\": depth,           # \"basic\" / \"advanced\"\n",
    "        \"include_images\": False,\n",
    "        \"include_answer\": False,\n",
    "        \"max_results\": max_results,\n",
    "    }\n",
    "    r = requests.post(url, json=payload, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    # 期望 data[\"results\"] 是 [{title, url, content, score?}, ...]\n",
    "    return data.get(\"results\", [])\n",
    "\n",
    "def _trim(s: str, n=8000):\n",
    "    return s if len(s) <= n else (s[:n] + \" ...[truncated]\")\n",
    "\n",
    "# --------------------- 显式“工作流”步骤 ---------------------\n",
    "\n",
    "def step_research(topic: str) -> dict:\n",
    "    \"\"\"Step-1: 搜索 + 生成结构化笔记（显式调用 Tavily 与本地LLM）\"\"\"\n",
    "    results = tavily_search(topic, max_results=6, depth=\"advanced\")\n",
    "    sources_md = \"\\n\".join(\n",
    "        f\"- [{it.get('title','(no title)')}]({it.get('url','')}) — {it.get('content','')[:240].replace('\\n', ' ')}\"\n",
    "        for it in results\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "你是研究助理。基于下列检索到的资料，为“{topic}”生成不超过 10 条的要点式研究笔记（Markdown）。\n",
    "要求：\n",
    "- 覆盖关键时间线/里程碑/核心人物或机构\n",
    "- 突出 21 世纪的发展（若相关）\n",
    "- 尽量引用来源（以 [编号] 形式对照下方“资料列表”序号）\n",
    "\n",
    "资料列表（按顺序标号）：\n",
    "{sources_md}\n",
    "\n",
    "请输出：\n",
    "### 笔记\n",
    "1. ...\n",
    "2. ...\n",
    "3. ...\n",
    "### 引用来源\n",
    "- [1] 标题（链接）\n",
    "- [2] ...\n",
    "\"\"\"\n",
    "    notes = llm.complete(prompt).text\n",
    "    state = {\n",
    "        \"topic\": topic,\n",
    "        \"search_results\": results,\n",
    "        \"notes_md\": notes,\n",
    "        \"sources_md\": sources_md,\n",
    "    }\n",
    "    return state\n",
    "\n",
    "def step_write_report(state: dict) -> dict:\n",
    "    \"\"\"Step-2: 写报告（显式把上一步的笔记传入 LLM）\"\"\"\n",
    "    topic = state[\"topic\"]\n",
    "    notes_md = state[\"notes_md\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "你是一名技术写作者。请基于下方“研究笔记”为主题“{topic}”撰写一篇**结构化 Markdown 报告**。\n",
    "要求：\n",
    "- 结构示例：# 概览 / ## 早期发展 / ## 1990s / ## 2000s / ## 2010s-2020s / ## 参考资料\n",
    "- 内容必须**紧密依赖**研究笔记，不要编造\n",
    "- 语言简洁，段落短小\n",
    "\n",
    "--- 研究笔记 ---\n",
    "{notes_md}\n",
    "\"\"\"\n",
    "    report_md = llm.complete(prompt).text\n",
    "    state[\"report_md\"] = report_md\n",
    "    return state\n",
    "\n",
    "def _extract_json_block(text: str) -> dict:\n",
    "    \"\"\"从模型输出中尽量提取 JSON（稳健解析）\"\"\"\n",
    "    # 先找 ```json ... ``` 包裹\n",
    "    m = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", text, flags=re.S)\n",
    "    if not m:\n",
    "        # 退化：找第一个 { ... } 块\n",
    "        m = re.search(r\"(\\{.*\\})\", text, flags=re.S)\n",
    "    if not m:\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(m.group(1))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def step_review(state: dict, strict: bool = False) -> dict:\n",
    "    \"\"\"Step-3: 审稿（输出 JSON：approved/changes）\"\"\"\n",
    "    report_md = state[\"report_md\"]\n",
    "    prompt = f\"\"\"\n",
    "你是审稿人。请仅以 JSON 方式给出审稿结论。\n",
    "规则：\n",
    "- 字段：approved (bool), summary (string), changes (string[])\n",
    "- 若 approved=false，请给出 3-6 条具体修改建议\n",
    "- 输出必须放在一个 ```json 块中，不要出现多余文字\n",
    "\n",
    "--- 待审报告 ---\n",
    "{_trim(report_md, 7000)}\n",
    "\"\"\"\n",
    "    review_raw = llm.complete(prompt).text\n",
    "    parsed = _extract_json_block(review_raw)\n",
    "    # 容错：默认通过\n",
    "    approved = bool(parsed.get(\"approved\", True))\n",
    "    changes = parsed.get(\"changes\", [])\n",
    "    summary = parsed.get(\"summary\", \"OK\")\n",
    "\n",
    "    state[\"review\"] = {\n",
    "        \"raw\": review_raw,\n",
    "        \"parsed\": parsed,\n",
    "        \"approved\": approved,\n",
    "        \"changes\": changes,\n",
    "        \"summary\": summary,\n",
    "    }\n",
    "    return state\n",
    "\n",
    "def step_revise_if_needed(state: dict, max_rounds: int = 1) -> dict:\n",
    "    \"\"\"可选：根据审稿意见进行 0~N 轮修订\"\"\"\n",
    "    rounds = 0\n",
    "    while rounds < max_rounds and not state[\"review\"][\"approved\"]:\n",
    "        changes = state[\"review\"][\"changes\"]\n",
    "        report_md = state[\"report_md\"]\n",
    "        topic = state[\"topic\"]\n",
    "        notes_md = state[\"notes_md\"]\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "根据以下审稿意见修订报告《{topic}》。必须遵循审稿条目逐条修改：\n",
    "- 审稿意见：\n",
    "{json.dumps(changes, ensure_ascii=False, indent=2)}\n",
    "\n",
    "限制：\n",
    "- 仍需严格依赖“研究笔记”，不要编造\n",
    "- 保持 Markdown 结构清晰\n",
    "\n",
    "--- 研究笔记 ---\n",
    "{notes_md}\n",
    "\n",
    "--- 旧版报告 ---\n",
    "{_trim(report_md, 7000)}\n",
    "\"\"\"\n",
    "        new_report = llm.complete(prompt).text\n",
    "        state[\"report_md\"] = new_report\n",
    "        # 重新审稿\n",
    "        state = step_review(state)\n",
    "        rounds += 1\n",
    "    return state\n",
    "\n",
    "# --------------------- 顶层 orchestrator ---------------------\n",
    "\n",
    "def run_explicit_workflow(user_request: str, revise_rounds: int = 1) -> dict:\n",
    "    # 1) 研究\n",
    "    state = step_research(user_request)\n",
    "    print(\"✅ Research done.\")\n",
    "\n",
    "    # 2) 写作\n",
    "    state = step_write_report(state)\n",
    "    print(\"✅ Draft written.\")\n",
    "\n",
    "    # 3) 审稿\n",
    "    state = step_review(state)\n",
    "    print(f\"✅ Review: approved={state['review']['approved']}\")\n",
    "\n",
    "    # 4) 需要的话，修订若干轮\n",
    "    state = step_revise_if_needed(state, max_rounds=revise_rounds)\n",
    "    print(f\"✅ Final approved={state['review']['approved']}\")\n",
    "\n",
    "    return state\n",
    "\n",
    "# --------------------- 运行示例 ---------------------\n",
    "if __name__ == \"__main__\":\n",
    "    topic = \"History of the World Wide Web and key 21st-century developments\"\n",
    "    final_state = run_explicit_workflow(topic, revise_rounds=1)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"# 最终报告（截断展示）\\n\")\n",
    "    print(_trim(final_state[\"report_md\"], 4000))\n",
    "    print(\"\\n# 审稿解析：\", final_state[\"review\"][\"parsed\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4a4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ecb7c17324c4031b58b8e3031c6efed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "WorkflowConfigurationError",
     "evalue": "At least one Event of type StopEvent must be returned by any step.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mWorkflowConfigurationError\u001b[39m                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m     m = re.search(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m```json\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*(\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m{\u001b[39m\u001b[33m.*?\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m})\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*```\u001b[39m\u001b[33m\"\u001b[39m, text, flags=re.S) \u001b[38;5;129;01mor\u001b[39;00m re.search(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m{\u001b[39m\u001b[33m.*\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m})\u001b[39m\u001b[33m\"\u001b[39m, text, flags=re.S)\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m json.loads(m.group(\u001b[32m1\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m wf = \u001b[43mWorkflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;129m@step\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresearch\u001b[39m(ev: StartEvent) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m     40\u001b[39m     topic = ev.input[\u001b[33m\"\u001b[39m\u001b[33mtopic\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\workflows\\workflow.py:108\u001b[39m, in \u001b[36mWorkflow.__init__\u001b[39m\u001b[34m(self, timeout, disable_validation, verbose, service_manager, resource_manager, num_concurrent_runs)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28mself\u001b[39m._disable_validation = disable_validation\n\u001b[32m    107\u001b[39m \u001b[38;5;28mself\u001b[39m._num_concurrent_runs = num_concurrent_runs\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[38;5;28mself\u001b[39m._stop_event_class = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ensure_stop_event_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;28mself\u001b[39m._start_event_class = \u001b[38;5;28mself\u001b[39m._ensure_start_event_class()\n\u001b[32m    110\u001b[39m \u001b[38;5;28mself\u001b[39m._sem = (\n\u001b[32m    111\u001b[39m     asyncio.Semaphore(num_concurrent_runs) \u001b[38;5;28;01mif\u001b[39;00m num_concurrent_runs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    112\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\workflows\\workflow.py:167\u001b[39m, in \u001b[36mWorkflow._ensure_stop_event_class\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_found == \u001b[32m0\u001b[39m:\n\u001b[32m    166\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mAt least one Event of type StopEvent must be returned by any step.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m WorkflowConfigurationError(msg)\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m num_found > \u001b[32m1\u001b[39m:\n\u001b[32m    169\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOnly one type of StopEvent is allowed per workflow, found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstop_events_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mWorkflowConfigurationError\u001b[39m: At least one Event of type StopEvent must be returned by any step."
     ]
    }
   ],
   "source": [
    "# 用 LlamaIndex 的 Workflow（仍是显式路径，不让模型挑工具）\n",
    "# explicit_workflow_llamaindex.py  —— 显式编排，无 function-calling\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os, json, re, requests\n",
    "from typing import Union\n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.workflow import (\n",
    "    Workflow, StartEvent, StopEvent, step, Event\n",
    ")\n",
    "\n",
    "# ================== 基础组件 ==================\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name     = r\"Qwen/Qwen3-1.7B\",\n",
    "    tokenizer_name = r\"Qwen/Qwen3-1.7B\",\n",
    "    context_window = 3900,\n",
    "    max_new_tokens = 640,\n",
    "    generate_kwargs={\"temperature\": 0.5, \"top_k\": 30, \"top_p\": 0.95},\n",
    "    device_map     = \"cpu\",\n",
    ")\n",
    "\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "def tavily_search(query: str, max_results: int = 6):\n",
    "    url = \"https://api.tavily.com/search\"\n",
    "    payload = {\n",
    "        \"api_key\": TAVILY_API_KEY,\n",
    "        \"query\": query,\n",
    "        \"search_depth\": \"advanced\",\n",
    "        \"max_results\": max_results\n",
    "    }\n",
    "    resp = requests.post(url, json=payload, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json().get(\"results\", [])\n",
    "\n",
    "def _extract_json_block(text: str) -> dict:\n",
    "    m = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", text, flags=re.S) or re.search(r\"(\\{.*\\})\", text, flags=re.S)\n",
    "    if not m:\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(m.group(1))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "# ================== 事件类型（显式定义） ==================\n",
    "class ResearchDone(Event):\n",
    "    topic: str\n",
    "    sources_md: str\n",
    "    notes_md: str\n",
    "\n",
    "class DraftWritten(Event):\n",
    "    topic: str\n",
    "    notes_md: str\n",
    "    report_md: str\n",
    "\n",
    "class ReviewApproved(Event):\n",
    "    topic: str\n",
    "    notes_md: str\n",
    "    report_md: str\n",
    "    review: dict  # {\"approved\": True, \"summary\": \"...\", \"changes\": []}\n",
    "\n",
    "class ReviewChangesNeeded(Event):\n",
    "    topic: str\n",
    "    notes_md: str\n",
    "    report_md: str\n",
    "    review: dict  # {\"approved\": False, \"summary\": \"...\", \"changes\": [...]}\n",
    "\n",
    "# ================== Workflow & Steps ==================\n",
    "class wf(Workflow):\n",
    "    # 因为是从前往后，单线， 所以不用Context也可以\n",
    "    @step\n",
    "    def research(ev: StartEvent) -> ResearchDone:\n",
    "        topic = ev.input[\"topic\"]\n",
    "        results = tavily_search(topic, max_results=6)\n",
    "        sources_md = \"\\n\".join(\n",
    "            f\"- [{it.get('title','(no title)')}]({it.get('url','')}) — {it.get('content','')[:240].replace('\\n',' ')}\"\n",
    "            for it in results\n",
    "        )\n",
    "        prompt = f\"请基于下列资料为“{topic}”生成 6-10 条要点式 Markdown 笔记：\\n{sources_md}\"\n",
    "        notes_md = llm.complete(prompt).text\n",
    "        return ResearchDone(topic=topic, sources_md=sources_md, notes_md=notes_md)\n",
    "\n",
    "    @step\n",
    "    def write(ev: ResearchDone) -> DraftWritten:\n",
    "        topic, notes_md = ev.topic, ev.notes_md\n",
    "        prompt = f\"基于研究笔记为“{topic}”写一篇结构化 Markdown 报告：\\n{notes_md}\"\n",
    "        report_md = llm.complete(prompt).text\n",
    "        return DraftWritten(topic=topic, notes_md=notes_md, report_md=report_md)\n",
    "\n",
    "    @step\n",
    "    def review(ev: DraftWritten) -> Union[ReviewApproved, ReviewChangesNeeded]:\n",
    "        report_md = ev.report_md\n",
    "        prompt = f\"\"\"你是审稿人，仅以 JSON 回答：{{\"approved\": true/false, \"summary\": \"...\", \"changes\": [\"...\"]}}。\n",
    "    报告：\n",
    "    {report_md}\n",
    "    \"\"\"\n",
    "        parsed = _extract_json_block(llm.complete(prompt).text)\n",
    "        if not parsed:\n",
    "            parsed = {\"approved\": True, \"summary\": \"OK\", \"changes\": []}\n",
    "\n",
    "        if bool(parsed.get(\"approved\", True)):\n",
    "            return ReviewApproved(\n",
    "                topic=ev.topic, notes_md=ev.notes_md, report_md=report_md, review=parsed\n",
    "            )\n",
    "        else:\n",
    "            return ReviewChangesNeeded(\n",
    "                topic=ev.topic, notes_md=ev.notes_md, report_md=report_md, review=parsed\n",
    "            )\n",
    "\n",
    "    @step\n",
    "    def revise(ev: ReviewChangesNeeded) -> DraftWritten:\n",
    "        \"\"\"按审稿意见修订一轮后，回到 review。\"\"\"\n",
    "        changes = ev.review.get(\"changes\", [])\n",
    "        prompt = (\n",
    "            \"根据以下审稿意见修订报告（保持 Markdown 结构，逐条落实）：\\n\"\n",
    "            f\"{json.dumps(changes, ensure_ascii=False, indent=2)}\\n\"\n",
    "            \"---\\n旧版：\\n\"\n",
    "            f\"{ev.report_md}\"\n",
    "        )\n",
    "        new_report = llm.complete(prompt).text\n",
    "        return DraftWritten(topic=ev.topic, notes_md=ev.notes_md, report_md=new_report)\n",
    "\n",
    "    @step\n",
    "    def end(ev: ReviewApproved) -> StopEvent:\n",
    "        \"\"\"终止：返回最终报告与审稿结果。\"\"\"\n",
    "        return StopEvent(result={\"report_md\": ev.report_md, \"review\": ev.review})\n",
    "\n",
    "\n",
    "# =============== 运行与打印 ===============\n",
    "handler = wf.run(input={\"topic\": \"History of the World Wide Web and 21st-century developments\"})\n",
    "\n",
    "# 可选：打印事件流\n",
    "for ev in handler.stream_events():\n",
    "    name = type(ev).__name__\n",
    "    payload = getattr(ev, \"result\", None)\n",
    "    if payload:\n",
    "        print(f\"[{name}] result keys: {list(payload) if isinstance(payload, dict) else str(payload)[:60]}\")\n",
    "    else:\n",
    "        print(f\"[{name}]\")\n",
    "\n",
    "final = handler.get()  # StopEvent.result\n",
    "print(\"\\n=== FINAL REPORT (snippet) ===\\n\")\n",
    "print(final[\"report_md\"][:2000])\n",
    "print(\"\\n=== REVIEW ===\\n\", json.dumps(final[\"review\"], ensure_ascii=False, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea779dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file \n",
    "\n",
    "from pathlib import Path \n",
    "\n",
    "pdf_root = r\"./log/simplePDF\"    # ~root/subdir/ ..data\n",
    "imPDF_root = r\"./log/ImagePDF\"   # ~root/subdir/ ..data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031680bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd70887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "project_root = Path(__file__).parent.parent\n",
    "file_md = r\"\"\n",
    "file_pdf = r\"\"\n",
    "\n",
    "\n",
    "file_md_im = r\"\"\n",
    "file_pdf_im = r\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15b7e3",
   "metadata": {},
   "source": [
    "### 部署  llama_deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2254e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
