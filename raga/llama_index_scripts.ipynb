{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5632732",
   "metadata": {},
   "source": [
    "### RAG  llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b415a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== å‘½ä¸­ 1 (score=0.423) ===\n",
      "# (19)ä¸­åäººæ°‘å…±å’Œå›½å›½å®¶çŸ¥è¯†äº§æƒå±€\n",
      "\n",
      "# (12)å®ç”¨æ–°å‹ä¸“åˆ©\n",
      "\n",
      "(10)æˆæƒå…¬å‘Šå·CN207225508U(45)æˆæƒå…¬å‘Šæ—¥2018.04.13\n",
      "\n",
      "(21)ç”³è¯·å·201721328994.5\n",
      "\n",
      "(22)ç”³è¯·æ—¥2017.10.16\n",
      "\n",
      "(73)ä¸“åˆ©æƒäººæ­å·å®‡æ ‘ç§‘æŠ€æœ‰é™å…¬å¸åœ°å€310051æµ™æ±Ÿçœæ­å·å¸‚æ»¨æ±ŸåŒºèšä¸šè·¯26å·é‡‘ç»£å›½é™…ç§‘æŠ€ä¸­å¿ƒBåº§106å®¤\n",
      "\n",
      "(72)å‘æ˜äººç‹å…´å…´ æ¨çŸ¥é›¨\n",
      "\n",
      "(74)ä¸“åˆ©ä»£ç†æœºæ„æµ™æ±Ÿç¿”éš†ä¸“åˆ©äº‹åŠ¡æ‰€ï¼ˆæ™®é€šåˆä¼™ï¼‰33206\n",
      "\n",
      "ä»£ç†äººè®¸å®ˆé‡‘\n",
      "\n",
      "(51)Int.Cl. ...\n",
      "ç›¸å…³å›¾ï¼š ['å›¾0']\n",
      "é™„å›¾æ ‡è®°ï¼š é™„å›¾æ ‡è®°è¯´æ˜ï¼š1ï¼šä¿æŠ¤ç½©æ€»æˆï¼Œ2ï¼šè¶³åŸºåº§æ€»æˆï¼Œ3ï¼šè¶³å«ï¼Œ2-1ï¼šä¼ æ„Ÿéƒ¨ä»¶ï¼Œ2-2ï¼šåº”å˜ç‰‡ï¼Œ2-3ï¼šå‘å…‰ä»¶ï¼Œ2-4ï¼šæµ‹æ§å•å…ƒï¼Œ2-5ï¼šè¶³åŸºåº§ï¼Œ2-1- 1ï¼šæ•æ„Ÿéƒ¨ï¼Œ2-1- 1-1ï¼šå¹³è¡Œå¹³é¢åŒºåŸŸ ...\n",
      "\n",
      "=== å‘½ä¸­ 2 (score=0.304) ===\n",
      "B62D57/032(2006.01) G01L1/18(2006.01) G01C21/10(2006.01)\n",
      "\n",
      "æƒåˆ©è¦æ±‚ä¹¦1é¡µ è¯´æ˜ä¹¦4é¡µ é™„å›¾4é¡µ\n",
      "\n",
      "# (54)å®ç”¨æ–°å‹åç§°\n",
      "\n",
      "ä¸€ç§æœºå™¨äººè¶³ç«¯ç»“æ„\n",
      "\n",
      "# (57)æ‘˜è¦\n",
      "\n",
      "æœ¬å®ç”¨æ–°å‹å…¬å¼€äº†ä¸€ç§æœºå™¨äººè¶³ç«¯ç»“æ„ï¼Œå±äºæœºå™¨äººè®¾å¤‡æŠ€æœ¯é¢†åŸŸã€‚ç°æœ‰æŠ€æœ¯çš„è¶³ç«¯ç»“æ„è¾ƒå¤æ‚ï¼Œä½“ç§¯åŠé‡é‡å¤§ï¼Œå†…ç½®çš„åŠ›ä¼ æ„Ÿå™¨å¯é æ€§ä½ã€‚ä¸€ç§æœºå™¨äººè¶³ç«¯ç»“æ„ï¼ŒåŒ…æ‹¬ç”¨äºå’Œæœºå™¨äººè…¿éƒ¨è¿æ†ç›¸è¿æ¥çš„è¶³åŸºåº§æ€»æˆã€ç”¨äºç¼“å†²ä¼ é€’å†²å‡»åŠ›çš„è¶³å«å’Œä¿æŠ¤ç½©æ€»æˆï¼Œæ‰€è¿°è¶³åŸºåº§æ€»æˆå†…çš„ä¼ æ„Ÿéƒ¨ä»¶ä¸Šè®¾æœ‰èƒ½äº§ç”Ÿè¾ƒå¤§å˜å½¢çš„æ•æ„Ÿéƒ¨ï¼Œæ‰€è¿°æ•æ„Ÿéƒ¨ç”±é«˜å±ˆæœå¼ºåº¦ææ–™åˆ¶æˆï¼Œæ‰€è¿°æ•æ„Ÿéƒ¨è¡¨é¢ç²˜è´´æœ‰åº”å˜æ¡¥ï¼›æ‰€è¿°è¶³å«ä¼ é€’è¶³å—åˆ°çš„æ”¯æ’‘åŠ›åˆ°è¶³åŸºåº§æ€»æˆï¼Œä½¿æ•æ„Ÿéƒ¨äº§ç”Ÿè¾ƒå¤§å˜å½¢ï¼Œä»è€Œé€šè¿‡åº”å˜æ¡¥æ£€æµ‹åˆ°è¶³ç«¯æ”¯æ’‘åŠ›çš„å¤§å°ã€‚æœ¬å®ç”¨æ–°å‹é›†åŠ›ä¼ æ„Ÿå™¨ä¸è¶³ç«¯ä¸ºä¸€ä½“ï¼Œé›¶éƒ¨ä»¶å°‘ï¼Œç»“æ„ç®€å•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡è½»æœºå™¨äººè¶³ç«¯çš„é‡é‡ä¸ä½“ç§¯ï¼Œä¾¿äºç”Ÿäº§åˆ¶é€ ï¼ŒåŠ›ä¼ æ„Ÿå™¨è¿‡è½½èƒ½åŠ›å¼ºä¸æ˜“å¤±æ•ˆï¼Œåˆ¶é€ æˆæœ¬ä½ã€‚\n",
      "\n",
      "\n",
      "1. ...\n",
      "ç›¸å…³å›¾ï¼š ['å›¾4']\n",
      "é™„å›¾æ ‡è®°ï¼š é™„å›¾æ ‡è®°è¯´æ˜ï¼š1ï¼šä¿æŠ¤ç½©æ€»æˆï¼Œ2ï¼šè¶³åŸºåº§æ€»æˆï¼Œ3ï¼šè¶³å«ï¼Œ2-1ï¼šä¼ æ„Ÿéƒ¨ä»¶ï¼Œ2-2ï¼šåº”å˜ç‰‡ï¼Œ2-3ï¼šå‘å…‰ä»¶ï¼Œ2-4ï¼šæµ‹æ§å•å…ƒï¼Œ2-5ï¼šè¶³åŸºåº§ï¼Œ2-1- 1ï¼šæ•æ„Ÿéƒ¨ï¼Œ2-1- 1-1ï¼šå¹³è¡Œå¹³é¢åŒºåŸŸ ...\n",
      "\n",
      "=== å‘½ä¸­ 3 (score=0.296) ===\n",
      "[0016] è¿›ä¸€æ­¥ï¼Œæœ¬å®ç”¨æ–°å‹çš„åº”å˜ç‰‡é»è´´äºæ•æ„Ÿéƒ¨ä¸Šï¼Œèƒ½å¤Ÿå‡†ç¡®æ„Ÿåº”ä¼ æ„Ÿéƒ¨ä»¶çš„å½¢å˜ä½ç§»ï¼ŒæŠŠæ”¯æ’‘åŠ›çš„å¤§å°è½¬åŒ–ä¸ºè‡ªèº«ç”µé˜»å˜åŒ–çš„å¤§å°ï¼Œè¿›è€Œå®ç°å¯¹è¶³ç«¯å—åŠ›çš„æµ‹é‡å¹¶ä¸”æµ‹é‡ç²¾åº¦é«˜ã€‚\n",
      "\n",
      "# å…·ä½“å®æ–½æ–¹å¼\n",
      "\n",
      "[0025] ä¸ºäº†ä½¿æœ¬å®ç”¨æ–°å‹çš„ç›®çš„ã€æŠ€æœ¯æ–¹æ¡ˆåŠä¼˜ç‚¹æ›´åŠ æ¸…æ¥šæ˜ç™½ï¼Œä»¥ä¸‹ç»“åˆé™„å›¾åŠå®æ–½ä¾‹ï¼Œå¯¹æœ¬å®ç”¨æ–°å‹è¿›è¡Œè¿›ä¸€æ­¥è¯¦ç»†è¯´æ˜ã€‚åº”å½“ç†è§£ï¼Œæ­¤å¤„æ‰€æè¿°çš„å…·ä½“å®æ–½ä¾‹ä»…ä»…ç”¨ä»¥è§£é‡Šæœ¬å®ç”¨æ–°å‹ï¼Œå¹¶ä¸ç”¨äºé™å®šæœ¬å®ç”¨æ–°å‹ã€‚\n",
      "\n",
      "[0026] ç›¸åï¼Œæœ¬å®ç”¨æ–°å‹æ¶µç›–ä»»ä½•ç”±æƒåˆ©è¦æ±‚å®šä¹‰çš„åœ¨æœ¬å®ç”¨æ–°å‹çš„ç²¾é«“å’ŒèŒƒå›´ä¸Šåšçš„æ›¿ä»£ã€ä¿®æ”¹ã€ç­‰æ•ˆæ–¹æ³•ä»¥åŠæ–¹æ¡ˆã€‚è¿›ä¸€æ­¥ï¼Œä¸ºäº†ä½¿å…¬ä¼—å¯¹æœ¬å®ç”¨æ–°å‹æœ‰æ›´å¥½çš„äº†è§£ï¼Œåœ¨ä¸‹æ–‡å¯¹æœ¬å®ç”¨æ–°å‹çš„ç»†èŠ‚æè¿°ä¸­ï¼Œè¯¦å°½æè¿°äº†ä¸€äº›ç‰¹å®šçš„ç»†èŠ‚éƒ¨åˆ†ã€‚å¯¹æœ¬é¢†åŸŸæŠ€æœ¯äººå‘˜æ¥è¯´æ²¡æœ‰è¿™äº›ç»†èŠ‚éƒ¨åˆ†çš„æè¿°ä¹Ÿå¯ä»¥å®Œå…¨ç†è§£æœ¬å®ç”¨æ–°å‹ã€‚\n",
      "\n",
      "[0027] éœ€è¦è¯´æ˜çš„æ˜¯ï¼Œå½“å…ƒä»¶è¢«ç§°ä¸ºâ€œå›ºå®šäºâ€å¦ä¸€ä¸ªå…ƒä»¶ï¼Œå®ƒå¯ä»¥ç›´æ¥åœ¨å¦ä¸€ä¸ªå…ƒä»¶ä¸Šæˆ–è€…ä¹Ÿå¯ä»¥å­˜åœ¨å±…ä¸­çš„å…ƒä»¶ã€‚å½“ä¸€ä¸ªå…ƒä»¶è¢«è®¤ä¸ºæ˜¯â€œè¿æ¥â€å¦ä¸€ä¸ªå…ƒä»¶ï¼Œå®ƒå¯ä»¥æ˜¯ç›´æ¥è¿æ¥åˆ°å¦ä¸€ä¸ªå…ƒä»¶æˆ–è€…å¯èƒ½åŒæ—¶å­˜åœ¨å±…ä¸­å…ƒä»¶ã€‚ç›¸åï¼Œå½“å…ƒä»¶è¢«ç§°ä½œâ€œç›´æ¥åœ¨â€å¦ä¸€å…ƒä»¶â€œä¸Šâ€æ—¶ï¼Œä¸å­˜åœ¨ä¸­é—´å…ƒä»¶ã€‚æœ¬æ–‡æ‰€ä½¿ç”¨çš„æœ¯è¯­â€œä¸Šâ€ã€â€œä¸‹â€ä»¥åŠç±»ä¼¼çš„è¡¨è¿°åªæ˜¯ä¸ºäº†è¯´æ˜çš„ç›®çš„ã€‚\n",
      "\n",
      "[0028] é™¤éå¦æœ‰å®šä¹‰ï¼Œæœ¬æ–‡æ‰€ä½¿ç”¨çš„æ‰€æœ‰æŠ€æœ¯å’Œç§‘å­¦æœ¯è¯­ä¸å±äºæœ¬å®ç”¨æ–°å‹çš„æŠ€æœ¯é¢†åŸŸçš„æŠ€æœ¯äººå‘˜é€šå¸¸ç†è§£çš„å«ä¹‰ç›¸åŒã€‚æœ¬æ–‡æ‰€ä½¿ç”¨çš„æœ¯è¯­åªæ˜¯ä¸ºäº†æè¿°å…·ä½“çš„å®æ–½ä¾‹çš„ç›®çš„ï¼Œä¸æ˜¯æ—¨åœ¨é™åˆ¶æœ¬å®ç”¨æ–°å‹ã€‚ ...\n",
      "ç›¸å…³å›¾ï¼š ['å›¾0']\n",
      "é™„å›¾æ ‡è®°ï¼š é™„å›¾æ ‡è®°è¯´æ˜ï¼š1ï¼šä¿æŠ¤ç½©æ€»æˆï¼Œ2ï¼šè¶³åŸºåº§æ€»æˆï¼Œ3ï¼šè¶³å«ï¼Œ2-1ï¼šä¼ æ„Ÿéƒ¨ä»¶ï¼Œ2-2ï¼šåº”å˜ç‰‡ï¼Œ2-3ï¼šå‘å…‰ä»¶ï¼Œ2-4ï¼šæµ‹æ§å•å…ƒï¼Œ2-5ï¼šè¶³åŸºåº§ï¼Œ2-1- 1ï¼šæ•æ„Ÿéƒ¨ï¼Œ2-1- 1-1ï¼šå¹³è¡Œå¹³é¢åŒºåŸŸ ...\n"
     ]
    }
   ],
   "source": [
    "# file-parser\n",
    "from typing import List, Dict, Any   \n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm \n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings \n",
    "from llama_index.core.node_parser import SentenceSplitter, TokenTextSplitter, MarkdownNodeParser \n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.extractors import TitleExtractor   # éœ€è¦æ¥å…¥llm\n",
    "\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# embedding\n",
    "embedding = HuggingFaceEmbedding(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    device=\"cpu\",                 # å»ºè®®æ”¾é¡¶å±‚\n",
    "    cache_folder=r\"E:\\local_models\\huggingface\\cache\\hub\",\n",
    "    trust_remote_code=True,       # å»ºè®®æ”¾é¡¶å±‚\n",
    "    model_kwargs={\"local_files_only\": False},   # å…è®¸è”ç½‘ False\n",
    ")\n",
    "Settings.embed_model = embedding\n",
    "\n",
    "# full_split.md      figs_MetaDict.json  \n",
    "data_root = r\"../.log/SimplePDF\"\n",
    "mdfs: str = str(next(Path(data_root).rglob('full_split.md'), None))\n",
    "assert  Path(mdfs).exists()\n",
    "\n",
    "# å…ƒæ•°æ® ++\n",
    "# ä¿å­˜å›¾ç‰‡ä¿¡æ¯å­—æ®µçš„jsonæ–‡ä»¶åœ¨mdfsçš„åŒçº§ç›®å½•ä¸‹ æ–‡ä»¶å figs_MetaDict.json\n",
    "def load_jsf(mdp: Path) -> Dict[str, str | List]:\n",
    "    jsp = str(next(Path(mdp).parent.glob(\"figs_MetaDict.json\"),None))\n",
    "    if not jsp:\n",
    "        raise TypeError(\"jsp is NoneType\")\n",
    "    # assert jsp\n",
    "    \n",
    "    with open(jsp, \"r\", encoding='utf-8') as jf:\n",
    "        ims_metadata = json.load(jf)\n",
    "    return ims_metadata\n",
    "\n",
    "\"\"\" figs_metadict.json  \n",
    "{\n",
    "    \"im_abs\": [\n",
    "        \"æ‘˜è¦å›¾\",\n",
    "        \"path/to/im_abs\"\n",
    "    ],\n",
    "    \"lines_ims\": [                    \n",
    "        \"å›¾1ä¸ºæœ¬å®ç”¨æ–°å‹çš„çˆ†ç‚¸å›¾\",\n",
    "        \"å›¾2ä¸ºæœ¬å®ç”¨æ–°å‹çš„ä¾§é¢ç¤ºå›¾\",\n",
    "        ...\n",
    "    ],\n",
    "    \"annos_ims\": \"å›¾ä¸­ï¼š1ã€é©±åŠ¨æ¿ï¼›2ã€è½¬å­ç¼–ç å™¨èŠ¯ç‰‡ï¼›...\",\n",
    "    \"im_1\": [\n",
    "        \"å›¾1ä¸ºæœ¬å®ç”¨æ–°å‹çš„çˆ†ç‚¸å›¾\",\n",
    "        \"path/to/im_1\"\n",
    "    ],\n",
    "    \"im_2\": [\n",
    "        \"å›¾2ä¸ºæœ¬å®ç”¨æ–°å‹çš„ä¾§é¢ç¤ºå›¾\",\n",
    "        \"path/to/im_2\"\n",
    "    ],\n",
    "    ...\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# load_data\n",
    "documents = SimpleDirectoryReader(input_files=[mdfs]).load_data()\n",
    "# # figs_metadict  \n",
    "figs_dict = load_jsf(mdfs)   \n",
    "\n",
    "# spliter\n",
    "text_spliter = SentenceSplitter(chunk_size=700, chunk_overlap=100)\n",
    "\n",
    "# èŠ‚ç‚¹åå¤„ç†å™¨  Postprocessor        \n",
    "# ç›¸åº”åˆæˆå™¨    Response Synthesizer    # todo.\n",
    "from llama_index.core.postprocessor import KeywordNodePostprocessor, LongContextReorder #  \n",
    "# pipeline \n",
    "pipeline = IngestionPipeline(transformations=[text_spliter])\n",
    "\n",
    "# nodes\n",
    "nodes = pipeline.run(documents=documents)\n",
    "\n",
    "# ç»™æ¯ä¸ª node è®°å½• doc_id/figs_pathï¼Œåé¢å¥½å–å›¾ï¼Œå›¾çš„æè¿°è¯­ä¹Ÿæ˜¯éœ€è¦ä¸€åŒæå–å‡ºæ¥çš„ï¼Œå›¾ä¸­çš„æ ‡è®°æˆ–è®¸ä¹Ÿéœ€è¦å±•ç¤º/è¾“å‡ºå‡ºæ¥\n",
    "doc_id = Path(mdfs).parent.name\n",
    "for n in nodes:\n",
    "    n.metadata[\"doc_id\"] = doc_id\n",
    "    n.metadata[\"figs_path\"] = str(Path(mdfs).with_name(\"figs_MetaDict.json\"))\n",
    "\n",
    "\n",
    "# index\n",
    "nodes_idx = VectorStoreIndex(nodes=nodes)\n",
    "\n",
    "# 3) çº¯æ£€ç´¢ï¼šä¸èµ° LLMï¼Œåªå– SourceNodes\n",
    "retriever = nodes_idx.as_retriever(similarity_top_k=3)\n",
    "query = \"ä»‹ç»ä¸€ä¸‹è¿™æ˜¯ä»€ä¹ˆä¸“åˆ©\"\n",
    "source_nodes = retriever.retrieve(query)\n",
    "\n",
    "# 4) å…³è”å›¾ç‰‡ï¼ˆæœ€ç®€å•è§„åˆ™ï¼šchunk æ–‡æœ¬é‡Œæ‰¾â€œå›¾Nâ€ï¼‰\n",
    "import re\n",
    "def pick_figs_for_text(text: str, figs: Dict[str, Any], top_k:int=2):\n",
    "    out = []\n",
    "    nums = [int(m.group(1)) for m in re.finditer(r'å›¾\\s*(\\d+)', text)]\n",
    "    seen = set()\n",
    "    for n in nums:\n",
    "        key = f\"im_{n}\"\n",
    "        if key in figs and n not in seen:\n",
    "            cap, path = figs[key]\n",
    "            out.append({\"no\": n, \"caption\": cap, \"url\": path})\n",
    "            seen.add(n)\n",
    "            if len(out) >= top_k: break\n",
    "    # è¡¥ä¸€ä¸ªæ‘˜è¦å›¾å…œåº•\n",
    "    if len(out) == 0 and \"im_abs\" in figs:\n",
    "        cap, path = figs[\"im_abs\"]\n",
    "        out.append({\"no\": 0, \"caption\": cap, \"url\": path})\n",
    "    return out\n",
    "\n",
    "payload = []\n",
    "for sn in source_nodes:\n",
    "    figs = figs_dict  # å•æ–‡æ¡£å°±ç›´æ¥ç”¨ï¼›å¤šæ–‡æ¡£å¯æŒ‰ sn.node.metadata[\"doc_id\"] é€‰æ‹©\n",
    "    figs_pick = pick_figs_for_text(sn.node.get_text(), figs, top_k=2)\n",
    "    payload.append({\n",
    "        \"score\": sn.score,\n",
    "        \"text\": sn.node.get_text(),\n",
    "        \"figures\": figs_pick,\n",
    "        \"annos\": figs.get(\"annos_ims\", \"\")\n",
    "    })\n",
    "\n",
    "# æ‰“å°/è¿”å›ç»™å‰ç«¯\n",
    "for i, item in enumerate(payload, 1):\n",
    "    print(f\"\\n=== å‘½ä¸­ {i} (score={item['score']:.3f}) ===\")\n",
    "    print(item[\"text\"], \"...\")\n",
    "    if item[\"figures\"]:\n",
    "        print(\"ç›¸å…³å›¾ï¼š\", [f\"å›¾{f['no']}\" for f in item[\"figures\"]])\n",
    "    if item[\"annos\"]:\n",
    "        print(\"é™„å›¾æ ‡è®°ï¼š\", item[\"annos\"][:120], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483a2d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ims-info-dict   txts-str \n",
    "\n",
    "# ims-info-dict.keys(): im_abs lines_ims annos_ims im_n\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c45b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0c1240f00b48f3a24fea03d8c6f58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# agents.ipynb\n",
    "\"\"\"   \n",
    "#### AgentWorkflowçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ\n",
    "# AgentWorkflow å•ä¸ªæ™ºèƒ½ä½“ã€å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚ å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­å¤šä¸ªæ™ºèƒ½ä½“åä½œå®Œæˆä»»åŠ¡ï¼Œå¹¶åœ¨éœ€è¦æ—¶å°†æ§åˆ¶æƒäº’ç›¸ç§»äº¤\n",
    "# - ResearchAgent  : å®ƒå°†æœç´¢ç½‘ç»œä¸€æŸ¥æ‰¾ç»™å®šä¸»é¢˜çš„ä¿¡æ¯\n",
    "# - WriteAgent     : å°†ç”¨ResearchAgentæ£€ç´¢åˆ°çš„ä¿¡æ¯æ¥æ’°å†™æŠ¥å‘Š\n",
    "# - ReviewAgent    : å°†å®¡æŸ¥æŠ¥å‘Šå¹¶æä¾›åé¦ˆ\n",
    "## éœ€è¦ç”¨åˆ°çš„å·¥å…·\n",
    "# web_searchå·¥å…·ï¼Œ  Tavily\n",
    "# record_noteså·¥å…·ï¼Œå°†ç½‘ç»œæœç´¢é“å¾·ç ”ç©¶ä¿å­˜åˆ°çŠ¶æ€ä¸­ï¼ˆAgentWorkflowä½¿ç”¨ä¸€ä¸ªåä¸ºstateçš„Contextå˜é‡ï¼‰ï¼Œç„¶åå…¶ä»–å·¥å…·å°±å¯ä»¥ä½¿ç”¨å®ƒ\n",
    "# write_repotå·¥å…·ï¼Œä½¿ç”¨ResearchAgentæ£€ç´¢åˆ°çš„ä¿¡æ¯æ’°å†™æŠ¥å‘Š\n",
    "# review_reportå·¥å…·ï¼Œå®¡æŸ¥æŠ¥å‘Šå’Œæä¾›åé¦ˆ\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"   \n",
    "# HuggingFaceLLM ä¸æ”¯æŒå‡½æ•°è°ƒç”¨/å·¥å…·é€‰æ‹©   è®¸å¤šæœ¬åœ°å°æ¨¡å‹éƒ½ä¸æ”¯æŒå‡½æ•°è°ƒç”¨ã€å·¥å…·æ”¯æŒï¼Œ\n",
    "\n",
    "æƒ³ç”¨æœ¬åœ°/ç¦»çº¿æ¨¡å‹ï¼Œä½†åˆè¦å¤šæ™ºèƒ½ä½“ä¸å·¥å…·è°ƒç”¨ï¼Œæœ‰ä¸¤æ¡è·¯ï¼š\n",
    "  1. åœ¨æœ¬åœ°æ¨¡å‹ä¸Šå†å¥—ä¸€å±‚åŒ…è£…å™¨ï¼ˆ       -- ç®€å•æ–¹æ³•ï¼Œ \"æ™ºèƒ½ä½“\"  æ¨¡å‹è‡ªå·±å†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·/å‡½æ•°ã€ç„¶åæœ¬åœ°æœºå™¨è°ƒç”¨å·¥å…·/å‡½æ•°\n",
    "- è·‘ä¸€ä¸ª OpenAI-å…¼å®¹ç½‘å…³ï¼ˆå¦‚ vLLM + OpenAI æ¥å£ã€LiteLLM ç­‰ï¼‰ï¼ŒæŠŠæœ¬åœ°æ¨¡å‹â€œæŒ‚æˆâ€ OpenAI-style APIï¼Œå†ç”¨ OpenAI å°è£…ï¼›\n",
    "  2. æ¨¡å‹åªè´Ÿè´£ç”Ÿæˆæ–‡æœ¬ï¼Œä¸è´Ÿè´£é€‰å·¥å…·   --å¤æ‚æ–¹æ³•\n",
    "- æ”¾å¼ƒâ€œç”±æ¨¡å‹è‡ªå·±å†³å®šä½•æ—¶è°ƒå·¥å…·â€ï¼Œæ”¹ä¸ºå·¥ä½œæµæ˜¾å¼ç¼–æ’ï¼šç”±ä½ åœ¨ Python ä¸­å†³å®šå…ˆæœâ†’å†è®°ç¬”è®°â†’å†å†™â†’å†å®¡ï¼ˆæ¨¡å‹åªè´Ÿè´£ç”Ÿæˆæ–‡æœ¬ï¼Œä¸è´Ÿè´£é€‰å·¥å…·ï¼‰ã€‚\n",
    "\"\"\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from llama_index.core.agent.workflow import AgentWorkflow \n",
    "from llama_index.core.workflow import Context \n",
    "from llama_index.core.agent.workflow import (\n",
    "    AgentOutput,\n",
    "    ToolCall,\n",
    "    ToolCallResult,\n",
    ")\n",
    "from llama_index.tools.tavily_research import TavilyToolSpec\n",
    "from llama_index.core.agent.workflow import FunctionAgent \n",
    "import os \n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM \n",
    "llm = HuggingFaceLLM(\n",
    "    model_name     = r\"Qwen/Qwen3-1.7B\",\n",
    "    tokenizer_name = r\"Qwen/Qwen3-1.7B\",\n",
    "    context_window = 3900,  \n",
    "    max_new_tokens = 640,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"top_k\": 30, \"top_p\": 0.95},\n",
    "    device_map     ='cpu' \n",
    ")\n",
    "# HuggingFaceLLM ä¸æ”¯æŒå‡½æ•°è°ƒç”¨/å·¥å…·é€‰æ‹©\n",
    "\n",
    "tavily_tool = TavilyToolSpec(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "search_web = tavily_tool.to_tool_list()[0]\n",
    "\n",
    "# ---- å·¥å…·å‡½æ•°   \n",
    "async def record_notes(ctx: Context, notes: str, notes_title: str) -> str:\n",
    "    \"\"\" useful for recording notes on a gaven topic.\"\"\"\n",
    "    current_state = await ctx.store.get(\"state\")\n",
    "    if \"research_notes\" not in current_state:\n",
    "        current_state[\"research_notes\"] = {}\n",
    "    current_state[\"research_notes\"][notes_title] = notes\n",
    "    await ctx.store.set(\"state\", current_state)\n",
    "    return \"Notes recorded.\"\n",
    "\n",
    "async def write_report(ctx:Context, report_content:str) -> str:\n",
    "    \"\"\" useful write a report on a gaven topic.\"\"\"\n",
    "    current_state = ctx.store.get(\"state\")\n",
    "    current_state[\"report_content\"] = report_content\n",
    "    await ctx.store.set(\"state\", current_state)\n",
    "    return \"Report written\"\n",
    "    \n",
    "async def review_report(ctx:Context, review:str) -> str:\n",
    "    \"\"\"useful for reviewing a report and providing feedbacks\"\"\"\n",
    "    current_state = ctx.store.get(\"state\")\n",
    "    current_state[\"review\"] = review\n",
    "    await ctx.store.set([\"state\"],current_state)\n",
    "    return \"Report reviewed\" \n",
    "\n",
    "# ------------- agent\n",
    "research_agent = FunctionAgent(\n",
    "    name=\"ResearchAgent\",\n",
    "    description=\"Useful for searching the web for information on a given topic and recording notes on the topic.\",\n",
    "    system_prompt=(\n",
    "        \"You are the ResearchAgent that can search the web for information on a given topic and record notes on the topic. \"\n",
    "        \"Once notes are recorded and you are satisfied, you should hand off control to the WriteAgent to write a report on the topic.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[search_web, record_notes],\n",
    "    can_handoff_to=[\"WriteAgent\"],\n",
    ")\n",
    "\n",
    "write_agent = FunctionAgent(\n",
    "    name=\"WriteAgent\",\n",
    "    description=\"Useful for writing a report on a given topic.\",\n",
    "    system_prompt=(\n",
    "        \"You are the WriteAgent that can write a report on a given topic. \"\n",
    "        \"Your report should be in a markdown format. The content should be grounded in the research notes. \"\n",
    "        \"Once the report is written, you should get feedback at least once from the ReviewAgent.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[write_report],\n",
    "    can_handoff_to=[\"ReviewAgent\", \"ResearchAgent\"],\n",
    ")\n",
    "\n",
    "review_agent = FunctionAgent(\n",
    "    name=\"ReviewAgent\",\n",
    "    description=\"Useful for reviewing a report and providing feedback.\",\n",
    "    system_prompt=(\n",
    "        \"You are the ReviewAgent that can review a report and provide feedback. \"\n",
    "        \"Your feedback should either approve the current report or request changes for the WriteAgent to implement.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[review_report],\n",
    "    can_handoff_to=[\"WriteAgent\"],\n",
    ")\n",
    "\n",
    "agent_workflow = AgentWorkflow(\n",
    "    agents=[research_agent, write_agent, review_agent],\n",
    "    root_agent=research_agent.name,\n",
    "    initial_state={\n",
    "        \"research_notes\": {},\n",
    "        \"report_content\": \"Not written yet.\",\n",
    "        \"review\": \"Review required.\",\n",
    "    },\n",
    ")\n",
    "\n",
    "async def main():\n",
    "    handler = agent_workflow.run(user_msg=\"\"\"\n",
    "        Write me a report on the history of the web. Briefly describe the history \n",
    "        of the world wide web, including the development of the internet and the \n",
    "        development of the web, including 21st century developments.\n",
    "    \"\"\")\n",
    "    \n",
    "    current_agent = None \n",
    "    current_tool_calls = \"\" \n",
    "    async for event in handler.stream_events():\n",
    "        if (\n",
    "            hasattr(event, \"current_agent_name\")\n",
    "            and event.current_agent_name != current_agent\n",
    "        ):\n",
    "            current_agent = event.current_agent_name\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"ğŸ¤– Agent: {current_agent}\")\n",
    "            print(f\"{'='*50}\\n\")\n",
    "        elif isinstance(event, AgentOutput):\n",
    "            if event.response.content:\n",
    "                print(\"ğŸ“¤ Output:\", event.response.content)\n",
    "            if event.tool_calls:\n",
    "                print(\n",
    "                    \"ğŸ› ï¸  Planning to use tools:\",\n",
    "                    [call.tool_name for call in event.tool_calls],\n",
    "                )\n",
    "        elif isinstance(event, ToolCallResult):\n",
    "            print(f\"ğŸ”§ Tool Result ({event.tool_name}):\")\n",
    "            print(f\"  Arguments: {event.tool_kwargs}\")\n",
    "            print(f\"  Output: {event.tool_output}\")\n",
    "        elif isinstance(event, ToolCall):\n",
    "            print(f\"ğŸ”¨ Calling Tool: {event.tool_name}\")\n",
    "            print(f\"  With arguments: {event.tool_kwargs}\")\n",
    "\n",
    "\"\"\"   .py\n",
    "if __name__ == '__main__':\n",
    "    import asyncio \n",
    "    asyncio.run(main())\n",
    "\"\"\"\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f0051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3319b4baaa30474e81ebdbeff9eb7e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Research done.\n",
      "âœ… Draft written.\n",
      "âœ… Review: approved=False\n",
      "âœ… Final approved=False\n",
      "\n",
      "================================================================================\n",
      "# æœ€ç»ˆæŠ¥å‘Šï¼ˆæˆªæ–­å±•ç¤ºï¼‰\n",
      "\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆè¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚\n",
      "### è¯·ç¡®ä¿è¾“å‡ºçš„æ¯ä¸ªè¦ç‚¹éƒ½ç¬¦åˆ\n",
      "\n",
      "# å®¡ç¨¿è§£æï¼š {'approved': False, 'summary': 'The manuscript requires significant revisions to enhance clarity, depth of analysis, and experimental details.', 'changes': ['Add detailed experimental procedures for the key methodologies used in the study.', 'Improve the statistical analysis by including more robust methods and reporting effect sizes.', 'Expand the discussion section to address potential limitations and alternative interpretations of the results.', 'Clarify the significance of the findings in the context of existing literature.', 'Refine the figure legends and add additional figures to support the main conclusions.']}\n"
     ]
    }
   ],
   "source": [
    "# æ˜¾ç¤ºå·¥ä½œæµå®‰æ’ï¼Œ å› ä¸ºç°åœ¨æˆ‘çš„æœ¬åœ°hf-llmä¸æ”¯æŒå·¥å…·/å‡½æ•°è°ƒç”¨ï¼ˆæ— function-callingï¼‰  -- runs ok  \n",
    "# ### ç¼ºç‚¹ï¼š ä¸­é—´ä¿¡æ¯å¤ç”¨çš„ä»£ç é€»è¾‘ä¸å¥½å†™ï¼Œæ‰€ä»¥è¿˜æ˜¯ç”¨llama-indexï¼ˆContextï¼‰\n",
    "# æœ¬åœ°hf-llmåªç”Ÿæˆæ–‡æœ¬ï¼ˆé€šè¿‡æç¤ºè¯æ§åˆ¶ ç”Ÿæˆæ–‡æœ¬ è¿™ä¸ªè¡Œä¸ºï¼‰ï¼Œ\n",
    "# å…¶ä½™çš„æˆ‘æ¥\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os, json, re, textwrap, requests\n",
    "\n",
    "# 1) æœ¬åœ°/ç¦»çº¿ LLMï¼ˆä¸éœ€è¦å·¥å…·è°ƒç”¨åè®®ï¼‰\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name     = r\"Qwen/Qwen3-1.7B\",\n",
    "    tokenizer_name = r\"Qwen/Qwen3-1.7B\",\n",
    "    context_window = 3900,\n",
    "    max_new_tokens = 640,\n",
    "    generate_kwargs={\"temperature\": 0.5, \"top_k\": 30, \"top_p\": 0.95},\n",
    "    device_map     = \"cpu\",\n",
    ")\n",
    "\n",
    "# 2) Tavily æœç´¢ï¼ˆç›´æ¥ RESTï¼›ä¸ä¾èµ– LlamaIndex çš„ Toolï¼‰\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "def tavily_search(query: str, max_results: int = 6, depth: str = \"advanced\"):\n",
    "    \"\"\"æ˜¾å¼è°ƒç”¨ Tavily çš„ REST APIã€‚\"\"\"\n",
    "    url = \"https://api.tavily.com/search\"\n",
    "    payload = {\n",
    "        \"api_key\": TAVILY_API_KEY,\n",
    "        \"query\": query,\n",
    "        \"search_depth\": depth,           # \"basic\" / \"advanced\"\n",
    "        \"include_images\": False,\n",
    "        \"include_answer\": False,\n",
    "        \"max_results\": max_results,\n",
    "    }\n",
    "    r = requests.post(url, json=payload, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    # æœŸæœ› data[\"results\"] æ˜¯ [{title, url, content, score?}, ...]\n",
    "    return data.get(\"results\", [])\n",
    "\n",
    "def _trim(s: str, n=8000):\n",
    "    return s if len(s) <= n else (s[:n] + \" ...[truncated]\")\n",
    "\n",
    "# --------------------- æ˜¾å¼â€œå·¥ä½œæµâ€æ­¥éª¤ ---------------------\n",
    "\n",
    "def step_research(topic: str) -> dict:\n",
    "    \"\"\"Step-1: æœç´¢ + ç”Ÿæˆç»“æ„åŒ–ç¬”è®°ï¼ˆæ˜¾å¼è°ƒç”¨ Tavily ä¸æœ¬åœ°LLMï¼‰\"\"\"\n",
    "    results = tavily_search(topic, max_results=6, depth=\"advanced\")\n",
    "    sources_md = \"\\n\".join(\n",
    "        f\"- [{it.get('title','(no title)')}]({it.get('url','')}) â€” {it.get('content','')[:240].replace('\\n', ' ')}\"\n",
    "        for it in results\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "ä½ æ˜¯ç ”ç©¶åŠ©ç†ã€‚åŸºäºä¸‹åˆ—æ£€ç´¢åˆ°çš„èµ„æ–™ï¼Œä¸ºâ€œ{topic}â€ç”Ÿæˆä¸è¶…è¿‡ 10 æ¡çš„è¦ç‚¹å¼ç ”ç©¶ç¬”è®°ï¼ˆMarkdownï¼‰ã€‚\n",
    "è¦æ±‚ï¼š\n",
    "- è¦†ç›–å…³é”®æ—¶é—´çº¿/é‡Œç¨‹ç¢‘/æ ¸å¿ƒäººç‰©æˆ–æœºæ„\n",
    "- çªå‡º 21 ä¸–çºªçš„å‘å±•ï¼ˆè‹¥ç›¸å…³ï¼‰\n",
    "- å°½é‡å¼•ç”¨æ¥æºï¼ˆä»¥ [ç¼–å·] å½¢å¼å¯¹ç…§ä¸‹æ–¹â€œèµ„æ–™åˆ—è¡¨â€åºå·ï¼‰\n",
    "\n",
    "èµ„æ–™åˆ—è¡¨ï¼ˆæŒ‰é¡ºåºæ ‡å·ï¼‰ï¼š\n",
    "{sources_md}\n",
    "\n",
    "è¯·è¾“å‡ºï¼š\n",
    "### ç¬”è®°\n",
    "1. ...\n",
    "2. ...\n",
    "3. ...\n",
    "### å¼•ç”¨æ¥æº\n",
    "- [1] æ ‡é¢˜ï¼ˆé“¾æ¥ï¼‰\n",
    "- [2] ...\n",
    "\"\"\"\n",
    "    notes = llm.complete(prompt).text\n",
    "    state = {\n",
    "        \"topic\": topic,\n",
    "        \"search_results\": results,\n",
    "        \"notes_md\": notes,\n",
    "        \"sources_md\": sources_md,\n",
    "    }\n",
    "    return state\n",
    "\n",
    "def step_write_report(state: dict) -> dict:\n",
    "    \"\"\"Step-2: å†™æŠ¥å‘Šï¼ˆæ˜¾å¼æŠŠä¸Šä¸€æ­¥çš„ç¬”è®°ä¼ å…¥ LLMï¼‰\"\"\"\n",
    "    topic = state[\"topic\"]\n",
    "    notes_md = state[\"notes_md\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "ä½ æ˜¯ä¸€åæŠ€æœ¯å†™ä½œè€…ã€‚è¯·åŸºäºä¸‹æ–¹â€œç ”ç©¶ç¬”è®°â€ä¸ºä¸»é¢˜â€œ{topic}â€æ’°å†™ä¸€ç¯‡**ç»“æ„åŒ– Markdown æŠ¥å‘Š**ã€‚\n",
    "è¦æ±‚ï¼š\n",
    "- ç»“æ„ç¤ºä¾‹ï¼š# æ¦‚è§ˆ / ## æ—©æœŸå‘å±• / ## 1990s / ## 2000s / ## 2010s-2020s / ## å‚è€ƒèµ„æ–™\n",
    "- å†…å®¹å¿…é¡»**ç´§å¯†ä¾èµ–**ç ”ç©¶ç¬”è®°ï¼Œä¸è¦ç¼–é€ \n",
    "- è¯­è¨€ç®€æ´ï¼Œæ®µè½çŸ­å°\n",
    "\n",
    "--- ç ”ç©¶ç¬”è®° ---\n",
    "{notes_md}\n",
    "\"\"\"\n",
    "    report_md = llm.complete(prompt).text\n",
    "    state[\"report_md\"] = report_md\n",
    "    return state\n",
    "\n",
    "def _extract_json_block(text: str) -> dict:\n",
    "    \"\"\"ä»æ¨¡å‹è¾“å‡ºä¸­å°½é‡æå– JSONï¼ˆç¨³å¥è§£æï¼‰\"\"\"\n",
    "    # å…ˆæ‰¾ ```json ... ``` åŒ…è£¹\n",
    "    m = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", text, flags=re.S)\n",
    "    if not m:\n",
    "        # é€€åŒ–ï¼šæ‰¾ç¬¬ä¸€ä¸ª { ... } å—\n",
    "        m = re.search(r\"(\\{.*\\})\", text, flags=re.S)\n",
    "    if not m:\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(m.group(1))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def step_review(state: dict, strict: bool = False) -> dict:\n",
    "    \"\"\"Step-3: å®¡ç¨¿ï¼ˆè¾“å‡º JSONï¼šapproved/changesï¼‰\"\"\"\n",
    "    report_md = state[\"report_md\"]\n",
    "    prompt = f\"\"\"\n",
    "ä½ æ˜¯å®¡ç¨¿äººã€‚è¯·ä»…ä»¥ JSON æ–¹å¼ç»™å‡ºå®¡ç¨¿ç»“è®ºã€‚\n",
    "è§„åˆ™ï¼š\n",
    "- å­—æ®µï¼šapproved (bool), summary (string), changes (string[])\n",
    "- è‹¥ approved=falseï¼Œè¯·ç»™å‡º 3-6 æ¡å…·ä½“ä¿®æ”¹å»ºè®®\n",
    "- è¾“å‡ºå¿…é¡»æ”¾åœ¨ä¸€ä¸ª ```json å—ä¸­ï¼Œä¸è¦å‡ºç°å¤šä½™æ–‡å­—\n",
    "\n",
    "--- å¾…å®¡æŠ¥å‘Š ---\n",
    "{_trim(report_md, 7000)}\n",
    "\"\"\"\n",
    "    review_raw = llm.complete(prompt).text\n",
    "    parsed = _extract_json_block(review_raw)\n",
    "    # å®¹é”™ï¼šé»˜è®¤é€šè¿‡\n",
    "    approved = bool(parsed.get(\"approved\", True))\n",
    "    changes = parsed.get(\"changes\", [])\n",
    "    summary = parsed.get(\"summary\", \"OK\")\n",
    "\n",
    "    state[\"review\"] = {\n",
    "        \"raw\": review_raw,\n",
    "        \"parsed\": parsed,\n",
    "        \"approved\": approved,\n",
    "        \"changes\": changes,\n",
    "        \"summary\": summary,\n",
    "    }\n",
    "    return state\n",
    "\n",
    "def step_revise_if_needed(state: dict, max_rounds: int = 1) -> dict:\n",
    "    \"\"\"å¯é€‰ï¼šæ ¹æ®å®¡ç¨¿æ„è§è¿›è¡Œ 0~N è½®ä¿®è®¢\"\"\"\n",
    "    rounds = 0\n",
    "    while rounds < max_rounds and not state[\"review\"][\"approved\"]:\n",
    "        changes = state[\"review\"][\"changes\"]\n",
    "        report_md = state[\"report_md\"]\n",
    "        topic = state[\"topic\"]\n",
    "        notes_md = state[\"notes_md\"]\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "æ ¹æ®ä»¥ä¸‹å®¡ç¨¿æ„è§ä¿®è®¢æŠ¥å‘Šã€Š{topic}ã€‹ã€‚å¿…é¡»éµå¾ªå®¡ç¨¿æ¡ç›®é€æ¡ä¿®æ”¹ï¼š\n",
    "- å®¡ç¨¿æ„è§ï¼š\n",
    "{json.dumps(changes, ensure_ascii=False, indent=2)}\n",
    "\n",
    "é™åˆ¶ï¼š\n",
    "- ä»éœ€ä¸¥æ ¼ä¾èµ–â€œç ”ç©¶ç¬”è®°â€ï¼Œä¸è¦ç¼–é€ \n",
    "- ä¿æŒ Markdown ç»“æ„æ¸…æ™°\n",
    "\n",
    "--- ç ”ç©¶ç¬”è®° ---\n",
    "{notes_md}\n",
    "\n",
    "--- æ—§ç‰ˆæŠ¥å‘Š ---\n",
    "{_trim(report_md, 7000)}\n",
    "\"\"\"\n",
    "        new_report = llm.complete(prompt).text\n",
    "        state[\"report_md\"] = new_report\n",
    "        # é‡æ–°å®¡ç¨¿\n",
    "        state = step_review(state)\n",
    "        rounds += 1\n",
    "    return state\n",
    "\n",
    "# --------------------- é¡¶å±‚ orchestrator ---------------------\n",
    "\n",
    "def run_explicit_workflow(user_request: str, revise_rounds: int = 1) -> dict:\n",
    "    # 1) ç ”ç©¶\n",
    "    state = step_research(user_request)\n",
    "    print(\"âœ… Research done.\")\n",
    "\n",
    "    # 2) å†™ä½œ\n",
    "    state = step_write_report(state)\n",
    "    print(\"âœ… Draft written.\")\n",
    "\n",
    "    # 3) å®¡ç¨¿\n",
    "    state = step_review(state)\n",
    "    print(f\"âœ… Review: approved={state['review']['approved']}\")\n",
    "\n",
    "    # 4) éœ€è¦çš„è¯ï¼Œä¿®è®¢è‹¥å¹²è½®\n",
    "    state = step_revise_if_needed(state, max_rounds=revise_rounds)\n",
    "    print(f\"âœ… Final approved={state['review']['approved']}\")\n",
    "\n",
    "    return state\n",
    "\n",
    "# --------------------- è¿è¡Œç¤ºä¾‹ ---------------------\n",
    "if __name__ == \"__main__\":\n",
    "    topic = \"History of the World Wide Web and key 21st-century developments\"\n",
    "    final_state = run_explicit_workflow(topic, revise_rounds=1)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"# æœ€ç»ˆæŠ¥å‘Šï¼ˆæˆªæ–­å±•ç¤ºï¼‰\\n\")\n",
    "    print(_trim(final_state[\"report_md\"], 4000))\n",
    "    print(\"\\n# å®¡ç¨¿è§£æï¼š\", final_state[\"review\"][\"parsed\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4a4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ecb7c17324c4031b58b8e3031c6efed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "WorkflowConfigurationError",
     "evalue": "At least one Event of type StopEvent must be returned by any step.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mWorkflowConfigurationError\u001b[39m                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m     m = re.search(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m```json\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*(\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m{\u001b[39m\u001b[33m.*?\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m})\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*```\u001b[39m\u001b[33m\"\u001b[39m, text, flags=re.S) \u001b[38;5;129;01mor\u001b[39;00m re.search(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m{\u001b[39m\u001b[33m.*\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m})\u001b[39m\u001b[33m\"\u001b[39m, text, flags=re.S)\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m json.loads(m.group(\u001b[32m1\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m wf = \u001b[43mWorkflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;129m@step\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresearch\u001b[39m(ev: StartEvent) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m     40\u001b[39m     topic = ev.input[\u001b[33m\"\u001b[39m\u001b[33mtopic\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\workflows\\workflow.py:108\u001b[39m, in \u001b[36mWorkflow.__init__\u001b[39m\u001b[34m(self, timeout, disable_validation, verbose, service_manager, resource_manager, num_concurrent_runs)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28mself\u001b[39m._disable_validation = disable_validation\n\u001b[32m    107\u001b[39m \u001b[38;5;28mself\u001b[39m._num_concurrent_runs = num_concurrent_runs\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[38;5;28mself\u001b[39m._stop_event_class = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ensure_stop_event_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;28mself\u001b[39m._start_event_class = \u001b[38;5;28mself\u001b[39m._ensure_start_event_class()\n\u001b[32m    110\u001b[39m \u001b[38;5;28mself\u001b[39m._sem = (\n\u001b[32m    111\u001b[39m     asyncio.Semaphore(num_concurrent_runs) \u001b[38;5;28;01mif\u001b[39;00m num_concurrent_runs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    112\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\developer\\miniconda\\envs\\langchain\\Lib\\site-packages\\workflows\\workflow.py:167\u001b[39m, in \u001b[36mWorkflow._ensure_stop_event_class\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_found == \u001b[32m0\u001b[39m:\n\u001b[32m    166\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mAt least one Event of type StopEvent must be returned by any step.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m WorkflowConfigurationError(msg)\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m num_found > \u001b[32m1\u001b[39m:\n\u001b[32m    169\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOnly one type of StopEvent is allowed per workflow, found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstop_events_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mWorkflowConfigurationError\u001b[39m: At least one Event of type StopEvent must be returned by any step."
     ]
    }
   ],
   "source": [
    "# ç”¨ LlamaIndex çš„ Workflowï¼ˆä»æ˜¯æ˜¾å¼è·¯å¾„ï¼Œä¸è®©æ¨¡å‹æŒ‘å·¥å…·ï¼‰\n",
    "# explicit_workflow_llamaindex.py  â€”â€” æ˜¾å¼ç¼–æ’ï¼Œæ—  function-calling\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os, json, re, requests\n",
    "from typing import Union\n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.workflow import (\n",
    "    Workflow, StartEvent, StopEvent, step, Event\n",
    ")\n",
    "\n",
    "# ================== åŸºç¡€ç»„ä»¶ ==================\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name     = r\"Qwen/Qwen3-1.7B\",\n",
    "    tokenizer_name = r\"Qwen/Qwen3-1.7B\",\n",
    "    context_window = 3900,\n",
    "    max_new_tokens = 640,\n",
    "    generate_kwargs={\"temperature\": 0.5, \"top_k\": 30, \"top_p\": 0.95},\n",
    "    device_map     = \"cpu\",\n",
    ")\n",
    "\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "def tavily_search(query: str, max_results: int = 6):\n",
    "    url = \"https://api.tavily.com/search\"\n",
    "    payload = {\n",
    "        \"api_key\": TAVILY_API_KEY,\n",
    "        \"query\": query,\n",
    "        \"search_depth\": \"advanced\",\n",
    "        \"max_results\": max_results\n",
    "    }\n",
    "    resp = requests.post(url, json=payload, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json().get(\"results\", [])\n",
    "\n",
    "def _extract_json_block(text: str) -> dict:\n",
    "    m = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", text, flags=re.S) or re.search(r\"(\\{.*\\})\", text, flags=re.S)\n",
    "    if not m:\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(m.group(1))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "# ================== äº‹ä»¶ç±»å‹ï¼ˆæ˜¾å¼å®šä¹‰ï¼‰ ==================\n",
    "class ResearchDone(Event):\n",
    "    topic: str\n",
    "    sources_md: str\n",
    "    notes_md: str\n",
    "\n",
    "class DraftWritten(Event):\n",
    "    topic: str\n",
    "    notes_md: str\n",
    "    report_md: str\n",
    "\n",
    "class ReviewApproved(Event):\n",
    "    topic: str\n",
    "    notes_md: str\n",
    "    report_md: str\n",
    "    review: dict  # {\"approved\": True, \"summary\": \"...\", \"changes\": []}\n",
    "\n",
    "class ReviewChangesNeeded(Event):\n",
    "    topic: str\n",
    "    notes_md: str\n",
    "    report_md: str\n",
    "    review: dict  # {\"approved\": False, \"summary\": \"...\", \"changes\": [...]}\n",
    "\n",
    "# ================== Workflow & Steps ==================\n",
    "class wf(Workflow):\n",
    "    # å› ä¸ºæ˜¯ä»å‰å¾€åï¼Œå•çº¿ï¼Œ æ‰€ä»¥ä¸ç”¨Contextä¹Ÿå¯ä»¥\n",
    "    @step\n",
    "    def research(ev: StartEvent) -> ResearchDone:\n",
    "        topic = ev.input[\"topic\"]\n",
    "        results = tavily_search(topic, max_results=6)\n",
    "        sources_md = \"\\n\".join(\n",
    "            f\"- [{it.get('title','(no title)')}]({it.get('url','')}) â€” {it.get('content','')[:240].replace('\\n',' ')}\"\n",
    "            for it in results\n",
    "        )\n",
    "        prompt = f\"è¯·åŸºäºä¸‹åˆ—èµ„æ–™ä¸ºâ€œ{topic}â€ç”Ÿæˆ 6-10 æ¡è¦ç‚¹å¼ Markdown ç¬”è®°ï¼š\\n{sources_md}\"\n",
    "        notes_md = llm.complete(prompt).text\n",
    "        return ResearchDone(topic=topic, sources_md=sources_md, notes_md=notes_md)\n",
    "\n",
    "    @step\n",
    "    def write(ev: ResearchDone) -> DraftWritten:\n",
    "        topic, notes_md = ev.topic, ev.notes_md\n",
    "        prompt = f\"åŸºäºç ”ç©¶ç¬”è®°ä¸ºâ€œ{topic}â€å†™ä¸€ç¯‡ç»“æ„åŒ– Markdown æŠ¥å‘Šï¼š\\n{notes_md}\"\n",
    "        report_md = llm.complete(prompt).text\n",
    "        return DraftWritten(topic=topic, notes_md=notes_md, report_md=report_md)\n",
    "\n",
    "    @step\n",
    "    def review(ev: DraftWritten) -> Union[ReviewApproved, ReviewChangesNeeded]:\n",
    "        report_md = ev.report_md\n",
    "        prompt = f\"\"\"ä½ æ˜¯å®¡ç¨¿äººï¼Œä»…ä»¥ JSON å›ç­”ï¼š{{\"approved\": true/false, \"summary\": \"...\", \"changes\": [\"...\"]}}ã€‚\n",
    "    æŠ¥å‘Šï¼š\n",
    "    {report_md}\n",
    "    \"\"\"\n",
    "        parsed = _extract_json_block(llm.complete(prompt).text)\n",
    "        if not parsed:\n",
    "            parsed = {\"approved\": True, \"summary\": \"OK\", \"changes\": []}\n",
    "\n",
    "        if bool(parsed.get(\"approved\", True)):\n",
    "            return ReviewApproved(\n",
    "                topic=ev.topic, notes_md=ev.notes_md, report_md=report_md, review=parsed\n",
    "            )\n",
    "        else:\n",
    "            return ReviewChangesNeeded(\n",
    "                topic=ev.topic, notes_md=ev.notes_md, report_md=report_md, review=parsed\n",
    "            )\n",
    "\n",
    "    @step\n",
    "    def revise(ev: ReviewChangesNeeded) -> DraftWritten:\n",
    "        \"\"\"æŒ‰å®¡ç¨¿æ„è§ä¿®è®¢ä¸€è½®åï¼Œå›åˆ° reviewã€‚\"\"\"\n",
    "        changes = ev.review.get(\"changes\", [])\n",
    "        prompt = (\n",
    "            \"æ ¹æ®ä»¥ä¸‹å®¡ç¨¿æ„è§ä¿®è®¢æŠ¥å‘Šï¼ˆä¿æŒ Markdown ç»“æ„ï¼Œé€æ¡è½å®ï¼‰ï¼š\\n\"\n",
    "            f\"{json.dumps(changes, ensure_ascii=False, indent=2)}\\n\"\n",
    "            \"---\\næ—§ç‰ˆï¼š\\n\"\n",
    "            f\"{ev.report_md}\"\n",
    "        )\n",
    "        new_report = llm.complete(prompt).text\n",
    "        return DraftWritten(topic=ev.topic, notes_md=ev.notes_md, report_md=new_report)\n",
    "\n",
    "    @step\n",
    "    def end(ev: ReviewApproved) -> StopEvent:\n",
    "        \"\"\"ç»ˆæ­¢ï¼šè¿”å›æœ€ç»ˆæŠ¥å‘Šä¸å®¡ç¨¿ç»“æœã€‚\"\"\"\n",
    "        return StopEvent(result={\"report_md\": ev.report_md, \"review\": ev.review})\n",
    "\n",
    "\n",
    "# =============== è¿è¡Œä¸æ‰“å° ===============\n",
    "handler = wf.run(input={\"topic\": \"History of the World Wide Web and 21st-century developments\"})\n",
    "\n",
    "# å¯é€‰ï¼šæ‰“å°äº‹ä»¶æµ\n",
    "for ev in handler.stream_events():\n",
    "    name = type(ev).__name__\n",
    "    payload = getattr(ev, \"result\", None)\n",
    "    if payload:\n",
    "        print(f\"[{name}] result keys: {list(payload) if isinstance(payload, dict) else str(payload)[:60]}\")\n",
    "    else:\n",
    "        print(f\"[{name}]\")\n",
    "\n",
    "final = handler.get()  # StopEvent.result\n",
    "print(\"\\n=== FINAL REPORT (snippet) ===\\n\")\n",
    "print(final[\"report_md\"][:2000])\n",
    "print(\"\\n=== REVIEW ===\\n\", json.dumps(final[\"review\"], ensure_ascii=False, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea779dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file \n",
    "\n",
    "from pathlib import Path \n",
    "\n",
    "pdf_root = r\"./log/simplePDF\"    # ~root/subdir/ ..data\n",
    "imPDF_root = r\"./log/ImagePDF\"   # ~root/subdir/ ..data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031680bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd70887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "project_root = Path(__file__).parent.parent\n",
    "file_md = r\"\"\n",
    "file_pdf = r\"\"\n",
    "\n",
    "\n",
    "file_md_im = r\"\"\n",
    "file_pdf_im = r\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15b7e3",
   "metadata": {},
   "source": [
    "### éƒ¨ç½²  llama_deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2254e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
