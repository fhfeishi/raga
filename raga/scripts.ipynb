{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Llama-index 是构建上下文增强LLM应用的框架\n",
    "# LlamaIndex 对您如何使用 LLM 没有限制。您可以将 LLM 用作自动完成、聊天机器人、代理等等。它只是让使用它们变得更容易。我们提供的工具包括：\n",
    "# - 数据连接器从其原生源和格式摄取您的现有数据。这些可以是 API、PDF、SQL 以及（更多）其他来源。\n",
    "# - 数据索引以易于 LLM 消费且性能良好的中间表示形式来组织您的数据。\n",
    "# - 引擎提供对您数据的自然语言访问。例如\n",
    "#   - 查询引擎是强大的问答界面（例如，一个 RAG 流程）。\n",
    "#   - 聊天引擎是用于与您的数据进行多消息、“来回”交互的对话界面。\n",
    "# - 代理是 LLM 驱动的知识工作者，并通过工具进行增强，这些工具从简单的辅助函数到 API 集成等等。\n",
    "# - 可观测性/评估集成使您能够在良性循环中严格地实验、评估和监控您的应用程序。\n",
    "# - 工作流允许您将以上所有功能组合到一个事件驱动系统中，这比其他基于图的方法更加灵活。\n",
    "\n",
    "\n",
    "#### 大语言模型（LLMs）\n",
    "# LLMs 是 LlamaIndex 诞生的根本创新。它们是一种人工智能 (AI) 计算机系统，能够理解、生成和处理自然语言，\n",
    "# 包括根据其训练数据或在查询时提供给它们的数据回答问题。\n",
    "\n",
    "\n",
    "#### 代理应用\n",
    "# 当 LLM 在应用程序中使用时，它通常用于做出决策、采取行动和/或与世界交互。这是代理应用的核心定义。\n",
    "# 尽管代理应用的定义很广泛，但有几个关键特征定义了代理应用\n",
    "## LLM 增强：LLM 通过工具（即代码中任意可调用的函数）、内存和/或动态提示进行增强。\n",
    "## 提示链：使用多个相互构建的 LLM 调用，一个 LLM 调用的输出用作下一个调用的输入。\n",
    "## 路由：LLM 用于将应用程序路由到应用程序中的下一个适当的步骤或状态。\n",
    "## 并行性：应用程序可以并行执行多个步骤或操作。\n",
    "## 编排：使用 LLM 的层级结构来编排较低级别的操作和 LLM。\n",
    "## 反思：LLM 用于反思和验证前一步骤或 LLM 调用的输出，这可以用来指导应用程序进入下一个适当的步骤或状态。\n",
    "#  在 LlamaIndex 中，您可以使用 Workflow 类来编排一系列步骤和 LLMs，从而构建代理应用\n",
    "\n",
    "\n",
    "#### 代理\n",
    "# 我们将代理定义为“代理应用”的一个具体实例。\n",
    "# 代理是一种软件，通过将 LLMs 与其他工具和内存结合，在推理循环中自主地执行任务，该循环决定接下来使用哪个工具（如果需要）。\n",
    "# 这在实践中意味着：\n",
    "# - 代理接收用户消息 \n",
    "# - 代理使用 LLM，结合先前的聊天历史、工具和最新的用户消息来确定要采取的下一个适当行动 \n",
    "# - 代理可能会调用一个或多个工具来协助处理用户的请求 \n",
    "# - 如果使用了工具，代理将解释工具输出并用其指导下一个行动 \n",
    "# - 一旦代理停止采取行动，它会将最终输出返回给用户\n",
    "\n",
    "\n",
    "#### 使用案例\n",
    "# 数据支持的 LLM 应用有无数的使用案例，但大致可以分为四类\n",
    "# - 代理：代理是由 LLM 驱动的自动化决策器，通过一套工具与世界交互。\n",
    "#        代理可以执行任意数量的步骤来完成给定任务，动态决定最佳行动方案，而不是遵循预设步骤。\n",
    "#        这使其具有额外的灵活性来处理更复杂的任务。\n",
    "# - 工作流：LlamaIndex 中的工作流是一种特定的事件驱动抽象，允许您编排一系列步骤和 LLMs 调用。\n",
    "#          工作流可用于实现任何代理应用，是 LlamaIndex 的核心组件。\n",
    "# - 结构化数据提取： Pydantic 提取器允许您指定要从数据中提取的精确数据结构，并以类型安全的方式使用 LLMs 填充缺失的部分。\n",
    "#                这对于从 PDF、网站等非结构化源中提取结构化数据非常有用，也是自动化工作流的关键。\n",
    "# - 查询引擎：查询引擎是一个端到端流程，允许您对数据提出问题。它接收自然语言查询，并返回响应以及检索到的并传递给 LLM 的参考上下文。\n",
    "# - 聊天引擎：聊天引擎是一个端到端流程，用于与您的数据进行对话（多次往返而不是单一问答）。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 构建Agent  代理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import FunctionAgent \n",
    "# FunctionAgent\n",
    "# 创建工具函数  func_tools [func_a, func_b, func_c]\n",
    "# 初始化llm      llm \n",
    "# 初始化Agent   workflow=FunctionAgent(llm tools prompt ...)\n",
    "# # 提问   workflow.run(query, context)\n",
    "\n",
    "\n",
    "\n",
    "### 工具集 Llama-Cloud\n",
    "# from llama_index.tools.xx import xxx\n",
    "\n",
    "\n",
    "\n",
    "### 维护状态\n",
    "# 默认情况下，AgentWorkflow 在各次运行之间是无状态的。这意味着 Agent 不会记住先前的运行信息。\n",
    "# 为了维护状态，我们需要跟踪先前的状态。\n",
    "# 在 LlamaIndex 中，Workflow 有一个 Context 类，可用于在运行内部和运行之间维护状态。\n",
    "# 由于 AgentWorkflow 只是一个预构建的 Workflow，我们现在也可以使用它。\n",
    "from llama_index.core.workflow import Context \n",
    "# context = Context(workflow)\n",
    "### 维护更长的状态\n",
    "# Context 是可序列化的，因此可以保存到数据库、文件等位置，稍后重新加载。\n",
    "# JsonSerializer 是一个简单的序列化器，它使用 json.dumps 和 json.loads 来序列化和反序列化 Context。\n",
    "# JsonPickleSerializer 是一个使用 pickle 来序列化和反序列化 Context 的序列化器。如果你的 Context 中包含不可序列化的对象，可以使用此序列化器。\n",
    "from llama_index.core.workflow import JsonPickleSerializer, JsonSerializer\n",
    "# Context 序列化为字典并保存到文件中\n",
    "# cdx_dict = context.to_dict(serializer=JsonSerializer())\n",
    "# restored_ctx = Context.from_dict(workflow, cdx_dict, serializer=JsonSerializer())\n",
    "Context.get()\n",
    "### 工具与状态\n",
    "# 工具也可以定义为可以访问 Workflow Context。这意味着你可以从 Context 中设置和检索变量并在工具中使用它们，或者在工具之间传递信息。\n",
    "# AgentWorkflow 使用一个名为 state 的 Context 变量，该变量对每个 Agent 都可用。你可以依赖 state 中的信息而无需显式传入。\n",
    "from llama_index.core.agent.workflow import AgentWorkflow \n",
    "\"\"\"\n",
    "# 要访问 Context，Context 参数应该是工具的第一个参数，就像我们在这里所做的，在一个简单地向状态添加名称的工具中\n",
    "def set_name(ctx: Context, name: str) -> str:\n",
    "    state = ctx.store.get(\"state\")\n",
    "    state[\"name\"] = name \n",
    "    ctx.set(\"state\", state)\n",
    "\n",
    "# 把普通的 Python 函数包装成 可被 LLM 调用的工具\n",
    "workflow = AgentWorkflow.from_tools_or_functions([set_name], \n",
    "                                                 llm=llm, \n",
    "                                                 system_prompt=\"xxx\",\n",
    "                                                 initial_state={\"name\":\"unset\"})\n",
    "                                                 \n",
    "就是说 workflow_1  就是将变量x变成 123\n",
    "再套一个 workflow = AgentWorkflow.from_tools_or_functions(..) # 变量x变成了什么xx\n",
    "\"\"\"\n",
    "\n",
    "#### 流式输出和事件 \n",
    "# 在实际应用中，代理可能需要很长时间才能运行。向用户提供关于代理进度的反馈至关重要，而流式传输可以实现这一点。\n",
    "# AgentWorkflow 提供了一系列预构建的事件，你可以使用它们向用户流式传输输出。让我们看看如何做到这一点。\n",
    "# 首先，我们将介绍一个需要一些时间执行的新工具。在这种情况下，我们将使用一个名为 Tavily 的网络搜索工具，它在 LlamaHub 中可用。\n",
    "# llama-index-tools-tavily-research  pypi\n",
    "# 它需要一个 API 密钥，我们将在 .env 文件中将其设置为 TAVILY_API_KEY \n",
    "from llama_index.tools.tavily_research import TavilyToolSpec\n",
    "import os\n",
    "# 初始化工具\n",
    "tavily_tool = TavilyToolSpec(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "# 初始化代理\n",
    "workflow = FunctionAgent(tools=tavily_tool.to_tool_list(),\n",
    "                         ) # llm, system_prompts \n",
    "# 在之前的示例中，我们使用 await 在 workflow.run 方法上获取代理的最终响应。\n",
    "# 然而，如果我们不对响应进行等待，我们将获得一个异步迭代器，我们可以迭代该迭代器来获取传入的事件。这个迭代器将返回各种事件。\n",
    "# 我们将从一个 AgentStream 事件开始，它包含输出传入时的“增量”（最新的变化）。我们需要导入该事件类型\n",
    "from llama_index.core.agent.workflow import AgentStream\n",
    "handler = workflow.run(user_msg=\"xxxxx\")\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, AgentStream):\n",
    "        print(event.delta, end=\"\", flush=True)\n",
    "# AgentStream 只是 AgentWorkflow 运行时发出的众多事件之一。其他的事件包括：\n",
    "# - AgentInput    : 开始代理执行的完整消息对象\n",
    "# - AgentOutput   : 来自代理的响应\n",
    "# - ToolCall      : 调用了哪些工具以及使用了哪些参数\n",
    "# - ToolCallResult: 工具调用的结果 \n",
    "\n",
    "\n",
    "#### 人在回路中\n",
    "# 工具也可以被定义为引入人在回路中。这对于需要人工输入的任务很有用，例如确认工具调用或提供反馈。\n",
    "# AgentWorkflow 的底层工作方式是通过运行既发出又接收事件的步骤来实现。\n",
    "# 为了引入人在回路中，我们将让工具发出一个工作流中其他任何步骤都不接收的事件。\n",
    "# 然后，我们将告诉工具等待，直到接收到特定的“回复”事件。\n",
    "# 我们有内置的 InputRequiredEvent 和 HumanResponseEvent 事件可用于此目的。\n",
    "# 如果您想捕获不同形式的人工输入，可以对这些事件进行子类化以满足您的偏好。我们来导入它们。\n",
    "from llama_index.core.workflow import (InputRequiredEvent, HumanResponseEvent)\n",
    "# 接下来，我们将创建一个执行假定危险任务的工具。这里有几点新内容：\n",
    "# - wait_for_event 用于等待 HumanResponseEvent。\n",
    "# - waiter_event 是写入事件流的事件，用于告知调用者我们正在等待响应。\n",
    "# - waiter_id 是此特定等待调用的唯一标识符。它有助于确保我们对于每个 waiter_id 只发送一个 waiter_event。\n",
    "# - requirements 参数用于指定我们要等待具有特定 user_name 的 HumanResponseEvent。\n",
    "async def dangerous_task(ctx: Context) -> str:\n",
    "    # emit a waiter event (InputRequiredEvent)\n",
    "    # and wait until we see a HumanResponseEvent\n",
    "    question = \" .. \"\n",
    "    response = await ctx.wait_for_event(\n",
    "        HumanResponseEvent,\n",
    "        waiter_id=question,\n",
    "        waiter_event=InputRequiredEvent(\n",
    "            prefix=question,\n",
    "            user_name='Lauire',\n",
    "        )\n",
    "        requirements={'user_name': 'lauire'}\n",
    "    )\n",
    "    # act on input from the event\n",
    "    if response.response.strip().lower == \"yes\":\n",
    "        return \"dangerous task completed successfully\"\n",
    "    else:\n",
    "        return \"dangerous task aborted\"\n",
    "# 创建Agent，传入dangerous_task\n",
    "workflow = FunctionAgent(tools=[dangerous_task], ) # llm system_prompt\n",
    "# 现在运行工作流，像处理任何其他流式事件一样处理InputRequiredEvent, 并使用Send_event方法传入一个HumanResponseEvent进行响应\n",
    "handler = workflow.run(user_msg=\" .. \")      # 可等待对象，同时还能流式产出中间事件 \n",
    "async for event in handler.stream_events():\n",
    "    # capture keyboard input\n",
    "    response = input(event.prefix)\n",
    "    # send or response back \n",
    "    handler.ctx.send_event(\n",
    "        HumanResponseEvent(\n",
    "            response=response,\n",
    "            user_name=event.user_name,   \n",
    "        )\n",
    "    )\n",
    "response = await handler\n",
    "print(str(response))\n",
    "# 您可以使用任何方式来捕获输入；可以使用 GUI、音频输入，甚至可以引入另一个独立的 Agent。\n",
    "# 如果您的输入需要一段时间或在另一个进程中发生，您可能希望序列化上下文并将其保存到数据库或文件中，以便之后可以恢复工作流。\n",
    "\n",
    "\n",
    "#### AgentWorkflow的多智能体系统\n",
    "# AgentWorkflow 单个智能体、多智能体系统。 多智能体系统中多个智能体协作完成任务，并在需要时将控制权互相移交\n",
    "# - ResearchAgent  : 它将搜索网络一查找给定主题的信息\n",
    "# - WriteAgent     : 将用ResearchAgent检索到的信息来撰写报告\n",
    "# - ReviewAgent    : 将审查报告并提供反馈\n",
    "## 需要用到的工具\n",
    "# web_search工具，  Tavily\n",
    "# record_notes工具，将网络搜索道德研究保存到状态中（AgentWorkflow使用一个名为state的Context变量），然后其他工具就可以使用它\n",
    "# write_repot工具，使用ResearchAgent检索到的信息撰写报告\n",
    "# review_report工具，审查报告和提供反馈\n",
    "# < 2025-09-07-morning >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.构建工作流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7b5caf17634bf58b3a0913660bbf62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step one is happening.\n",
      " \n",
      "I \n",
      "need \n",
      "the \n",
      "text \n",
      "to \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "I \n",
      "need \n",
      "the \n",
      "text \n",
      "to \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "I \n",
      "need \n",
      "the \n",
      "text \n",
      "to \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first \n",
      "chapter \n",
      "of \n",
      "\n",
      "Moby \n",
      "\n",
      "Dick. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "first \n",
      "person \n",
      "\n",
      "perspective. \n",
      "The \n",
      "text \n",
      "should \n",
      "be \n",
      "in \n",
      "the \n",
      "style \n",
      "of \n",
      "the \n",
      "first\n",
      "step three is happening\n",
      "final result: workflow complete.\n",
      "streaming_workflow.html\n"
     ]
    }
   ],
   "source": [
    "### 什么是工作流？ \n",
    "# 工作流是一种事件驱动、基于步骤的方式，用于控制应用程序的执行流程。\n",
    "# 您的应用程序被划分为称为“步骤”（Steps）的部分，这些步骤由“事件”（Events）触发，并且本身也会发出事件，从而触发进一步的步骤。\n",
    "# 通过组合步骤和事件，您可以创建任意复杂、封装逻辑的流程，使您的应用程序更易于维护和理解。一个步骤可以是单行代码，也可以是复杂的智能体。\n",
    "# 它们可以具有任意输入和输出，这些输入和输出通过事件传递。\n",
    "### 为什么使用工作流 ？\n",
    "# 随着生成式 AI 应用变得越来越复杂，管理数据流和控制应用程序执行变得越来越困难。\n",
    "# 工作流提供了一种管理此复杂性的方法，将应用程序分解为更小、更易于管理的部分。\n",
    "## 其他框架和 LlamaIndex 本身之前曾尝试使用有向无环图（DAG）解决此问题，但这些方法存在一些工作流没有的限制：\n",
    "# - 循环和分支等逻辑需要编码到图的边缘中，这使得它们难以阅读和理解。\n",
    "# - 在 DAG 的节点之间传递数据会产生可选值和默认值以及应传递哪些参数的复杂性。\n",
    "# - 对于尝试开发复杂、循环、分支 AI 应用的开发者来说，DAG 并不自然。\n",
    "# \n",
    "# \n",
    "#### 基本工作流\n",
    "# 工作流内置于 LlamaIndex 核心中  \n",
    "# from llama_index.core.workflow ..\n",
    "# from llama_index.utils.workflow ..\n",
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    ")\n",
    "# \n",
    "#### 单步工作流\n",
    "# 工作流通常实现为一个继承自WorkFlow的类。该类可以定义为任意数量的步骤，每个步骤都是用 @step 装饰器修饰的方法：\n",
    "class MyWorkflow(Workflow):\n",
    "    @step \n",
    "    async def my_Step(self, ev: StartEvent) -> StopEvent:\n",
    "        # do something \n",
    "        return StopEvent(result=\"hello world!\")\n",
    "# w = MyWorkflow(timeout=100, verbose=False)\n",
    "# result = await w.run()\n",
    "# print(result)\n",
    "# \n",
    "# Start事件和Stop事件\n",
    "# StartEvent 和 StopEvent 是用于启动和停止工作流的特殊事件。\n",
    "# 任何接受 StartEvent 的步骤都会由 run 命令触发。\n",
    "# 发出 StopEvent 将结束工作流的执行并返回最终结果，即使其他步骤尚未执行。\n",
    "#\n",
    "#\n",
    "#### 在普通python中运行工作流\n",
    "# 工作流默认是异步的，async   ，因此您可以使用 await 获取 run 命令的结果。这在 Notebook 环境中运行良好\n",
    "# 在普通的 Python 脚本中，您需要导入 asyncio 并将代码包装在一个异步函数中，像这样\n",
    "\"\"\"  .py import asyncio\n",
    "import asyncio\n",
    "async def main():\n",
    "    w = MyWorkflow(timeout=100, verbose=False)\n",
    "    result = await w.run()\n",
    "    print(result)\n",
    "\"\"\"\n",
    "# \n",
    "# \n",
    "#### 可视化工作流  还不错\n",
    "#  工作流的一个很棒的功能是内置的可视化工具，我们已经安装了它。让我们可视化刚刚创建的简单工作流\n",
    "# from llama_index.utils.workflow import draw_all_possible_flows\n",
    "# draw_all_possible_flows(MyWorkflow, filename=\"basic_workflow.html\")\n",
    "# \n",
    "# \n",
    "#### 自定义事件\n",
    "# 通过定义可以由步骤发出并触发其他步骤的自定义事件来创建多个步骤。\n",
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    ")\n",
    "from llama_index.utils.workflow import draw_all_possible_flows \n",
    "class FirstEvent(Event):\n",
    "    first_output: str \n",
    "class SecondEvent(Event):\n",
    "    second_output: str \n",
    "# step_one 接受一个 StartEvent 并返回一个 FirstEvent\n",
    "# step_two 接受一个 FirstEvent 并返回一个 SecondEvent\n",
    "# step_three 接受一个 SecondEvent 并返回一个 StopEvent\n",
    "class MyWorkflowb(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent) -> FirstEvent:\n",
    "        print(ev.first_input)\n",
    "        return FirstEvent(first_output=\"first step complete.\")    \n",
    "    \n",
    "    @step\n",
    "    async def step_two(self, ev:FirstEvent) -> SecondEvent:\n",
    "        print(ev.first_output)\n",
    "        return SecondEvent(second_output=\"second step complete.\")\n",
    "    \n",
    "    @step \n",
    "    async def step_three(self, ev:SecondEvent) -> StopEvent:\n",
    "        print(ev.second_output)\n",
    "        return StopEvent(result=\"Worflow complete.\")\n",
    "    \n",
    "# w = MyWorkflowb(timeout=10, verbose=False)\n",
    "# result = await w.run(first_input=\"start the workflow.\")\n",
    "# print(result)\n",
    "# #  hello world!\n",
    "# # start the workflow.\n",
    "# # first step complete.\n",
    "# # second step complete.\n",
    "# # Worflow complete.\n",
    "# from llama_index.utils.workflow import draw_all_possible_flows\n",
    "# draw_all_possible_flows(MyWorkflowb, filename=\"multi_step_workflow.html\")\n",
    "# \n",
    "# \n",
    "#### 分支和循环\n",
    "# 工作流的一个关键特性是它们能够实现分支和循环逻辑，比基于图的方法更简单、更灵活。\n",
    "# \n",
    "### 工作流中的循环 \n",
    "# 要创建一个循环，我们将使用前一个教程中的示例 `MyWorkflow` 并添加一个新的自定义事件类型。\n",
    "# 我们将它命名为 `LoopEvent`，但同样它可以是任何任意名称。\n",
    "import random \n",
    "class LoopEvent(Event):\n",
    "    loop_output: str \n",
    "\n",
    "class MyWorkflowc(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent | LoopEvent) -> FirstEvent | LoopEvent:\n",
    "        if random.randint(0,1) == 1:\n",
    "            print(\"bad thing happened.\")\n",
    "            return LoopEvent(loop_output=\"back to step one.\")\n",
    "        else:\n",
    "            print(\"good thing happened.\")\n",
    "            return FirstEvent(first_output=\"first step complete.\")  \n",
    "    \n",
    "    @step\n",
    "    async def step_two(self, ev:FirstEvent) -> SecondEvent:\n",
    "        print(ev.first_output)\n",
    "        return SecondEvent(second_output=\"second step complete.\")\n",
    "    \n",
    "    @step \n",
    "    async def step_three(self, ev:SecondEvent) -> StopEvent:\n",
    "        print(ev.second_output)\n",
    "        return StopEvent(result=\"Worflow complete.\")\n",
    "# w = MyWorkflowb(timeout=10, verbose=False)\n",
    "# result = await w.run(first_input=\"start the workflow.\")\n",
    "# print(result)\n",
    "# from llama_index.utils.workflow import draw_all_possible_flows\n",
    "# draw_all_possible_flows(MyWorkflowc, filename=\"multi_step_loop_workflowc.html\")\n",
    "# \n",
    "# \n",
    "### 工作流中的分支 \n",
    "# 与循环密切相关的是分支。正如您已经看到的，您可以有条件地返回不同的事件。让我们看一个分支到两个不同路径的工作流。\n",
    "class BranchA1Event(Event):\n",
    "    payload: str\n",
    "\n",
    "\n",
    "class BranchA2Event(Event):\n",
    "    payload: str\n",
    "\n",
    "\n",
    "class BranchB1Event(Event):\n",
    "    payload: str\n",
    "\n",
    "\n",
    "class BranchB2Event(Event):\n",
    "    payload: str\n",
    "\n",
    "\n",
    "class BranchWorkflow(Workflow):\n",
    "    @step\n",
    "    async def start(self, ev: StartEvent) -> BranchA1Event | BranchB1Event:\n",
    "        if random.randint(0, 1) == 0:\n",
    "            print(\"Go to branch A\")\n",
    "            return BranchA1Event(payload=\"Branch A\")\n",
    "        else:\n",
    "            print(\"Go to branch B\")\n",
    "            return BranchB1Event(payload=\"Branch B\")\n",
    "\n",
    "    @step\n",
    "    async def step_a1(self, ev: BranchA1Event) -> BranchA2Event:\n",
    "        print(ev.payload)\n",
    "        return BranchA2Event(payload=ev.payload)\n",
    "\n",
    "    @step\n",
    "    async def step_b1(self, ev: BranchB1Event) -> BranchB2Event:\n",
    "        print(ev.payload)\n",
    "        return BranchB2Event(payload=ev.payload)\n",
    "\n",
    "    @step\n",
    "    async def step_a2(self, ev: BranchA2Event) -> StopEvent:\n",
    "        print(ev.payload)\n",
    "        return StopEvent(result=\"Branch A complete.\")\n",
    "\n",
    "    @step\n",
    "    async def step_b2(self, ev: BranchB2Event) -> StopEvent:\n",
    "        print(ev.payload)\n",
    "        return StopEvent(result=\"Branch B complete.\")\n",
    "# w = BranchWorkflow(timeout=10, verbose=False)\n",
    "# result = await w.run(first_input=\"start the workflow.\")\n",
    "# print(result)\n",
    "# from llama_index.utils.workflow import draw_all_possible_flows\n",
    "# draw_all_possible_flows(BranchWorkflow, filename=\"multi_step_branch_workflow.html\")   \n",
    "#\n",
    "# \n",
    "#  \n",
    "#### 维护状态\n",
    "# 在目前位置的示例中，我们使用自定义事件的属性在步骤间传递数据，这是一种强大的数据传递方式，但也有局限性。\n",
    "# 例如，我想在不直接相连的步骤之间传递数据，则需要通过中间的所有步骤来传递数据，这会让代码难写难以阅读和维护。\n",
    "#  AgentWorkflow 的 state  ： Context  共享的。\n",
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    "    Context,\n",
    ")\n",
    "# Start 事件，检查数据是否已加载到上下文中。如果还没有，返回一个SetupEvent，该事件除法setup函数加载数据并返回start\n",
    "class SetupEvent(Event):\n",
    "    query: str \n",
    "class StepTwoEvent(Event):\n",
    "    query: str \n",
    "\n",
    "class StatefulFlow(Workflow):\n",
    "    @step \n",
    "    async def start(\n",
    "        self, ctx:Context, ev: StartEvent\n",
    "    ) -> SetupEvent | StepTwoEvent:\n",
    "        db = await ctx.store.get(\"some_database\", default=None) \n",
    "        if db is None:\n",
    "            print(\"need load data.\")\n",
    "            return SetupEvent(query=ev.query)\n",
    "        # do something \n",
    "        return StepTwoEvent(query=ev.query)\n",
    "    \n",
    "    @step\n",
    "    async def setup(self, ctx:Context, ev:SetupEvent) -> StartEvent:\n",
    "        # load data \n",
    "        await ctx.store.set(\"some_database\", [1,2,3])\n",
    "        return StartEvent(query=ev.query)\n",
    "    \n",
    "    @step \n",
    "    async def step_two(self, ctx:Context, ev:StepTwoEvent) -> StopEvent:\n",
    "        # do something there \n",
    "        print(\"Data is :\", await ctx.store.get(\"some_database\"))\n",
    "        \n",
    "        return StopEvent(result=await ctx.store.get(\"some_database\"))\n",
    "    \n",
    "# w = StatefulFlow(timeout=10, verbose=False)\n",
    "# result = await w.run(query=\"Some query\")\n",
    "# print(result)\n",
    "# from llama_index.utils.workflow import draw_all_possible_flows\n",
    "# draw_all_possible_flows(StatefulFlow, filename=\"multi_step_state_workflow.html\")  \n",
    "#\n",
    "#\n",
    "#### 流式事件\n",
    "# 弘佐六可能很复杂， 它们旨在处理复杂、分支、并行的逻辑，这意味着它们可能需要一些时间才能完全执行。\n",
    "# 为了给用户提供良好的体验，通常会希望通过流式事件来提供进度指示。工作流在Context对象上内置了对此的支持。\n",
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    "    Context\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "# local_chat_model\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM \n",
    "chat_model = HuggingFaceLLM(\n",
    "    # cache_root可以省，model_name\n",
    "    # or  local_dir\n",
    "    model_name     = r\"Qwen/Qwen3-1.7B\",\n",
    "    tokenizer_name = r\"Qwen/Qwen3-1.7B\",\n",
    "    context_window = 3900,  \n",
    "    max_new_tokens = 640,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"top_k\": 30, \"top_p\": 0.95},\n",
    "    device_map     ='cpu' \n",
    ")\n",
    "\n",
    "\n",
    "# 三步工作流，再加上一个事件来处理进行时的进度流式传输，\n",
    "class FirstEvent(Event):\n",
    "    first_output: str \n",
    "class SecondEvent(Event):\n",
    "    second_output: str \n",
    "    response: str \n",
    "class ProgressEvent(Event):\n",
    "    msg: str \n",
    "# 发送事件的工作流类\n",
    "class MyWorkflowd(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ctx:Context, ev:StartEvent)->FirstEvent:\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Step one is happening.\"))\n",
    "        return FirstEvent(first_output=\"first step copmplete\")\n",
    "    \n",
    "    @step \n",
    "    async def step_two(self, ctx:Context, ev:FirstEvent)->SecondEvent:\n",
    "        generator = await chat_model.astream_complete(\n",
    "            \"Please give me the first 3 paragraphs of Moby Dick, a book in the public domain.\"\n",
    "        )\n",
    "        async for response in generator:\n",
    "            # allow the workflow to stream  this piece of response\n",
    "            ctx.write_event_to_stream(ProgressEvent(msg=response.delta))\n",
    "        return SecondEvent(\n",
    "            second_output=\"second step complete\",\n",
    "            response=str(response),\n",
    "        )\n",
    "\n",
    "    @step \n",
    "    async def step_three(self, ctx:Context, ev:SecondEvent)->StopEvent:\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"step three is happening\"))\n",
    "        return StopEvent(result=\"workflow complete.\")\n",
    "\n",
    "# w = MyWorkflowd(timeout=1000, verbose=False)\n",
    "# handler = w.run(first_input = \"Start the workflow\")\n",
    "# async for ev in handler.stream_events():\n",
    "#     if isinstance(ev, ProgressEvent):\n",
    "#         print(ev.msg)\n",
    "#\n",
    "# final_result = await handler\n",
    "# print(\"final result:\", final_result)\n",
    "# draw_all_possible_flows(MyWorkflowd, filename=\"streaming_workflow.html\")\n",
    "# \n",
    "# \n",
    "# \n",
    "#### 并行执行\n",
    "# 工作流还可以并行执行\n",
    "# 当您有多个步骤可以独立运行且它们包含耗时的 await 操作时，并发执行非常有用，这允许其他步骤并行运行。\n",
    "### 发射多个事件\n",
    "# 一个步骤可以发射多个事件（之前的例子中都是返回一个事件），发射多个事件  end_event\n",
    "class ParallelFlow(Workflow):\n",
    "    @step \n",
    "    async def start(self, ctx:Context, ev:StartEvent) -> StepTwoEvent:\n",
    "        ctx.send_event(StepTwoEvent(query=\"q 1\"))\n",
    "        ctx.send_event(StepTwoEvent(query=\"q 2\"))\n",
    "        ctx.send_event(StepTwoEvent(query=\"q 3\"))\n",
    "    \n",
    "    @step(num_workers=4)\n",
    "    async def step_two(self, ctx:Context, ev:StepTwoEvent) -> StopEvent:\n",
    "        print('Running slow query', ev.query)\n",
    "        return StopEvent(result=ev.query)\n",
    "# start 步骤发射 3 个 StepTwoEvent。step_two 步骤使用 num_workers=4 进行装饰，这会告诉工作流最多并发运行此步骤的 4 个实例（这是默认值）\n",
    "# 执行，工作流会在第一个完成的查询之后停止。\n",
    "#\n",
    "### 收集事件\n",
    "# 如果希望等待所有耗时操作完成后再继续下一步\n",
    "# 以使用 collect_events 来实现这一点。\n",
    "class StepThreeEvent(Event):\n",
    "    query: str \n",
    "    \n",
    "class ConcurrentFlow(Workflow):\n",
    "    @step \n",
    "    async def start(self, ctx:Context, ev:StartEvent) -> StepTwoEvent:\n",
    "        ctx.send_event(StepTwoEvent(query=\"q 1\"))\n",
    "        ctx.send_event(StepTwoEvent(query=\"q 2\"))\n",
    "        ctx.send_event(StepTwoEvent(query=\"q 3\"))\n",
    "    \n",
    "    @step(num_wokers=4)\n",
    "    async def ste_two(self, ctx:Context, ev:StepTwoEvent) -> StepThreeEvent:\n",
    "        print(\"Running query \", ev.query)\n",
    "        return StepThreeEvent(result=ev.query)\n",
    "    \n",
    "    @step\n",
    "    async def step_three(self, ctx: Context, ev: StepThreeEvent) -> StopEvent:\n",
    "        # wait until we receive 3 events\n",
    "        result = ctx.collect_events(ev, [StepThreeEvent] * 3)\n",
    "        if result is None:\n",
    "            return None\n",
    "        # do something with all 3 results together\n",
    "        print(result)\n",
    "        return StopEvent(result=\"Done\")\n",
    "# collect_events 方法位于 Context 上，它接受触发该步骤的事件以及要等待的事件类型数组。\n",
    "# 在此示例中，我们正在等待 3 个相同 StepThreeEvent 类型的事件。\n",
    "# step_three 步骤在每次收到 StepThreeEvent 时触发，但 collect_events 将返回 None，直到收到所有 3 个事件。\n",
    "# 此时，该步骤将继续执行，您可以一起处理所有 3 个结果。\n",
    "# 从 collect_events 返回的 result 是一个包含收集到的事件的数组，其顺序与收到事件的顺序一致。  \n",
    "# \n",
    "# \n",
    "### 多种事件类型 \n",
    "# 当然，您不必等待相同类型的事件。您可以等待任何您喜欢的事件组合，例如在此示例中\n",
    "\"\"\"  \n",
    "class ConcurrentFlow(Workflow):\n",
    "    @step\n",
    "    async def start(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> StepAEvent | StepBEvent | StepCEvent:\n",
    "        ctx.send_event(StepAEvent(query=\"Query 1\"))\n",
    "        ctx.send_event(StepBEvent(query=\"Query 2\"))\n",
    "        ctx.send_event(StepCEvent(query=\"Query 3\"))\n",
    "\n",
    "    @step\n",
    "    async def step_a(self, ctx: Context, ev: StepAEvent) -> StepACompleteEvent:\n",
    "        print(\"Doing something A-ish\")\n",
    "        return StepACompleteEvent(result=ev.query)\n",
    "\n",
    "    @step\n",
    "    async def step_b(self, ctx: Context, ev: StepBEvent) -> StepBCompleteEvent:\n",
    "        print(\"Doing something B-ish\")\n",
    "        return StepBCompleteEvent(result=ev.query)\n",
    "\n",
    "    @step\n",
    "    async def step_c(self, ctx: Context, ev: StepCEvent) -> StepCCompleteEvent:\n",
    "        print(\"Doing something C-ish\")\n",
    "        return StepCCompleteEvent(result=ev.query)\n",
    "\n",
    "    @step\n",
    "    async def step_three(\n",
    "        self,\n",
    "        ctx: Context,\n",
    "        ev: StepACompleteEvent | StepBCompleteEvent | StepCCompleteEvent,\n",
    "    ) -> StopEvent:\n",
    "        print(\"Received event \", ev.result)\n",
    "\n",
    "        # wait until we receive 3 events\n",
    "        if (\n",
    "            ctx.collect_events(\n",
    "                ev,\n",
    "                [StepCCompleteEvent, StepACompleteEvent, StepBCompleteEvent],\n",
    "            )\n",
    "            is None\n",
    "        ):\n",
    "            return None\n",
    "\n",
    "        # do something with all 3 results together\n",
    "        return StopEvent(result=\"Done\")\n",
    "\"\"\"\n",
    "# \n",
    "# \n",
    "# \n",
    "#### 工作流子类化 <扩展工作流的方法 1>\n",
    "# 工作流的另一个很棒的特性是它们的可扩展性。你可以使用其让人编写的工作流或LlamaIndex的内置工作流，并对其进行扩展、自定义。\n",
    "# # 子类化  \n",
    "# 工作流只是普通的Python类，这意味着你可以进行子类化以添加新功能。继承和多态\n",
    "# \n",
    "# \n",
    "#### 嵌套工作流  <扩展工作流的方法 2>\n",
    "# 扩展工作流的另一种方法是嵌套额外的工作流。可以在现有流程中创建明确的插槽，以便可以在其中提供一个完整的额外工作流。\n",
    "# 例如：假设现在有一个使用LLM来反思该查询质量的查询。作者可能希望您修改反思步骤，并留下一个插槽来执行此操作\n",
    "class Step2Event(Event):\n",
    "    query: str \n",
    "class MainWorkflow(Workflow):\n",
    "    @step \n",
    "    async def start(\n",
    "        self, ctx:Context, ev:StartEvent, reflection_workflow: Workflow\n",
    "    ) -> Step2Event:\n",
    "        print(\"need add reflection\")\n",
    "        res = await reflection_workflow.run(query=ev.query)\n",
    "    \n",
    "    @step \n",
    "    async def step_two(self, ctx:Context, ev:Step2Event) -> StopEvent:\n",
    "        print(\"query is \", ev.query)\n",
    "        # do something \n",
    "        return StopEvent(result=ev.query)\n",
    "# 需要完善 reflection_workflow\n",
    "class ReflectionWorkflow(Workflow):\n",
    "    @step \n",
    "    async def sub_start(Self, ctx:Context, ev:StartEvent) -> StopEvent:\n",
    "        print(\"do reflection\")\n",
    "        return StopEvent(result=\"Improved query\")\n",
    "# 现在我们可以通过使用 add_worflows 方法提供这个自定义的反思嵌套流程来运行主工作流，\n",
    "# 我们将一个 ReflectionWorkflow 类的实例传递给该方法\n",
    "w = MainWorkflow(timeout=100, verbose=False)\n",
    "w.add_workflows(reflection_workflow=ReflectionWorkflow())\n",
    "print(result)\n",
    "# 请注意，因为嵌套流程是一个完全不同的工作流，而不是一个步骤，所以\n",
    "# draw_all_possible_flows  只会绘制  MainWorkflow  的流程图。\n",
    "#### 默认工作流\n",
    "# 函数的默认值一样的存在\n",
    "# \n",
    "##### 可观测性\n",
    "# # 可视化   draw_all_possible_flows \n",
    "# # 详细模式  verbose=True ,  简洁模式 verbose=Fasle\n",
    "# # 步进执行  AgentWorkflow.run(stepwise=True) # 默认 False\n",
    "# # 可视化最近的执行   \n",
    "# from llama_index.utils.workflow import draw_most_recent_execution\n",
    "# draw_most_recent_execution(w, filename=\"last_execution.html\")\n",
    "# 注意，这里传递的是工作流的实例 w，而不是类名。\n",
    "# # 检查点\n",
    "# 检查完整的工作流执行可能需要花费大量时间，而且通常只需要同时调试和观察几个步骤。\n",
    "# 为了加快工作流开发周期，WorkflowCheckpointer会封装一个Workflow，并在每次运行的每个步骤完成后创建并存储checkpoint。\n",
    "# 这些检查点可以被查看、检查，并被选作未来运行的起始点。\n",
    "# from llama_index.core.workflow.checkpointer import WorkflowCheckpointer\n",
    "\"\"\"  \n",
    "from llama_index.core.workflow.checkpointer import WorkflowCheckpointer\n",
    "\n",
    "w = ConcurrentFlow()\n",
    "w_ckptr = WorkflowCheckpointer(workflow=w)\n",
    "\n",
    "# run the workflow via w_ckptr to get checkpoints\n",
    "handler = w_cptr.run()\n",
    "await handler\n",
    "\n",
    "# view checkpoints of the last run\n",
    "w_ckptr.checkpoints[handler.run_id]\n",
    "\n",
    "# run from a previous ckpt\n",
    "ckpt = w_ckptr.checkpoints[handler.run_id][0]\n",
    "handler = w_ckptr.run_from(checkpoint=ckpt)\n",
    "await handler\n",
    "\"\"\"\n",
    "# \n",
    "# \n",
    "# # 第三方工具 Arize ...\n",
    "\n",
    "#### 未绑定语法\n",
    "# 从无绑定函数创建工作流\n",
    "# 可以通过独立的或 无绑定 的函数定义工作流中的步骤，并使用 @step 装饰器将它们分配给工作流\n",
    "# \n",
    "class TestWorkflow(Workflow):\n",
    "    pass \n",
    "@step(Workflow=TestWorkflow)      # 配置一下装饰器就ok\n",
    "def some_step(ev: StartEvent) -> StopEvent:\n",
    "    return StopEvent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.构建 RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- IngestionPipeline\n",
    "- [IngestPipeline](https://docs.llamaindex.org.cn/en/stable/module_guides/loading/ingestion_pipeline/)  （LLM处理`准备好的数据`之前，需要准备好数据，准备好数据的过程`处理并加载数据`）\n",
    "- 加载数据： 数据加载器、\n",
    "- [转换数据](https://docs.llamaindex.org.cn/en/stable/module_guides/loading/ingestion_pipeline/transformations/)\n",
    "- [索引与嵌入](https://docs.llamaindex.org.cn/en/stable/understanding/indexing/indexing/)和[存储数据](https://docs.llamaindex.org.cn/en/stable/understanding/storing/storing/)：\n",
    "- 概念：[嵌入](https://docs.llamaindex.org.cn/en/stable/module_guides/models/embeddings/#list-of-supported-embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### 1 加载与提取\n",
    "#\n",
    "# IngestPipeline 1.加载数据 \n",
    "# (加载数据、转换数据、索引和存储数据)     \n",
    "#\n",
    "###### 数据加载器   \n",
    "# 在您选择的 LLM 处理您的数据之前，您需要加载数据。LlamaIndex 通过数据连接器来实现这一点，也称为 Reader。\n",
    "# 数据连接器从不同的数据源摄取数据，并将数据格式化为 Document 对象。\n",
    "# Document 是数据的集合（目前是文本，未来将包含图像和音频）以及关于该数据的元数据。\n",
    "#\n",
    "### SimpleDirectoryReader 可以将给定目录中的每个文件创建为文档\n",
    "from llama_index.core import SimpleDirectoryReader      # md, pdf, docx,pptx, img, wav, mp4\n",
    "# documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "### or --->  LlamaHub的读取器\n",
    "from llama_index.core import download_loader\n",
    "# from llama_index.readers.xxx import xxxx \n",
    "### 直接创建Document对象， Node\n",
    "from llama_index.core import Document \n",
    "documents = Document(text=\"text\")\n",
    "#\n",
    "# IngestPipeline 2.转换数据 \n",
    "# (加载数据、转换数据、索引和存储数据)   \n",
    "###### 转换\n",
    "# 数据加载后，您需要在将其放入存储系统之前对其进行处理和转换。这些转换包括分块、提取元数据以及嵌入每个块。\n",
    "# 这对于确保数据可以被检索并被 LLM 最佳地使用是必要的。\n",
    "# 转换的输入/输出是 Node 对象（Document 是 Node 的子类）。转换也可以堆叠和重新排序。\n",
    "# llma-index提供了用于转换文档的高级和低级 API。\n",
    "\"\"\" 转换\n",
    "https://docs.llamaindex.org.cn/en/stable/module_guides/loading/ingestion_pipeline/transformations/\n",
    "# 转换是一种以节点列表作为输入，并返回一个节点列表的事物。每个实现 Transformation 基类的组件都具有同步的 __call__() 定义和异步的 acall() 定义。\n",
    "# 以下组件是Transformations的对象\n",
    "- 文本分割器\n",
    "- 节点解析器\n",
    "- 元数据提取器\n",
    "- Embeddings模型\n",
    "\n",
    "\"\"\"\n",
    "#\n",
    "# ### 高级转换API \n",
    "#  .from_documents() 方法，它接受一个 Document 对象数组，并会正确解析和分块。然而，有时您可能需要更精细地控制文档如何分割。\n",
    "from llama_index.core import VectorStoreIndex\n",
    "vector_index = VectorStoreIndex.from_documents(documents)\n",
    "vector_index.as_query_engine()\n",
    "# 在底层，这将您的 Document 分割成 Node 对象，Node 对象与 Document 类似（它们包含文本和元数据），但与它们的父 Document 存在关系。\n",
    "# \n",
    "# 如果您想通过这种抽象定制核心组件，例如文本分割器，您可以传入自定义的 transformations 列表或应用于全局 Settings。\n",
    "from llama_index.core.node_parser import SentenceSplitter \n",
    "text_spliter = SentenceSplitter(chunk_size=512, chunk_overlap=128)\n",
    "# global\n",
    "from llama_index.core import Settings \n",
    "Settings.text_splitter = text_spliter\n",
    "# pre-index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents, transformations=[text_spliter]\n",
    ")\n",
    "#\n",
    "### 低级转换API\n",
    "# 显式定义这些步骤。\n",
    "# 您可以通过将我们的转换模块（文本分割器、元数据提取器等）用作独立组件，或者在我们的声明式 转换管道接口 Ingestpipeline 中组合它们来完成此操作。\n",
    "\"\"\"  IngestPipeline\n",
    "# IngestionPipeline 使用了应用于输入数据的转换概念。这些转换应用于您的输入数据，生成的节点要么返回，要么插入到向量数据库（如果提供）。\n",
    "# 每个节点+转换对都会被缓存，因此后续使用相同节点+转换组合的运行（如果缓存已持久化）可以使用缓存结果，从而节省时间。\n",
    "https://docs.llamaindex.org.cn/en/stable/module_guides/loading/ingestion_pipeline/\n",
    "\n",
    "\"\"\"\n",
    "#\n",
    "## 将文档分割成node\n",
    "# 处理文档的一个关键步骤是将其分割成“块”/Node 对象。核心思想是将您的数据处理成适合检索/馈送给 LLM 的小片段。\n",
    "# LlamaIndex 支持多种 文本分割器，范围从基于段落/句子/token 的分割器到基于文件的分割器，如 HTML、JSON。\n",
    "\"\"\"  文本分割器\n",
    "https://docs.llamaindex.org.cn/en/stable/module_guides/loading/node_parsers/modules/\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"  这些也可以单独使用作为IngestPipeline的一部分\n",
    "https://docs.llamaindex.org.cn/en/stable/module_guides/loading/node_parsers/modules/#text-splitters\n",
    "\n",
    "\"\"\"\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.ingestion import IngestionPipeline \n",
    "from llama_index.core.node_parser import TokenTextSplitter \n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "pipeline = IngestionPipeline(transformations=[TokenTextSplitter(), ...])\n",
    "nodes = pipeline.run(documents=documents)\n",
    "#\n",
    "## 添加元数据\n",
    "# 您还可以选择为文档和 Node 添加元数据。这可以手动完成，也可以使用自动元数据提取器完成。\n",
    "\"\"\" 自动元数据提取器\n",
    "https://docs.llamaindex.org.cn/en/stable/module_guides/loading/documents_and_nodes/usage_metadata_extractor/\"\"\"\n",
    "# 这里是关于 1) 如何定制 Document 和 2) 如何定制 Node 的指南。\n",
    "\"\"\"  1) 如何定制 Document \n",
    "https://docs.llamaindex.org.cn/en/stable/module_guides/loading/documents_and_nodes/usage_documents/\n",
    "\"\"\"\n",
    "\"\"\"  2) 如何定制 Node  \n",
    "https://docs.llamaindex.org.cn/en/stable/module_guides/loading/documents_and_nodes/usage_nodes/\n",
    "\"\"\"\n",
    "## 添加嵌入\n",
    "# 要将 Node 插入向量索引，它应该具有嵌入。\n",
    "\"\"\" IngestionPipeline or 嵌入 \n",
    "https://docs.llamaindex.org.cn/en/stable/module_guides/loading/ingestion_pipeline/\n",
    "https://docs.llamaindex.org.cn/en/stable/module_guides/models/embeddings/\n",
    "\"\"\"\n",
    "# \n",
    "## 直接创建和传递Node\n",
    "from llama_index.core.schema import TextNode\n",
    "node1 = TextNode(text='<text_chunk>', id='<node_id>')\n",
    "node2 = TextNode(text='<text_chunk>', id='<node_id>')\n",
    "index = VectorStoreIndex([node1, node2])\n",
    "\n",
    "\n",
    "\n",
    "###### 2 索引与嵌入\n",
    "#\n",
    "#### 索引\n",
    "# 加载数据后，您现在拥有一个 Document 对象列表（或 Node 对象列表）。现在是时候在这些对象之上构建一个 Index，以便您可以开始查询它们了。\n",
    "\"\"\"什么是索引？\n",
    "在 LlamaIndex 中，Index 是一个由 Document 对象构成的数据结构，旨在支持 LLM 进行查询。您的索引设计用于补充您的查询策略。\n",
    "LlamaIndex 提供了几种不同的索引类型。这里我们将介绍其中两种最常见的。\n",
    "- 向量存储索引\n",
    "- 摘要索引\n",
    "\"\"\"\n",
    "#\n",
    "### 向量存储索引\n",
    "# VectorStoreIndex 是您将遇到的最常见的索引类型。\n",
    "# 向量存储索引接收您的 Documents 并将其分割成 Nodes。然后，它为每个节点的文本创建向量嵌入，以便 LLM 进行查询。\n",
    "#\n",
    "### 摘要索引\n",
    "# 摘要索引是一种更简单的索引形式，最适合于正如其名称所示，您正在尝试生成文档中文本摘要的查询。\n",
    "# 它只是简单地存储所有 Documents 并将它们全部返回给您的查询引擎。\n",
    "\n",
    "\n",
    "\n",
    "###### 3 存储  \n",
    "# 加载并索引数据后，您可能希望存储它，以避免重新索引的时间和成本。默认情况下，您的索引数据仅存储在内存中。\n",
    "#### 持久化到磁盘\n",
    "# 存储索引数据最简单的方法是使用每个 Index 内置的 .persist() 方法，该方法将所有数据写入指定位置的磁盘。这适用于任何类型的索引。\n",
    "index.storage_context.persist(persist_dir=\"<persist_dir>\")\n",
    "# 可组合图（Composable Graph）的示例\n",
    "# graph.root_index.storage_context.persist(persist_dir=\"<persist_dir>\")\n",
    "# 可以通过持久化的碎银来避免重新加载和重新索引数据\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"<persist_dir>\")\n",
    "# load index \n",
    "index = load_index_from_storage(storage_context)\n",
    "# 重要提示：如果您使用自定义的 transformations、embed_model 等初始化了索引，则需要在 load_index_from_storage 期间传入相同的选项，或者将其设置为全局设置。\n",
    "# from llama_index.core import Settings\n",
    "# \n",
    "####  使用向量存储\n",
    "# 正如在索引中所讨论的，Index 最常见的类型之一是 VectorStoreIndex。\n",
    "# Embedding API调用 推荐。\n",
    "# LlamaIndex 支持大量的向量存储，它们在架构、复杂性和成本方面各不相同。\n",
    "# chroma 开源  支持向量存储\n",
    "# 使用Chroma存储VectorStoreIndex中的embedding，需要\n",
    "# - 初始化Chroma客户端\n",
    "# - 在Chroma中创建一个Collection来存储数据\n",
    "# - 在StorageContext中将Chroma指定为vector_store\n",
    "# - 使用该StorageContext初始化VectorStoreIndex\n",
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader \n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore \n",
    "from llama_index.core import StorageContext \n",
    "# \n",
    "# load files \n",
    "documents = SimpleDirectoryReader(\"./xx\").load_data()\n",
    "# \n",
    "# initial chroma client, set local_db path  to persist\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "# \n",
    "# create collection\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "#\n",
    "# assign chroma as the vector_store to the context \n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# \n",
    "# create index\n",
    "index = VectorStoreIndex.from_documents(documents=documents, storage_context=storage_context)\n",
    "# create a query engine and query\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"...\")\n",
    "print(response)\n",
    "#\n",
    "# 如果您已经创建并存储了 embedding，您会希望直接加载它们，而无需加载文档或创建新的 VectorStoreIndex\n",
    "#### 加载\n",
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "# init client\n",
    "db = chromadb.PersistentClient(path=\"./load_chroma_db\")\n",
    "# get collection\n",
    "chroma_collection = db.get_collection(\"quickstart\")\n",
    "# assign chroma as the vector_store to the context \n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# load index\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store, storage_context=storage_context)\n",
    "# create a query engine \n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"...\")\n",
    "print(response)\n",
    "#\n",
    "#### 插入文档或者节点\n",
    "# 如果已经创建了索引，可以使用 insert 方法向索引中添加新文档。\n",
    "from llama_index.core import VectorStoreIndex\n",
    "index = VectorStoreIndex([])\n",
    "for doc in documents:\n",
    "    index.insert(doc)\n",
    "#\n",
    "###### 4 查询 \n",
    "# - 最简单的查询只是对 LLM 的一次提示调用：它可以是一个问题并获取答案，或者是一个摘要请求，亦或是一个更复杂的指令。\n",
    "# - 更复杂的查询可能涉及重复/链式提示 + LLM 调用，甚至跨多个组件的推理循环。\n",
    "# 所有查询的基础是 QueryEngine。获取 QueryEngine 的最简单方法是让您的索引为您创建一个\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"...\")\n",
    "print(response)\n",
    "# 查询并不像初看起来那么简单。查询由三个不同的阶段组成:\n",
    "# - 检索是指您从 Index 中找到并返回与您的查询最相关的文档。正如之前在索引中讨论的，最常见的检索类型是“top-k”语义检索，但还有许多其他检索策略。\n",
    "#   - 后处理是指对检索到的 Node 可选地进行重排、转换或过滤，例如要求它们具有特定的元数据（如关键词）。\n",
    "#   - 响应合成是指将您的查询、最相关的数据和提示组合起来，发送给您的 LLM 以返回响应。\n",
    "#   - 提示\n",
    "## LlamaIndex 提供了一个低级组合 API，让您可以精细控制您的查询。 \n",
    "# 在此示例中，我们自定义检索器，对 top_k 使用不同的值，并添加一个后处理步骤，要求检索到的节点达到最低相似度分数才能包含在内。\n",
    "# 这会使您在有相关结果时获得大量数据，但在没有任何相关内容时可能不会获得数据\n",
    "# 还可以通过实现相应的接口来添加您自己的检索、响应合成和整体查询逻辑。\n",
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "# build index\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "# configure retriever \n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")\n",
    "# configure response synthesizer \n",
    "response_synthesizer = get_response_synthesizer()\n",
    "# assemble query engine \n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")\n",
    "# query\n",
    "response = query_engine.query(\"...\")\n",
    "print(response)\n",
    "\"\"\"  检索其模块指南\n",
    "https://docs.llamaindex.org.cn/en/stable/module_guides/querying/retriever/\n",
    "\"\"\"\n",
    "#\n",
    "### 配置节点后处理器\n",
    "# 我们支持高级 Node 过滤和增强，这可以进一步提高检索到的 Node 对象的相关性。这有助于减少时间/LLM 调用次数/成本或提高响应质量。\n",
    "# - KeywordNodePostprocessor: 通过 required_keywords 和 exclude_keywords 过滤节点。\n",
    "#   - SimilarityPostprocessor: 通过设置相似度分数阈值来过滤节点（因此仅嵌入式检索器支持）\n",
    "#   -PrevNextNodePostprocessor: 基于 Node 关系，用额外的相关上下文增强检索到的 Node 对象。\n",
    "\"\"\"  节点后处理器\n",
    "https://docs.llamaindex.org.cn/en/stable/api_reference/postprocessor/\n",
    "\"\"\"\n",
    "#\n",
    "### 配置响应合成\n",
    "from llama_index.core.postprocessor import KeywordNodePostprocessor\n",
    "node_postprocessors = [\n",
    "    KeywordNodePostprocessor=(\n",
    "        required_keywords=[\"key_a\"],\n",
    "        exclude_keywords=[\"key_b\"],\n",
    "    )\n",
    "]\n",
    "query_engine = RetrieverQueryEngine.from_args[\n",
    "    retriever=retriever, \n",
    "    node_postprocessors=node_postprocessors,\n",
    "]\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "# 检索器获取相关节点后，BaseSynthesizer 通过组合信息合成最终响应。\n",
    "#\n",
    "# - default：“创建并精炼”答案，通过依次处理每个检索到的 Node；这会为每个节点进行单独的 LLM 调用。适用于需要更详细的答案。\n",
    "# - compact：在每次 LLM 调用期间“压缩”提示，将尽可能多的 Node 文本块填充到最大提示大小内。如果在一个提示中无法容纳太多块，则通过多个提示进行“创建并精炼”答案。\n",
    "# - tree_summarize：给定一组 Node 对象和查询，递归构建一棵树并返回根节点作为响应。适用于摘要目的。\n",
    "# - no_text：仅运行检索器以获取本应发送给 LLM 的节点，但不实际发送。然后可以通过检查 response.source_nodes 进行检查。响应对象将在第 5 节更详细地介绍。\n",
    "# - accumulate：给定一组 Node 对象和查询，将查询应用于每个 Node 文本块，同时将响应累积到数组中。返回所有响应的连接字符串。适用于需要针对每个文本块单独运行相同查询的情况。\n",
    "# - 结构化输出\n",
    "# \n",
    "# \n",
    "# 后续：结构化输出、工作流"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.结构化数据提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### 1 使用结构化llms\n",
    "\n",
    "from datetime import datetime \n",
    "from pydantic import BaseModel, Field \n",
    "from typing import List \n",
    "    \n",
    "class LineItem(BaseModel):\n",
    "    \"\"\" line item in a table-like data \"\"\"\n",
    "    item_name: str = Field(description=\"name of item \")\n",
    "    price: float = Field(description=\"price of this item\")\n",
    "    \n",
    "class Invoice(BaseModel):\n",
    "    \"\"\" representation of information from a table-like data \"\"\"\n",
    "    invoice_id: str = Field(description=\"id\")\n",
    "    date: datetime = Field(description=\"date\")\n",
    "    line_items: List[LineItem] = Field(description=\"list of LineItem\")\n",
    "    \n",
    "\n",
    "##### LLM做结构化信息提取, \n",
    "class Patent(BaseModel):\n",
    "    \"\"\" patent \"\"\"\n",
    "    pass \n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM \n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name     = r\"Qwen/Qwen3-1.7B\",\n",
    "    tokenizer_name = r\"Qwen/Qwen3-1.7B\",\n",
    "    context_window = 3900,  \n",
    "    max_new_tokens = 640,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"top_k\": 30, \"top_p\": 0.95},\n",
    "    device_map     ='cpu' \n",
    ")\n",
    "# struct_llm = llm.as_structured_llm(Patent)\n",
    "# response = struct_llm.complete(text)   \n",
    "# ### response 是一个 LlamaIndex CompletionResponse 对象，它有两个属性： text 和 raw。 text 包含通过Pydantic摄取的响应的JSON序列化形式\n",
    "\n",
    "# 结构化LLM的工作方式与常规LLM类完全相同：可以调用 chat、stream、achat、astream 等，并且在所有情况下，它都会以Pydantic对象响应。\n",
    "# 还可以将结构化LLM作为参数传递给 VectorStoreIndex.as_query_engine(llm=sllm)，它将自动以结构化对象响应您的RAG查询。\n",
    "\n",
    "##### 2 结构化信息预测\n",
    "#\n",
    "from llama_index.core.prompts import PromptTemplate \n",
    "prompt = PromptTemplate(\n",
    "    \"Extract an invoice from the following text. If you cannot find an invoice ID, use the company name '{company_name}' and the date as the invoice ID: {text}\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 跟踪和调试  todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 评估  todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 集成所有组件  todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embedding = HuggingFaceEmbedding(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    max_length=1024,\n",
    "    trust_remote_code=True,\n",
    "    model_kwargs={\"local_files_only\": True},   # 允许联网 False , 禁止联网 True\n",
    "    device='cpu',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
